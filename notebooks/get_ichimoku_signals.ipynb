{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# external packages\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt \n",
    "from importlib import reload\n",
    "import matplotlib\n",
    "%matplotlib qt\n",
    "# %matplotlib inline\n",
    "import numpy as np\n",
    "matplotlib.style.use('default')\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "from collections import namedtuple\n",
    "from collections import deque\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ForexMachine.util' from 'c:\\\\github repos\\\\forexmachine\\\\ForexMachine\\\\util.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# local modules and packages\n",
    "from ForexMachine.Preprocessing import get_indicators as gi\n",
    "from ForexMachine import util\n",
    "reload(gi)\n",
    "reload(util)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trying loading and adding indicators to raw data w/ ForexMachine package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# convert config to dictionary\n",
    "config = util.yaml_to_dict()\n",
    "current_model = config['current_model']\n",
    "indicators = config[current_model]['indicators']\n",
    "print(indicators)\n",
    "# Read in data with indicators\n",
    "data_with_indicators = gi.add_indicators_to_raw(filepath='../Data/RawData/EURUSDi1440.csv', save_to_disk=True, \n",
    "                                                config=config)\n",
    "data_with_indicators.head(55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define helper plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_index_range(datetime1, datetime2, datetimes):\n",
    "    i1 = -1\n",
    "    i2 = -1\n",
    "    if datetime1 <= datetime2:\n",
    "        for i in range(len(datetimes)):\n",
    "            i1 = i\n",
    "            if datetimes[i] == datetime1:\n",
    "                break\n",
    "            if datetimes[i] > datetime1:\n",
    "                i1 = i-1 if i-1 >= 0 else 0\n",
    "                break\n",
    "        for i in range(i1, len(datetimes)):\n",
    "            i2 = i\n",
    "            if datetimes[i] == datetime2:\n",
    "                break\n",
    "            if datetimes[i] > datetime2:\n",
    "                i2 = i-1 if i-1 >= 0 else 0\n",
    "                break\n",
    "    return i1, i2\n",
    "\n",
    "# date format 'yyyy.mm.dd'\n",
    "def show_data_from_range(df, date1, date2, main_indicator, sub_indicators = [], visualize_crosses=False, crosses=None,\n",
    "                         visualize_labels=False, labels_df=None, labels=None):\n",
    "    \n",
    "    start, stop = get_index_range(pd.Timestamp.fromisoformat(date1), pd.Timestamp.fromisoformat(date2), df['datetime'].to_numpy())\n",
    "    if start < 0 or stop < 0:\n",
    "        print(f'invalid dates (start i = {start}, stop i = {stop})')\n",
    "        return\n",
    "    \n",
    "    data_range = df.iloc[start:stop+1]\n",
    "    chart_count = len(sub_indicators) + 1\n",
    "    \n",
    "    top_chart_ratio = 1\n",
    "    sub_chart_ratio = 0\n",
    "    if chart_count == 2:\n",
    "        top_chart_ratio = 3\n",
    "        sub_chart_ratio = 2 / (chart_count-1)\n",
    "    if chart_count > 2:\n",
    "        top_chart_ratio = 1\n",
    "        sub_chart_ratio = 1 / (chart_count-1)\n",
    "    height_ratios = [top_chart_ratio]\n",
    "    height_ratios.extend([sub_chart_ratio]*(chart_count-1))\n",
    "    fig, axes = plt.subplots(chart_count,1,sharex='col', gridspec_kw={'height_ratios':height_ratios})\n",
    "    fig.tight_layout(pad=1.8, h_pad=0.0)\n",
    "    \n",
    "    top_ax = None\n",
    "    bottom_ax = None\n",
    "    if chart_count > 1:\n",
    "        top_ax = axes[0]\n",
    "        bottom_ax = axes[len(axes)-1]\n",
    "    else:\n",
    "        bottom_ax = top_ax = axes\n",
    "    top_ax.plot(data_range.Close.to_list(), label='Close',color='brown')\n",
    "    \n",
    "    plot_indicator_funcs = {\n",
    "        'ichimoku': lambda ax, dataf: add_ichimoku_to_plot(ax, dataf, visualize_crosses, crosses),\n",
    "        'rsi': lambda ax, dataf: add_rsi_to_plot(ax, dataf),\n",
    "        'extra': lambda ax, extra_df, plot_range: add_extra_data_to_plot(ax, extra_df, plot_range)\n",
    "    }\n",
    "    \n",
    "    plot_indicator_funcs[main_indicator](top_ax, data_range)\n",
    "    \n",
    "    for i in range(len(sub_indicators)):\n",
    "        item = sub_indicators[i]\n",
    "        if isinstance(item, str):\n",
    "            plot_indicator_funcs[sub_indicators[i]](axes[i+1], data_range)\n",
    "        elif isinstance(item, pd.DataFrame):\n",
    "            plot_indicator_funcs['extra'](axes[i+1], item, (start, stop))\n",
    "        \n",
    "    if visualize_labels and labels_df is not None:\n",
    "        add_labels_to_plot(top_ax, df, labels_df, (start, stop), labels)\n",
    "\n",
    "    bottom_ax.set_xticks(np.arange(len(data_range)))\n",
    "    x_labels = [dt.strftime('%Y-%m-%d %H:%M') * ((i+1)%2) for i,dt in enumerate(data_range['datetime'])]\n",
    "    bottom_ax.set_xticklabels(x_labels,rotation=80, wrap=True)\n",
    "    \n",
    "    if chart_count > 1:\n",
    "        for ax in axes:\n",
    "            ax.legend()\n",
    "    else:\n",
    "        top_ax.legend()\n",
    "        \n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "Functions for adding indicators to a matplotlib chart\n",
    "\"\"\"\n",
    "\n",
    "def add_ichimoku_to_plot(ax, df, visualize_crosses = False, crosses=None):\n",
    "    ax.plot(df.trend_visual_ichimoku_a.to_list(), label='Senkou-Span a',linestyle='--',color='green')\n",
    "    ax.plot(df.trend_visual_ichimoku_b.to_list(), label='Senkou-Span b',linestyle='--',color='red')\n",
    "    ax.fill_between(np.arange(len(df)),df.trend_visual_ichimoku_a,\n",
    "                    df.trend_visual_ichimoku_b,alpha=0.2,color='green',\n",
    "                    where=(df.trend_visual_ichimoku_a > df.trend_visual_ichimoku_b))\n",
    "    ax.fill_between(np.arange(len(df)),df.trend_visual_ichimoku_a,\n",
    "                    df.trend_visual_ichimoku_b,alpha=0.2,color='red',\n",
    "                    where=(df.trend_visual_ichimoku_a <= df.trend_visual_ichimoku_b))\n",
    "    ax.plot(df.trend_ichimoku_conv.to_list(), label='Tenkan-Sen (conversion)',color='cyan')\n",
    "    ax.plot(df.trend_ichimoku_base.to_list(), label='Kijun Sen (base)',color='blue')\n",
    "    ax.plot(df.chikou_span_visual.to_list(), label='chikou span',linestyle=':',color='orange')\n",
    "    \n",
    "    if visualize_crosses:\n",
    "        colors = {\n",
    "            'tk_cross': 'hotpink',\n",
    "            'tk_price_cross': 'brown',\n",
    "            'senkou_cross': 'blue',\n",
    "            'chikou_cross': 'orange',\n",
    "            'kumo_breakout': 'purple'\n",
    "        }\n",
    "        \n",
    "        df_idx = {df.columns[i]: i for i in range(len(df.columns))}\n",
    "        data = df.to_numpy()\n",
    "        \n",
    "        if crosses is None:\n",
    "            crosses = set(['tk_cross', 'tk_price_cross', 'senkou_cross', 'chikou_cross', 'kumo_breakout'])\n",
    "        else:\n",
    "            crosses = set(crosses)\n",
    "                \n",
    "        for i in range(len(data)):   \n",
    "            close = data[i][df_idx['Close']]            \n",
    "            vert_occupied = False\n",
    "            filler = ''\n",
    "            \n",
    "            # tk cross\n",
    "            if 'tk_cross' in crosses:\n",
    "                tk_cross_bull_strength = abs(data[i][df_idx['tk_cross_bull_strength']])\n",
    "                tk_cross_bear_strength = abs(data[i][df_idx['tk_cross_bear_strength']])\n",
    "                tk_cross_length_bull = data[i][df_idx['tk_cross_bull_length']]\n",
    "                tk_cross_length_bear = data[i][df_idx['tk_cross_bear_length']]\n",
    "            \n",
    "                if not np.isnan(tk_cross_bull_strength) and tk_cross_bull_strength > 0:\n",
    "                    ax.axvline(x = i, color = colors['tk_cross'])\n",
    "                    ax.text(x = i, y = close, color = colors['tk_cross'],\n",
    "                            s = f'^ TK Cross Bull\\nstrength={tk_cross_bull_strength}\\nlength={tk_cross_length_bull}')\n",
    "                    vert_occupied = True\n",
    "\n",
    "                if not np.isnan(tk_cross_bear_strength) and tk_cross_bear_strength > 0:\n",
    "                    if vert_occupied:\n",
    "                        filler += '\\n'*3\n",
    "                    ax.axvline(x = i, color = colors['tk_cross'])\n",
    "                    ax.text(x = i, y = close, color = colors['tk_cross'],\n",
    "                            s = f'_ TK Cross Bear\\nstrength={tk_cross_bear_strength}'\n",
    "                                f'\\nlength={tk_cross_length_bear}{filler}')\n",
    "                    vert_occupied = True\n",
    "            \n",
    "            # tk price cross\n",
    "            if 'tk_price_cross' in crosses:\n",
    "                tk_price_cross_bull_strength = abs(data[i][df_idx['tk_price_cross_bull_strength']])\n",
    "                tk_price_cross_bear_strength = abs(data[i][df_idx['tk_price_cross_bear_strength']])\n",
    "                tk_price_cross_length_bull = data[i][df_idx['tk_price_cross_bull_length']]\n",
    "                tk_price_cross_length_bear = data[i][df_idx['tk_price_cross_bear_length']]\n",
    "                \n",
    "                if not np.isnan(tk_price_cross_bull_strength) and tk_price_cross_bull_strength > 0:\n",
    "                    if vert_occupied:\n",
    "                        filler += '\\n'*3\n",
    "                    ax.axvline(x = i, color = colors['tk_price_cross'])\n",
    "                    ax.text(x = i, y = close, color = colors['tk_price_cross'],\n",
    "                            s = f'^ TK Price Cross Bull\\nstrength={tk_price_cross_bull_strength}'\n",
    "                                f'\\nlength={tk_price_cross_length_bull}{filler}')\n",
    "                    vert_occupied = True\n",
    "\n",
    "                if not np.isnan(tk_price_cross_bear_strength) and tk_price_cross_bear_strength > 0:\n",
    "                    if vert_occupied:\n",
    "                        filler += '\\n'*3\n",
    "                    ax.axvline(x = i, color = colors['tk_price_cross'])\n",
    "                    ax.text(x = i, y = close, color = colors['tk_price_cross'],\n",
    "                            s = f'_ TK Price Cross Bear\\nstrength={tk_price_cross_bear_strength}'\n",
    "                                f'\\nlength={tk_price_cross_length_bear}{filler}')\n",
    "                    vert_occupied = True\n",
    "            \n",
    "            # senkou cross\n",
    "            if 'senkou_cross' in crosses:\n",
    "                senkou_cross_bull_strength = abs(data[i][df_idx['senkou_cross_bull_strength']])\n",
    "                senkou_cross_bear_strength = abs(data[i][df_idx['senkou_cross_bear_strength']])\n",
    "                senkou_cross_length_bull = data[i][df_idx['senkou_cross_bull_length']]\n",
    "                senkou_cross_length_bear = data[i][df_idx['senkou_cross_bear_length']]\n",
    "                \n",
    "                if not np.isnan(senkou_cross_bull_strength) and senkou_cross_bull_strength > 0:\n",
    "                    if vert_occupied:\n",
    "                        filler += '\\n'*3\n",
    "                    ax.axvline(x = i, color = colors['senkou_cross'])\n",
    "                    ax.text(x = i, y = close, color = colors['senkou_cross'],\n",
    "                            s = f'^ Senkou Cross Bull\\nstrength={senkou_cross_bull_strength}'\n",
    "                                f'\\nlength={senkou_cross_length_bull}{filler}')\n",
    "                    vert_occupied = True\n",
    "\n",
    "                if not np.isnan(senkou_cross_bear_strength) and senkou_cross_bear_strength > 0:\n",
    "                    if vert_occupied:\n",
    "                        filler += '\\n'*3\n",
    "                    ax.axvline(x = i, color = colors['senkou_cross'])\n",
    "                    ax.text(x = i, y = close, color = colors['senkou_cross'],\n",
    "                            s = f'_ Senkou Cross Bear\\nstrength={senkou_cross_bear_strength}'\n",
    "                                f'\\nlength={senkou_cross_length_bear}{filler}')\n",
    "                    vert_occupied = True\n",
    "                \n",
    "            # chikou cross\n",
    "            if 'chikou_cross' in crosses:\n",
    "                chikou_cross_bull_strength = abs(data[i][df_idx['chikou_cross_bull_strength']])\n",
    "                chikou_cross_bear_strength = abs(data[i][df_idx['chikou_cross_bear_strength']])\n",
    "                chikou_cross_length_bull = data[i][df_idx['chikou_cross_bull_length']]\n",
    "                chikou_cross_length_bear = data[i][df_idx['chikou_cross_bear_length']]\n",
    "                \n",
    "                if not np.isnan(chikou_cross_bull_strength) and chikou_cross_bull_strength > 0:\n",
    "                    if vert_occupied:\n",
    "                        filler += '\\n'*3\n",
    "                    ax.axvline(x = i, color = colors['chikou_cross'])\n",
    "                    ax.text(x = i, y = close, color = colors['chikou_cross'],\n",
    "                            s = f'^ Chikou Cross Bull\\nstrength={chikou_cross_bull_strength}'\n",
    "                                f'\\nlength={chikou_cross_length_bull}{filler}')\n",
    "                    vert_occupied = True\n",
    "\n",
    "                if not np.isnan(chikou_cross_bear_strength) and chikou_cross_bear_strength > 0:\n",
    "                    if vert_occupied:\n",
    "                        filler += '\\n'*3\n",
    "                    ax.axvline(x = i, color = colors['chikou_cross'])\n",
    "                    ax.text(x = i, y = close, color = colors['chikou_cross'],\n",
    "                            s = f'_ Chikou Cross Bear\\nstrength={chikou_cross_bear_strength}'\n",
    "                                f'\\nlength={chikou_cross_length_bear}{filler}')\n",
    "                    vert_occupied = True\n",
    "            \n",
    "            # kumo breakout\n",
    "            if 'kumo_breakout' in crosses:\n",
    "                cloud_breakout_bull = data[i][df_idx['cloud_breakout_bull']]\n",
    "                cloud_breakout_bear = data[i][df_idx['cloud_breakout_bear']]\n",
    "                \n",
    "                if cloud_breakout_bull:\n",
    "                    if vert_occupied:\n",
    "                        filler += '\\n'*3\n",
    "                    ax.axvline(x = i, color = colors['kumo_breakout'])\n",
    "                    ax.text(x = i, y = close, color = colors['kumo_breakout'], s = f'^ Kumo Breakout Bullish{filler}')\n",
    "                    vert_occupied = True\n",
    "\n",
    "                if cloud_breakout_bear:\n",
    "                    if vert_occupied:\n",
    "                        filler += '\\n'*3\n",
    "                    ax.axvline(x = i, color = colors['kumo_breakout'])\n",
    "                    ax.text(x = i, y = close, color = colors['kumo_breakout'], s = f'_ Kumo Breakout Bearish{filler}')\n",
    "                    vert_occupied = True\n",
    "        \n",
    "\n",
    "def add_rsi_to_plot(ax, df):\n",
    "    ax.plot(df.momentum_rsi.to_list(), label='RSI', color='purple')\n",
    "    ax.plot([30]*len(df),color='gray',alpha=0.5)\n",
    "    ax.plot([70]*len(df),color='gray',alpha=0.5)\n",
    "    ax.fill_between(np.arange(len(df)),[30]*len(df),[70]*len(df),color='gray',alpha=0.2)\n",
    "    ax.set_ylim(15,85)\n",
    "    ax.set_yticks(np.arange(20,100,20))\n",
    "\n",
    "def add_labels_to_plot(ax, all_feat_df, labels_df, plot_range, labels=None):\n",
    "    if labels is None:\n",
    "        labels = set(['first_decision','ticks_till_best_profit_first_decision', 'best_profit_first_decision', 'profit_peak_first_decision',\n",
    "                      'second_decision', 'ticks_till_best_profit_second_decision', 'best_profit_second_decision', 'profit_peak_second_decision',\n",
    "                      'decision_pred','ticks_till_best_profit_decision_pred', 'best_profit_decision_pred', 'profit_peak_decision_pred'])\n",
    "    else:\n",
    "        labels = set(labels)\n",
    "    \n",
    "    colors = {\n",
    "        'buy': 'green',\n",
    "        'sell': 'red',\n",
    "    }\n",
    "    \n",
    "    start, stop = plot_range\n",
    "    plot_data_len = stop-start+1\n",
    "    \n",
    "    feat_data = all_feat_df.to_numpy()\n",
    "    labels_data = labels_df.to_numpy()\n",
    "    feat_df_idx = {all_feat_df.columns[i]: i for i in range(len(all_feat_df.columns))}\n",
    "    labels_df_idx = {labels_df.columns[i]: i for i in range(len(labels_df.columns))}\n",
    "    \n",
    "    verts_occupied = {}\n",
    "    for i in range(plot_data_len):\n",
    "        labels_i = i + start\n",
    "        close = feat_data[labels_i][feat_df_idx['Close']]            \n",
    "        \n",
    "        # the 1st and 2nd decisions should never occupy the same vert\n",
    "        printed_causes = False\n",
    "        for label_name in ['first_decision', 'second_decision', 'decision_pred']: \n",
    "            decision = None if label_name not in labels else labels_data[labels_i][labels_df_idx[label_name]]\n",
    "            if not pd.isnull(decision):\n",
    "                decision_type = 'true'\n",
    "                if label_name == 'decision_pred':\n",
    "                    decision_type = 'prediction'\n",
    "                \n",
    "                lines = 2\n",
    "                filler = '\\n'\n",
    "                if i in verts_occupied:\n",
    "                    filler = '\\n' * (verts_occupied[i] + 1)\n",
    "                \n",
    "                color = colors[decision]\n",
    "                txt = [f'{filler}---------------------------------------',\n",
    "                       f'{decision_type} {label_name}: {decision}']\n",
    "\n",
    "                if f'best_profit_{label_name}' in labels:\n",
    "                    profit = labels_data[labels_i][labels_df_idx[f'best_profit_{label_name}']] \n",
    "                    txt.append(f'best profit: {profit}')\n",
    "                    lines+=1\n",
    "\n",
    "                    if f'profit_peak_{label_name}' in labels:\n",
    "                        peak_idx = int(labels_data[labels_i][labels_df_idx[f'profit_peak_{label_name}']])\n",
    "                        plot_idx = peak_idx - start\n",
    "                        txt.append(f'best profit datetime: {feat_data[peak_idx][feat_df_idx[\"datetime\"]].strftime(\"%Y-%m-%d %H:%M\")}')\n",
    "                        lines+=1\n",
    "\n",
    "                        if plot_idx < plot_data_len:\n",
    "                            peak_close = feat_data[peak_idx][feat_df_idx['Close']]   \n",
    "                            ax.plot(plot_idx, peak_close, marker='o', markersize=12, color='black')\n",
    "                            filler_2 = '\\n'\n",
    "                            if plot_idx in verts_occupied:\n",
    "                                filler_2 = ' \\n' * (verts_occupied[plot_idx] + 1)\n",
    "                                verts_occupied[plot_idx] += 2\n",
    "                            else:\n",
    "                                verts_occupied[plot_idx] = 2\n",
    "                            ax.text(x=plot_idx, y=peak_close, color=color, verticalalignment='top',\n",
    "                                    s=f'{filler_2}closed {decision_type} {decision} from '\n",
    "                                      f'{feat_data[labels_i][feat_df_idx[\"datetime\"]].strftime(\"%Y-%m-%d %H:%M\")}\\nprofit: {profit}')\n",
    "\n",
    "                if f'ticks_till_best_profit_{label_name}' in labels:\n",
    "                    ticks = int(labels_data[labels_i][labels_df_idx[f'ticks_till_best_profit_{label_name}']]) \n",
    "                    txt.append(f'ticks till best: {ticks}')\n",
    "                    lines+=1\n",
    "\n",
    "                if 'causes' in labels and not printed_causes:\n",
    "                    causes = labels_data[labels_i][labels_df_idx['causes']] \n",
    "                    txt.append(f'causes: {causes}')\n",
    "                    printed_causes = True\n",
    "                    lines+=1\n",
    "                \n",
    "                txt = '\\n'.join(txt)\n",
    "                ax.plot(i, close, marker='o', markersize=12, color='black')\n",
    "                ax.text(x=i, y=close, color=color, verticalalignment='top',\n",
    "                        s=txt)\n",
    "            \n",
    "                if i in verts_occupied:\n",
    "                    verts_occupied[i] += lines\n",
    "                else:\n",
    "                    verts_occupied[i] = lines\n",
    "\n",
    "def add_extra_data_to_plot(ax, extra_df, plot_range):\n",
    "    start, stop = plot_range\n",
    "    extra_df = extra_df.iloc[start:stop+1]\n",
    "    for col in extra_df:\n",
    "        ax.plot(extra_df[col].to_numpy(), label=col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define helper functions and classes for generating features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(df, inplace=False, negative_bears=True, include_most_recent_feats=False):\n",
    "    \n",
    "    ### temporal features\n",
    "    \n",
    "    quarters = []\n",
    "    days_of_week = []\n",
    "    months = []\n",
    "    days = []\n",
    "    minutes = []\n",
    "    hours = []\n",
    "    years = []\n",
    "    \n",
    "    ### ichimoku features\n",
    "    \n",
    "    is_price_above_cb_lines = []\n",
    "    is_price_above_cloud = []\n",
    "    is_price_inside_cloud = []\n",
    "    is_price_below_cloud = []\n",
    "    cloud_breakout_bull = []\n",
    "    cloud_breakout_bear = []\n",
    "    ticks_since_cloud_breakout_bull = []\n",
    "    ticks_since_cloud_breakout_bear = []\n",
    "    \n",
    "    first_kumo_breakout_bull = False\n",
    "    first_kumo_breakout_bear = False\n",
    "    \n",
    "    # names of each cross type\n",
    "    cross_names = ['tk_cross','tk_price_cross','senkou_cross','chikou_cross']\n",
    "    # dict to hold similar features of each cross type\n",
    "    crosses_dict = {} \n",
    "    for name in cross_names:\n",
    "        crosses_dict[name] = {\n",
    "            'most_recent_bull_strength': [],\n",
    "            'most_recent_bear_strength': [],\n",
    "            'bull_strength': [],\n",
    "            'bear_strength': [],\n",
    "            'ticks_since_bull': [],\n",
    "            'ticks_since_bear': [],\n",
    "            'most_recent_bull_length': [],\n",
    "            'most_recent_bear_length': [],\n",
    "            'bull_length': [],\n",
    "            'bear_length': [],\n",
    "            'first_bull': False,\n",
    "            'first_bear': False\n",
    "        }\n",
    "    \n",
    "    data = df.to_numpy()\n",
    "    feature_indices = {df.columns[i]:i for i in range(len(df.columns))}\n",
    "    \n",
    "    fg = FeatuteGenerator(data, feature_indices)\n",
    "    for i in range(len(data)):\n",
    "        # get temporal features signals\n",
    "        temporal_features = fg.get_temporal_features(i)\n",
    "        quarters.append(temporal_features.quarter)\n",
    "        days_of_week.append(temporal_features.day_of_week)\n",
    "        months.append(temporal_features.month)\n",
    "        days.append(temporal_features.day)\n",
    "        minutes.append(temporal_features.minute)\n",
    "        hours.append(temporal_features.hour)\n",
    "        years.append(temporal_features.year)\n",
    "        \n",
    "        # get ichimoku signals\n",
    "        ichimoku_features = fg.get_ichimoku_features(i, cross_length_limit=np.Inf)\n",
    "        is_price_above_cb_lines.append(ichimoku_features['is_price_above_cb_lines'])\n",
    "        is_price_above_cloud.append(ichimoku_features['is_price_above_cloud'])\n",
    "        is_price_inside_cloud.append(ichimoku_features['is_price_inside_cloud'])\n",
    "        is_price_below_cloud.append(ichimoku_features['is_price_below_cloud'])\n",
    "        \n",
    "        # handle kumo breakout\n",
    "        cloud_breakout_bull.append(ichimoku_features['cloud_breakout_bull'])\n",
    "        cloud_breakout_bear.append(ichimoku_features['cloud_breakout_bear']) \n",
    "        \n",
    "        if ichimoku_features['cloud_breakout_bull']:\n",
    "            first_kumo_breakout_bull = True\n",
    "        if ichimoku_features['cloud_breakout_bear']:\n",
    "            first_kumo_breakout_bear = True\n",
    "        \n",
    "        if first_kumo_breakout_bull:\n",
    "            if ichimoku_features['cloud_breakout_bull']:\n",
    "                ticks_since_cloud_breakout_bull.append(0)\n",
    "            else:\n",
    "                ticks_since_cloud_breakout_bull.append(ticks_since_cloud_breakout_bull[-1] + 1)\n",
    "        else:\n",
    "            ticks_since_cloud_breakout_bull.append(None)\n",
    "        \n",
    "        if first_kumo_breakout_bear:\n",
    "            if ichimoku_features['cloud_breakout_bear']:\n",
    "                ticks_since_cloud_breakout_bear.append(0)\n",
    "            else:\n",
    "                ticks_since_cloud_breakout_bear.append(ticks_since_cloud_breakout_bear[-1] + 1)\n",
    "        else:\n",
    "            ticks_since_cloud_breakout_bear.append(None)\n",
    "        \n",
    "        # handle other ichimoku cloud crosses\n",
    "        for cross_name in crosses_dict:\n",
    "            cross_dict = crosses_dict[cross_name]\n",
    "            \n",
    "            bull_strength, bear_strength, cross_length = ichimoku_features[cross_name]\n",
    "            \n",
    "            if bull_strength > 0:\n",
    "                cross_dict['first_bull'] = True \n",
    "            if bear_strength > 0:\n",
    "                cross_dict['first_bear'] = True\n",
    "            \n",
    "            if cross_dict['first_bull']:\n",
    "                if bull_strength > 0:\n",
    "                    cross_dict['most_recent_bull_strength'].append(bull_strength)\n",
    "                    cross_dict['bull_strength'].append(bull_strength)\n",
    "                    cross_dict['ticks_since_bull'].append(0)\n",
    "                    cross_dict['most_recent_bull_length'].append(cross_length)\n",
    "                    cross_dict['bull_length'].append(cross_length)\n",
    "                else:\n",
    "                    cross_dict['most_recent_bull_strength'].append(cross_dict['most_recent_bull_strength'][-1])\n",
    "                    cross_dict['bull_strength'].append(0)\n",
    "                    cross_dict['ticks_since_bull'].append(cross_dict['ticks_since_bull'][-1] + 1)\n",
    "                    cross_dict['most_recent_bull_length'].append(cross_dict['most_recent_bull_length'][-1])\n",
    "                    cross_dict['bull_length'].append(0)\n",
    "            else:\n",
    "                cross_dict['most_recent_bull_strength'].append(None)\n",
    "                cross_dict['bull_strength'].append(None)\n",
    "                cross_dict['ticks_since_bull'].append(None)\n",
    "                cross_dict['most_recent_bull_length'].append(None)\n",
    "                cross_dict['bull_length'].append(None)\n",
    "            \n",
    "            if cross_dict['first_bear']:\n",
    "                if bear_strength > 0: \n",
    "                    if negative_bears:\n",
    "                        bear_strength *= -1\n",
    "                    cross_dict['most_recent_bear_strength'].append(bear_strength)\n",
    "                    cross_dict['bear_strength'].append(bear_strength)\n",
    "                    cross_dict['ticks_since_bear'].append(0)\n",
    "                    cross_dict['most_recent_bear_length'].append(cross_length)\n",
    "                    cross_dict['bear_length'].append(cross_length)\n",
    "                else:\n",
    "                    cross_dict['most_recent_bear_strength'].append(cross_dict['most_recent_bear_strength'][-1])\n",
    "                    cross_dict['bear_strength'].append(0)\n",
    "                    cross_dict['ticks_since_bear'].append(cross_dict['ticks_since_bear'][-1] + 1)\n",
    "                    cross_dict['most_recent_bear_length'].append(cross_dict['most_recent_bear_length'][-1])\n",
    "                    cross_dict['bear_length'].append(0)\n",
    "            else:\n",
    "                cross_dict['most_recent_bear_strength'].append(None)\n",
    "                cross_dict['bear_strength'].append(None)\n",
    "                cross_dict['ticks_since_bear'].append(None)\n",
    "                cross_dict['most_recent_bear_length'].append(None)\n",
    "                cross_dict['bear_length'].append(None)\n",
    "    \n",
    "    if not inplace:\n",
    "        df = df.copy()\n",
    "    \n",
    "    df['quarter'] = quarters\n",
    "    df['day_of_week'] = days_of_week\n",
    "    df['month'] = months\n",
    "    df['day'] = days\n",
    "    df['minute'] = minutes\n",
    "    df['hour'] = hours\n",
    "    df['year'] = years\n",
    "    df['is_price_above_cb_lines'] = is_price_above_cb_lines\n",
    "    df['is_price_above_cloud'] = is_price_above_cloud\n",
    "    df['is_price_inside_cloud'] = is_price_inside_cloud\n",
    "    df['is_price_below_cloud'] = is_price_below_cloud\n",
    "    df['cloud_breakout_bull'] = cloud_breakout_bull\n",
    "    df['cloud_breakout_bear'] = cloud_breakout_bear\n",
    "    \n",
    "    if include_most_recent_feats:\n",
    "        df['ticks_since_cloud_breakout_bull'] = ticks_since_cloud_breakout_bull\n",
    "        df['ticks_since_cloud_breakout_bear'] = ticks_since_cloud_breakout_bear\n",
    "        \n",
    "    for cross_name in crosses_dict:\n",
    "        df[f'{cross_name}_bull_strength'] = crosses_dict[cross_name]['bull_strength']\n",
    "        df[f'{cross_name}_bear_strength'] = crosses_dict[cross_name]['bear_strength']\n",
    "        df[f'{cross_name}_bull_length'] = crosses_dict[cross_name]['bull_length']\n",
    "        df[f'{cross_name}_bear_length'] = crosses_dict[cross_name]['bear_length']\n",
    "        \n",
    "        if include_most_recent_feats:\n",
    "            df[f'{cross_name}_most_recent_bull_strength'] = crosses_dict[cross_name]['most_recent_bull_strength']\n",
    "            df[f'{cross_name}_most_recent_bear_strength'] = crosses_dict[cross_name]['most_recent_bear_strength']\n",
    "            df[f'{cross_name}_ticks_since_bull'] = crosses_dict[cross_name]['ticks_since_bull']\n",
    "            df[f'{cross_name}_ticks_since_bear'] = crosses_dict[cross_name]['ticks_since_bear']\n",
    "            df[f'{cross_name}_most_recent_bull_length'] = crosses_dict[cross_name]['most_recent_bull_length']\n",
    "            df[f'{cross_name}_most_recent_bear_length'] = crosses_dict[cross_name]['most_recent_bear_length']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatuteGenerator:\n",
    "    def __init__(self, data, feature_indices):\n",
    "        self.data = data\n",
    "        self.feature_indices = feature_indices\n",
    "#         self.rsi_divergence_range = 30\n",
    "        self.last_rsi_divergence = 0 # 0 - None, 1 - bearish, 2 -  hidden bearish, 3 - bullish, 4 - hidden bullish\n",
    "        self.rsi_highs = deque()\n",
    "        self.rsi_lows = deque()\n",
    "        self.cross_lengths = {}\n",
    "        self.price_entered_tk_region_from_top = False\n",
    "        self.price_entered_tk_region_from_bot = False\n",
    "        self.temporal_features = namedtuple('temporal_features', 'quarter year month day day_of_week hour minute')\n",
    "        self.safe_start_idx = self._end_of_missing_data_idx(['chikou_span_visual'])\n",
    "    \n",
    "    def get_temporal_features(self,i):\n",
    "        dt = self.data[i][self.feature_indices['datetime']]\n",
    "        \n",
    "        features = self.temporal_features(quarter=dt.quarter, year=dt.year, month=dt.month, day=dt.day,\n",
    "                                          day_of_week=dt.dayofweek, hour=dt.hour, minute=dt.minute)\n",
    "        return features\n",
    "    \n",
    "#     def check_rsi_divergence(self,index):\n",
    "#         momentum_rsi_i = self.feature_indices['momentum_rsi']\n",
    "#         if not pd.isna(self.data[index][momentum_rsi_i]):\n",
    "#             rsi1 = self.data[index][momentum_rsi_i]\n",
    "#             rsi2 = self.data[index-1][momentum_rsi_i]\n",
    "#             rsi3 = self.data[index-2][momentum_rsi_i]\n",
    "#             if rsi1 < rsi2 and rsi3 < rsi2:\n",
    "#                 self.rsi_highs.appendleft((index-1,rsi2))\n",
    "#                 for high in self.rsi_highs:\n",
    "#                     if high[0] == index-1:\n",
    "#                         continue\n",
    "                    \n",
    "#             elif rsi1 > rsi2 and rsi3 > rsi2:\n",
    "#                 self.rsi_lows.appendleft((index-1,rsi2))\n",
    "\n",
    "#             if len(self.rsi_highs) > 0:\n",
    "#                 if self.rsi_highs[0][0] < index - self.rsi_divergence_range:\n",
    "#                     self.rsi_highs.pop()\n",
    "#             if len(self.rsi_lows) > 0:\n",
    "#                 if self.rsi_lows[0][0] < index - self.rsi_divergence_range:\n",
    "#                     self.rsi_lows.pop()           \n",
    "    \n",
    "    def get_ichimoku_features(self, i, cross_length_limit = 1):\n",
    "        is_price_above_cb_lines = None\n",
    "        is_price_above_cloud = None\n",
    "        is_price_inside_cloud = None\n",
    "        is_price_below_cloud = None\n",
    "        cloud_top = None\n",
    "        cloud_bottom = None\n",
    "\n",
    "        # cross signals represented as tuples: (bullish strength, bearish strength, cross length)\n",
    "        # - cross signal strength indicated by 0, 1, 2, 3 for none, weak, neutral, strong\n",
    "        #    or just 0, 1, 3 for none, weak, strong\n",
    "        # - cross length is just the number of ticks the cross occured over\n",
    "        tk_cross = (0,0,0)\n",
    "        tk_price_cross = (0,0,0)\n",
    "        senkou_cross = (0,0,0)\n",
    "        chikou_cross = (0,0,0)\n",
    "        cloud_breakout_bull = False\n",
    "        cloud_breakout_bear = False\n",
    "        \n",
    "        close = self.feature_indices['Close']\n",
    "        trend_visual_ichimoku_a = self.feature_indices['trend_visual_ichimoku_a']\n",
    "        trend_visual_ichimoku_b = self.feature_indices['trend_visual_ichimoku_b']\n",
    "        trend_ichimoku_a = self.feature_indices['trend_ichimoku_a']\n",
    "        trend_ichimoku_b = self.feature_indices['trend_ichimoku_b']\n",
    "        trend_ichimoku_conv = self.feature_indices['trend_ichimoku_conv']\n",
    "        trend_ichimoku_base = self.feature_indices['trend_ichimoku_base']\n",
    "        chikou_span = self.feature_indices['chikou_span']\n",
    "        \n",
    "        cloud_top, cloud_bottom = self._get_top_and_bottom_line_idx(trend_visual_ichimoku_a,trend_visual_ichimoku_b,i)\n",
    "\n",
    "        if not pd.isna(self.data[i][trend_ichimoku_conv]) and not pd.isna(self.data[i][trend_ichimoku_base]):\n",
    "            if self.data[i][close] > self.data[i][trend_ichimoku_conv] and self.data[i][close] > self.data[i][trend_ichimoku_base]:\n",
    "                is_price_above_cb_lines = True\n",
    "            else:\n",
    "                is_price_above_cb_lines = False\n",
    "            \n",
    "            if self._is_line_between_region(close, cloud_top, cloud_bottom, i):\n",
    "                is_price_inside_cloud = True\n",
    "                is_price_above_cloud = False\n",
    "                is_price_below_cloud = False\n",
    "            else:\n",
    "                is_price_inside_cloud = False\n",
    "                if self.data[i][close] <= self.data[i][cloud_bottom]:\n",
    "                    is_price_above_cloud = False\n",
    "                    is_price_below_cloud = True\n",
    "                else:\n",
    "                    is_price_above_cloud = True\n",
    "                    is_price_below_cloud = False\n",
    "        \n",
    "        ### check for crosses\n",
    "        \n",
    "        if i >= self.safe_start_idx:\n",
    "            \n",
    "            ### tk cross\n",
    "            \n",
    "            cross, length, top_line_i, bottom_line_i = \\\n",
    "                self._get_cross_and_length('tk_cross', trend_ichimoku_conv,trend_ichimoku_base,i)\n",
    "            \n",
    "            # price cross clean through both tk region (cross == 2), or price cross through both \n",
    "            # tk region over limited amout of ticks (cross == 3 and length <= cross_length_limit)\n",
    "            if cross == 2 \\\n",
    "                    or (cross == 3 and length <= cross_length_limit):\n",
    "                \n",
    "                # bullish\n",
    "                if top_line_i == trend_ichimoku_conv:\n",
    "                    if self._is_line_between_region(top_line_i,cloud_top,cloud_bottom,i) \\\n",
    "                            and self._is_line_between_region(bottom_line_i,cloud_top,cloud_bottom,i):\n",
    "                        tk_cross = (2,0,length)\n",
    "                    elif self.data[i][bottom_line_i] >= self.data[i][cloud_top]:\n",
    "                        tk_cross = (3,0,length)\n",
    "                    else:\n",
    "                        tk_cross = (1,0,length)\n",
    "                # bearish\n",
    "                elif top_line_i == trend_ichimoku_base:\n",
    "                    if self._is_line_between_region(top_line_i,cloud_top,cloud_bottom,i) \\\n",
    "                            and self._is_line_between_region(bottom_line_i,cloud_top,cloud_bottom,i):\n",
    "                        tk_cross = (0,2,length)\n",
    "                    elif self.data[i][top_line_i] <= self.data[i][cloud_bottom]:\n",
    "                        tk_cross = (0,3,length)\n",
    "                    else:\n",
    "                        tk_cross = (0,1,length)\n",
    "                else:\n",
    "                    print('weird 5:', self.data[i][self.feature_indices['datetime']])\n",
    "                \n",
    "            ### tk price cross\n",
    "            \n",
    "            cross_res = self._get_cross_and_length_regions('tk_price_cross', trend_ichimoku_conv, trend_ichimoku_base,\n",
    "                                                            close, close, i)\n",
    "            cross, length, first_line, second_line, third_line, fourth_line = cross_res\n",
    "            \n",
    "            if cross == 2 or (cross == 3 and length <= cross_length_limit):\n",
    "                \n",
    "                # \"Itâ€™s a noise zone when price is in the Cloud\"\n",
    "                #  https://www.tradeciety.com/the-complete-ichimoku-trading-guide-how-to-use-the-ichimoku-indicator/\n",
    "                \n",
    "                # bullish \n",
    "                if first_line == close:\n",
    "                    if self.data[i][close] >= self.data[i][cloud_top]:\n",
    "                        tk_price_cross = (3,0,length)\n",
    "                    elif self.data[i][close] <= self.data[i][cloud_bottom]:\n",
    "                        tk_price_cross = (1,0,length)\n",
    "                # bearish\n",
    "                elif fourth_line == close:\n",
    "                    if self.data[i][close] >= self.data[i][cloud_top]:\n",
    "                        tk_price_cross = (0,1,length)\n",
    "                    elif self.data[i][close] <= self.data[i][cloud_bottom]:\n",
    "                        tk_price_cross = (0,3,length)\n",
    "            elif cross == 3 and length>cross_length_limit:\n",
    "                print(f'cross type = {cross}, cross length = {length}, {self.data[i][self.feature_indices[\"datetime\"]]}')\n",
    "            \n",
    "            ### cloud (senkou) cross\n",
    "            \n",
    "            # As the Senkou Spans are projected forward, the cross that triggers this signal will be 26 days ahead of the \n",
    "            # price and, hence, the actual date that the signal occurs.  The strength of the signal is determined by the \n",
    "            # relationship of the price on the date of the signal (not the trigger) to the Kumo (Cloud)\n",
    "            # - https://www.ichimokutrader.com/signals.html\n",
    "            \n",
    "            cross, length, top_line_i, bottom_line_i = \\\n",
    "                self._get_cross_and_length('cloud_cross', trend_ichimoku_a, trend_ichimoku_b,i)\n",
    "            \n",
    "            if cross == 2 \\\n",
    "                    or (cross == 3 and length <= cross_length_limit):\n",
    "                \n",
    "                # bullish\n",
    "                if top_line_i == trend_ichimoku_a:\n",
    "                    if self._is_line_between_region(close,cloud_top,cloud_bottom,i):\n",
    "                        senkou_cross = (2,0,length)\n",
    "                    elif self.data[i][close] >= self.data[i][cloud_top]:\n",
    "                        senkou_cross = (3,0,length)\n",
    "                    else:\n",
    "                        senkou_cross = (1,0,length)\n",
    "                # bearish\n",
    "                elif top_line_i == trend_ichimoku_b:\n",
    "                    if self._is_line_between_region(close,cloud_top,cloud_bottom,i):\n",
    "                        senkou_cross = (0,2,length)\n",
    "                    elif self.data[i][close] <= self.data[i][cloud_bottom]:\n",
    "                        senkou_cross = (0,3,length)\n",
    "                    else:\n",
    "                        senkou_cross = (0,1,length)\n",
    "                else:\n",
    "                    print('weird 55:', self.data[i][self.feature_indices['datetime']])\n",
    "                \n",
    "            ### chikou span cross\n",
    "\n",
    "            # Note (1) that the Chikou Span must be rising when it crosses to above the price for a bull signal \n",
    "            # and falling when it crosses to below for a bear signal; just crossing the price alone is not \n",
    "            # sufficient to trigger the signal. (2) As the Chikou Span is the closing price shifted into the past, \n",
    "            # the cross that triggers this signal will be 26 days behind the price and, hence, the actual date \n",
    "            # that the signal occurs.The strength of the signal is determined by the relationship of the price \n",
    "            # on the date of the signal (not the trigger) to the Kumo (Cloud).\n",
    "            # - https://www.ichimokutrader.com/signals.html\n",
    "            \n",
    "            # remember the chikou_span at this point is just the price 26 (or whatever chikou/senkou projection is) ticks ago\n",
    "            cross, length, top_line_i, bottom_line_i = \\\n",
    "                self._get_cross_and_length('chikou_cross', chikou_span, close, i)\n",
    "            \n",
    "            if cross == 2 \\\n",
    "                    or (cross == 3 and length <= cross_length_limit):\n",
    "                # bullish\n",
    "                if top_line_i == close:\n",
    "                    if self._is_line_between_region(close, cloud_top, cloud_bottom, i):\n",
    "                        chikou_cross = (2,0,length)\n",
    "                    elif self.data[i][close] > self.data[i][cloud_top]:\n",
    "                        chikou_cross = (3,0,length)\n",
    "                    else:\n",
    "                        chikou_cross = (1,0,length)\n",
    "                # bearish\n",
    "                elif top_line_i == chikou_span:\n",
    "                    if self._is_line_between_region(close, cloud_top, cloud_bottom, i):\n",
    "                        chikou_cross = (0,2,length)\n",
    "                    elif self.data[i][close] < self.data[i][cloud_bottom]:\n",
    "                        chikou_cross = (0,3,length)\n",
    "                    else:\n",
    "                        chikou_cross = (0,1,length)\n",
    "                else:\n",
    "                    print('weird 6:', self.data[i][self.feature_indices['datetime']])\n",
    "            \n",
    "            ### kumo (cloud) breakout\n",
    "            \n",
    "            cross_res = self._get_cross_and_length_regions('kumo_breakout', cloud_top, cloud_bottom,\n",
    "                                                            close, close, i)\n",
    "            cross, length, first_line, second_line, third_line, fourth_line = cross_res\n",
    "            \n",
    "            # The Kumo Breakout signal occurs when the price leaves or crosses the Kumo (Cloud), which is why\n",
    "            # we also want to check for if cross == 4 (end of overlap but not a cross)\n",
    "            # - https://www.ichimokutrader.com/signals.html\n",
    "            if cross == 2 or cross == 3 or cross == 4:\n",
    "                # bullish \n",
    "                if first_line == close:\n",
    "                    cloud_breakout_bull = True\n",
    "                # bearish\n",
    "                elif fourth_line == close:\n",
    "                    cloud_breakout_bear = True\n",
    "        \n",
    "        features = {\n",
    "            'is_price_above_cb_lines': is_price_above_cb_lines,\n",
    "            'is_price_above_cloud': is_price_above_cloud,      \n",
    "            'is_price_inside_cloud': is_price_inside_cloud,   \n",
    "            'is_price_below_cloud': is_price_below_cloud,   \n",
    "            'cloud_top': cloud_top,   \n",
    "            'cloud_bottom': cloud_bottom,   \n",
    "            'tk_cross': tk_cross,   \n",
    "            'tk_price_cross': tk_price_cross,   \n",
    "            'senkou_cross': senkou_cross,   \n",
    "            'chikou_cross': chikou_cross,   \n",
    "            'cloud_breakout_bull': cloud_breakout_bull,\n",
    "            'cloud_breakout_bear': cloud_breakout_bear\n",
    "        }\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _end_of_missing_data_idx(self, exluded_features):\n",
    "        exluded_features = set(exluded_features)\n",
    "        safe_idx = None\n",
    "\n",
    "        for i in range(len(self.data)):\n",
    "            nan_in_row = False\n",
    "            \n",
    "            for feature in self.feature_indices:\n",
    "                if feature in exluded_features:\n",
    "                    continue\n",
    "                    \n",
    "                feature_i = self.feature_indices[feature]\n",
    "                if isinstance(self.data[i][feature_i], float) and np.isnan(self.data[i][feature_i]):\n",
    "                    safe_idx = None\n",
    "                    nan_in_row = True\n",
    "                    break\n",
    "            \n",
    "            if not nan_in_row and not safe_idx:\n",
    "                safe_idx = i\n",
    "        \n",
    "        # add 1 because we are looking for crosses and need to look back one tick in order to do so\n",
    "        return safe_idx + 1\n",
    "    \n",
    "    def _get_top_and_bottom_line_idx(self,line1_i,line2_i,i):\n",
    "        \"\"\"\n",
    "        line1_i is top if line values are equal\n",
    "        \"\"\"\n",
    "        top_line_i = line1_i\n",
    "        bottom_line_i = line2_i\n",
    "        if self.data[i][line1_i] < self.data[i][line2_i]:\n",
    "            top_line_i = line2_i\n",
    "            bottom_line_i = line1_i\n",
    "        return top_line_i, bottom_line_i\n",
    "    \n",
    "    def _is_line_between_region(self,target_line_i,top_line_i,bottom_line_i,i):\n",
    "        if self.data[i][target_line_i] > self.data[i][bottom_line_i] \\\n",
    "            and self.data[i][target_line_i] < self.data[i][top_line_i]:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def _get_cross_and_length_regions(self, cross_name, r1_line1, r1_line2, r2_line1, r2_line2, i):\n",
    "        \"\"\"\n",
    "        cross type can be: no cross '=' (0), start of overlap '>' (1), full cross 'X' (2), end of cross '<' (3),\n",
    "            or end of overlap w/ no cross (4)\n",
    "        \"\"\"\n",
    "        \n",
    "        old_r1_top, old_r1_bot = self._get_top_and_bottom_line_idx(r1_line1,r1_line2,i-1)\n",
    "        old_r2_top, old_r2_bot = self._get_top_and_bottom_line_idx(r2_line1,r2_line2,i-1)\n",
    "        \n",
    "        r1_top, r1_bot = self._get_top_and_bottom_line_idx(r1_line1,r1_line2,i)\n",
    "        r2_top, r2_bot = self._get_top_and_bottom_line_idx(r2_line1,r2_line2,i)\n",
    "        \n",
    "        # defines lines from top to bottom between both regions\n",
    "        sorted_regions_lines = sorted([(line, self.data[i][line]) for line in [r1_top,r1_bot,r2_top,r2_bot]],\n",
    "                                     key = lambda line_tuple: line_tuple[1], reverse=True)\n",
    "        first_line, second_line, third_line, fourth_line = sorted_regions_lines\n",
    "        \n",
    "        ### check for no cross\n",
    "        \n",
    "        old_top_region_bot = None\n",
    "    \n",
    "        # region 1 is fully above region 2\n",
    "        if self.data[i-1][old_r1_bot] > self.data[i-1][old_r2_top]:\n",
    "            old_top_region_bot = old_r1_bot\n",
    "            if self.data[i][r1_bot] > self.data[i][r2_top]:\n",
    "                return 0, 0, first_line[0], second_line[0], third_line[0], fourth_line[0]\n",
    "        # region 2 is fully above region 1\n",
    "        elif self.data[i-1][old_r2_bot] > self.data[i-1][old_r1_top]:\n",
    "            old_top_region_bot = old_r2_bot\n",
    "            if self.data[i][r2_bot] > self.data[i][r1_top]:\n",
    "                return 0, 0, first_line[0], second_line[0], third_line[0], fourth_line[0]\n",
    "        \n",
    "        ### check for full cross\n",
    "        \n",
    "        # region 1 crossed to below region 2\n",
    "        if self.data[i-1][old_r1_bot] > self.data[i-1][old_r2_top] \\\n",
    "                and self.data[i][r1_top] < self.data[i][r2_bot]:\n",
    "            return 2, 0, first_line[0], second_line[0], third_line[0], fourth_line[0]\n",
    "        \n",
    "        # region 2 crossed to below region 1\n",
    "        elif self.data[i-1][old_r2_bot] > self.data[i-1][old_r1_top] \\\n",
    "                and self.data[i][r2_top] < self.data[i][r1_bot]:\n",
    "            return 2, 0, first_line[0], second_line[0], third_line[0], fourth_line[0]\n",
    "        \n",
    "        ### check for start of overlap\n",
    "        \n",
    "        top_region_top = top_region_bot = bot_region_top = bot_region_bot = None\n",
    "        # region 1 is highest\n",
    "        if self.data[i][r1_top] > self.data[i][r2_top]:\n",
    "            top_region_top = r1_top\n",
    "            top_region_bot = r1_bot\n",
    "            bot_region_top = r2_top\n",
    "            bot_region_bot = r2_bot\n",
    "        # region 2 is highest\n",
    "        else:\n",
    "            top_region_top = r2_top\n",
    "            top_region_bot = r2_bot\n",
    "            bot_region_top = r1_top\n",
    "            bot_region_bot = r1_bot\n",
    "\n",
    "        # checking for start of overlap\n",
    "        if cross_name not in self.cross_lengths:  \n",
    "            # if the bottom line of the top region is still not defined then just consider no cross \n",
    "            if not old_top_region_bot:\n",
    "                return 0, 0, first_line[0], second_line[0], third_line[0], fourth_line[0]  \n",
    "            else:\n",
    "                # one region is beginning to intertwine or completely swallow the other, regardless this counts\n",
    "                # as the start of an overlap\n",
    "                if self.data[i][bot_region_top] <= self.data[i][top_region_top] \\\n",
    "                        and self.data[i][bot_region_top] >= self.data[i][top_region_bot]:\n",
    "                    self.cross_lengths[cross_name] = (0, old_top_region_bot)\n",
    "                    return 1, 0, first_line[0], second_line[0], third_line[0], fourth_line[0]\n",
    "                print('weird 11:', self.data[i][self.feature_indices['datetime']])\n",
    "        else:\n",
    "            # check for continuation of overlap\n",
    "            if self.data[i][bot_region_top] <= self.data[i][top_region_top] \\\n",
    "                    and self.data[i][bot_region_top] >= self.data[i][top_region_bot]:\n",
    "                self.cross_lengths[cross_name] = (self.cross_lengths[cross_name][0] + 1, self.cross_lengths[cross_name][1])\n",
    "                return 0, self.cross_lengths[cross_name][0], first_line[0], second_line[0], third_line[0], fourth_line[0]\n",
    "            # otherwise, 1 region must be completely above the other\n",
    "            else:\n",
    "                original_top_region_bot = self.cross_lengths[cross_name][1]\n",
    "                res = None\n",
    "                \n",
    "                # check for end of cross\n",
    "                if original_top_region_bot != top_region_bot and original_top_region_bot != top_region_top:\n",
    "                    res = 3\n",
    "                # otherwise, end of overlap w/ no cross\n",
    "                else:\n",
    "                    res = 4\n",
    "                \n",
    "                cross_length = self.cross_lengths[cross_name][0]\n",
    "                del self.cross_lengths[cross_name]\n",
    "                return res, cross_length, first_line[0], second_line[0], third_line[0], fourth_line[0]\n",
    "\n",
    "            \n",
    "    def _get_cross_and_length(self, cross_name, line_index1, line_index2, index):\n",
    "        \"\"\"\n",
    "        cross type can be: no cross '=' (0), start of overlap '>' (1), full cross 'X' (2), end of cross '<' (3),\n",
    "            or end of overlap w/ no cross (4)\n",
    "        \"\"\"\n",
    "        \n",
    "        # remember that if lines are of equal values the first line argument to _get_top_and_bottom_line_idx()\n",
    "        # will be returned as the top line\n",
    "        old_top_line_i, old_bottom_line_i = self._get_top_and_bottom_line_idx(line_index1,line_index2,index - 1)\n",
    "        top_line_i, bottom_line_i = self._get_top_and_bottom_line_idx(line_index1,line_index2,index)\n",
    "        \n",
    "        ## check for no cross\n",
    "        \n",
    "        if self.data[index][line_index1] != self.data[index][line_index2] \\\n",
    "                and self.data[index-1][line_index1] != self.data[index-1][line_index2] \\\n",
    "                and old_top_line_i == top_line_i \\\n",
    "                and bottom_line_i == old_bottom_line_i:\n",
    "            return 0, 0, top_line_i, bottom_line_i\n",
    "\n",
    "        ## check for full cross\n",
    "        \n",
    "        if old_top_line_i != top_line_i \\\n",
    "                and self.data[index-1][old_top_line_i] > self.data[index-1][old_bottom_line_i] \\\n",
    "                and self.data[index][old_top_line_i] < self.data[index][old_bottom_line_i]:\n",
    "            return 2, 0, top_line_i, bottom_line_i\n",
    "        \n",
    "        ##check for start of overlap\n",
    "        \n",
    "        if cross_name not in self.cross_lengths:\n",
    "            if self.data[index][line_index1] == self.data[index][line_index2]:\n",
    "                self.cross_lengths[cross_name] = (0, old_top_line_i,self.data[index][self.feature_indices['datetime']])\n",
    "                return 1, 0, top_line_i, bottom_line_i\n",
    "            print('weird 1:', self.data[index][self.feature_indices['datetime']])\n",
    "        else:\n",
    "            \n",
    "            ## check for continuation of overlap\n",
    "            \n",
    "            if self.data[index][line_index1] == self.data[index][line_index2]:\n",
    "                self.cross_lengths[cross_name] = (self.cross_lengths[cross_name][0] + 1, self.cross_lengths[cross_name][1]\n",
    "                                                  ,self.cross_lengths[cross_name][2])\n",
    "                return 0, self.cross_lengths[cross_name][0], top_line_i, bottom_line_i\n",
    "            else:\n",
    "                cross_old_top_line_i = self.cross_lengths[cross_name][1]\n",
    "                res = None\n",
    "\n",
    "                ## check for end of cross\n",
    "                \n",
    "                if cross_old_top_line_i != top_line_i:\n",
    "                    res = 3\n",
    "                # otherwise, end of overlap w/ no cross\n",
    "                else:\n",
    "                    res = 4 \n",
    "\n",
    "                cross_length = self.cross_lengths[cross_name][0]\n",
    "                del self.cross_lengths[cross_name]\n",
    "                return res,cross_length, top_line_i, bottom_line_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trying loading data from mt5 terminal w/ ForexMachine package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tick_data_filepath = gi.download_mt5_data(\"EURUSD\", 'H1', '2012-01-02', '2020-06-06')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    'current_model':'ichi_cloud',\n",
    "    'ichi_cloud':{\n",
    "        'indicators': {\n",
    "            'ichimoku': {\n",
    "                'tenkan_period': 9,\n",
    "                'kijun_period': 26,\n",
    "                'chikou_period': 26,\n",
    "                'senkou_b_period': 24\n",
    "            },\n",
    "            'rsi': {\n",
    "                'periods': 14\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "data_with_indicators_2 = gi.add_indicators_to_raw(filepath=tick_data_filepath, \n",
    "                                                  save_to_disk=False, \n",
    "                                                  config=model_config, \n",
    "                                                  has_headers=True,\n",
    "                                                  datetime_col='datetime',\n",
    "                                                  file_save_name='testing1')\n",
    "data_with_ichi_2 = add_features(data_with_indicators_2)\n",
    "data_with_ichi_2.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crosses = ['tk_cross', 'tk_price_cross', 'senkou_cross', 'chikou_cross', 'kumo_breakout']\n",
    "crosses = ['kumo_breakout']\n",
    "show_data_from_range(data_with_ichi_2, '2019-01-01', '2019-02-04', main_indicator='ichimoku', sub_indicators=['rsi'], visualize_crosses=True, crosses=crosses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = gi.save_data_with_indicators(data_with_ichi_2,filename=f'ichimoku_sigs-{tick_data_filepath.stem}')\n",
    "str(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# backtest tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### high level process of using backtrader:\n",
    "\n",
    "```python\n",
    "class MyStrategy(bt.Strategy):\n",
    "    def next(self):\n",
    "        pass\n",
    "\n",
    "# Instantiate Cerebro engine\n",
    "cerebro = bt.Cerebro()\n",
    "\n",
    "# add strategy to cerebro\n",
    "cerebro.addstrategy(MyStrategy)\n",
    "\n",
    "# run cerebro engine\n",
    "cerebro.run()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import backtrader as bt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintClose(bt.Strategy):\n",
    "    def __init__(self):\n",
    "        # keep a reference to the \"close\" line in the data[0] dataseries\n",
    "        self.dataclose = self.datas[0].close\n",
    "    \n",
    "    def log(self, txt, dt=None):\n",
    "        dt = dt or self.datas[0].datetime.date(0)\n",
    "        print(f'{dt.isoformat()}, {txt}')\n",
    "    \n",
    "    def next(self):\n",
    "        # Simply log the closing price of the series from the reference\n",
    "        self.log(f'Close: {self.dataclose[0]}')\n",
    "\n",
    "class MAcrossover(bt.Strategy):\n",
    "    # Moving average parameters\n",
    "    params = (('pfast', 20), ('pslow', 50))\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.dataclose = self.datas[0].close\n",
    "        \n",
    "        # Order variable will contain ongoing order details/status\n",
    "        self.order = None\n",
    "        \n",
    "        # Instantiate moving averages\n",
    "        self.slow_sma = bt.indicators.MovingAverageSimple(self.datas[0], \n",
    "                        period=self.params.pslow)\n",
    "        self.fast_sma = bt.indicators.MovingAverageSimple(self.datas[0], \n",
    "                        period=self.params.pfast)\n",
    "    \n",
    "    def log(self, txt, dt=None):\n",
    "        dt = dt or self.datas[0].datetime.date(0)\n",
    "        print(f'{dt.isoformat()} {txt}') # Comment this line when running optimization\n",
    "        \n",
    "    def notify_order(self, order):\n",
    "        if order.status in [order.Submitted, order.Accepted]:\n",
    "            # An activate buy/sell order has been submitted/accepted - Nothing to do\n",
    "            return\n",
    "        \n",
    "        # Check if an order has been completed\n",
    "        # Attention: broker could reject order if not enough cash\n",
    "        if order.status in [order.Completed]:\n",
    "            if order.isbuy():\n",
    "                self.log(f'BUY EXECUTED, size: {order.executed.size}, price: {order.executed.price}, cost: {order.executed.value}, commision: {order.executed.comm}')\n",
    "                print(f'current balance: {self.broker.getvalue()}')\n",
    "            elif order.issell():\n",
    "                self.log(f'SELL EXECUTED, size: {order.executed.size}, price: {order.executed.price}, cost: {order.executed.value}, commision: {order.executed.comm}')\n",
    "                print(f'current balance: {self.broker.getvalue()}')\n",
    "            self.bar_executed = len(self)\n",
    "        \n",
    "        elif order.status in [order.Canceled, order.Margin, order.Rejected]:\n",
    "            self.log('Order Canceled/Margin/Rejected')\n",
    "        \n",
    "        # Reset orders\n",
    "        self.order = None\n",
    "    \n",
    "    def notify_trade(self, trade):\n",
    "        if not trade.isclosed:\n",
    "            return\n",
    "        \n",
    "        print()\n",
    "        self.log(f'OPERATION PROFIT, GROSS: {trade.pnl}, NET: {trade.pnlcomm}')\n",
    "        print(f'open dt: {trade.open_datetime()} close dt: {trade.close_datetime()}')\n",
    "        print(f'close price: {trade.price}')\n",
    "        print(f'bar opened: {trade.baropen}, bar closed: {trade.barclose}')\n",
    "        print(f'number of bars trade was open for: {trade.barlen}')\n",
    "        print(f'current balance: {self.broker.getvalue()}')\n",
    "        print()\n",
    "    \n",
    "    def next(self):\n",
    "        # Check if an order is pending ... if yes, we cannot send a 2nd one\n",
    "        if self.order:\n",
    "#             print(f'order value: {order.value}')\n",
    "#             print(f'order pprice: {order.pprice}')\n",
    "#             print(f'order psize: {order.psize}')\n",
    "#             print(f'order pnl: {order.pnl}')\n",
    "            return\n",
    "        \n",
    "        # Check if we are in the market\n",
    "        if not self.position:\n",
    "            # We are not in the market, look for a signal to OPEN trades\n",
    "            \n",
    "            #If the 20 SMA is above the 50 SMA\n",
    "            if self.fast_sma[0] > self.slow_sma[0] and self.fast_sma[-1] < self.slow_sma[-1]:\n",
    "                self.log(f'BUY CREATE {self.dataclose[0]}')\n",
    "                # Keep track of the created order to avoid a 2nd order\n",
    "                self.order = self.buy()\n",
    "            #Otherwise if the 20 SMA is below the 50 SMA\n",
    "            elif self.fast_sma[0] < self.slow_sma[0] and self.fast_sma[-1] > self.slow_sma[-1]:\n",
    "                self.log(f'SELL CREATE {self.dataclose[0]}')\n",
    "                # Keep track of the created order to avoid a 2nd order\n",
    "                self.order = self.sell()\n",
    "        else:\n",
    "            # We are already in the market, look for a signal to CLOSE trades\n",
    "            if len(self) >= (self.bar_executed + 5):\n",
    "                self.log(f'CLOSE CREATE {self.dataclose[0]}')\n",
    "                self.order = self.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Instantiate Cerebro engine\n",
    "cerebro = bt.Cerebro()\n",
    "\n",
    "total_time_delta = data_with_ichi_2.iloc[-1,0] - data_with_ichi_2.iloc[0,0]\n",
    "from1, to1 = data_with_ichi_2.iloc[0,0], data_with_ichi_2.iloc[0,0] + total_time_delta / 2\n",
    "from2, to2 = data_with_ichi_2.iloc[0,0] + total_time_delta / 2, data_with_ichi_2.iloc[-1,0]\n",
    "\n",
    "forex_data1 = bt.feeds.GenericCSVData(\n",
    "    dataname=str(filepath),\n",
    "    datetime=0,\n",
    "    open=1,\n",
    "    high=2,\n",
    "    low=3,\n",
    "    close=4,\n",
    "    volume=-1,\n",
    "    openinterest=-1,\n",
    "    fromdate=from1.to_pydatetime(),\n",
    "    todate=to1.to_pydatetime(),\n",
    "    timeframe=bt.TimeFrame.Minutes\n",
    ")\n",
    "\n",
    "forex_data2 = bt.feeds.GenericCSVData(\n",
    "    dataname=str(filepath),\n",
    "    datetime=0,\n",
    "    open=1,\n",
    "    high=2,\n",
    "    low=3,\n",
    "    close=4,\n",
    "    volume=-1,\n",
    "    openinterest=-1,\n",
    "    fromdate=from2.to_pydatetime(),\n",
    "    todate=to2.to_pydatetime(),\n",
    "    timeframe=bt.TimeFrame.Minutes\n",
    ")\n",
    "\n",
    "# add data to cerebro to read over\n",
    "cerebro.adddata(forex_data1)\n",
    "\n",
    "# add strategy to cerebro\n",
    "cerebro.addstrategy(MAcrossover)\n",
    "\n",
    "# Default position size\n",
    "cerebro.addsizer(bt.sizers.SizerFix, stake=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_balance = cerebro.broker.getvalue()\n",
    "\n",
    "cerebro.run()\n",
    "\n",
    "end_balance = cerebro.broker.getvalue()\n",
    "\n",
    "pnl = end_balance - start_balance\n",
    "print(f'Starting Portfolio Value: {start_balance}')\n",
    "print(f'Final Portfolio Value: {end_balance}')\n",
    "print(f'PnL: {pnl}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# helper preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disregard_rows_with_missing_data(x_df, y_df=None, ignored_x_cols=None, ignored_y_cols=None, seperate_chunks=False):\n",
    "    if y_df is not None:\n",
    "        if x_df.shape[0] != y_df.shape[0]:\n",
    "            print(f'x_df (rows={x_df.shape[0]}) and y_df (rows={y_df.shape[0]}) do not have the same number of rows')\n",
    "            return\n",
    "    \n",
    "    if ignored_x_cols:\n",
    "        ignored_x_cols = set([x_df.columns.get_loc(col_name) for col_name in ignored_x_cols])\n",
    "    else:\n",
    "        ignored_x_cols = set()\n",
    "        \n",
    "    if y_df is not None:\n",
    "        if ignored_y_cols:\n",
    "            ignored_y_cols = set([y_df.columns.get_loc(col_name) for col_name in ignored_y_cols])\n",
    "        else:\n",
    "            ignored_y_cols = set()\n",
    "        \n",
    "    wanted_data = []\n",
    "    cur_x_data = []\n",
    "    cur_y_data = []\n",
    "    \n",
    "    x_data = x_df.to_numpy()\n",
    "    if y_df is not None:\n",
    "        y_data = y_df.to_numpy()    \n",
    "    \n",
    "    for i in range(len(x_data)):\n",
    "        missing_data = False\n",
    "        for j in range(len(x_data[i])):\n",
    "            if j not in ignored_x_cols and pd.isnull(x_data[i][j]):\n",
    "                missing_data = True\n",
    "                break\n",
    "        \n",
    "        if y_df is not None and not missing_data:\n",
    "            for j in range(len(y_data[i])):\n",
    "                if j not in ignored_y_cols and pd.isnull(y_data[i][j]):\n",
    "                    missing_data = True\n",
    "                    break\n",
    "        \n",
    "        if not missing_data:\n",
    "            cur_x_data.append(x_data[i])\n",
    "            if y_df is not None:\n",
    "                cur_y_data.append(y_data[i])\n",
    "        elif seperate_chunks and len(cur_x_data) > 0:\n",
    "            if y_df is not None:\n",
    "                wanted_data.append((pd.DataFrame(cur_x_data, columns=x_df.columns), pd.DataFrame(cur_y_data, columns=y_df.columns)))\n",
    "            else:\n",
    "                wanted_data.append((pd.DataFrame(cur_x_data, columns=x_df.columns), None))\n",
    "            cur_x_data = []\n",
    "            cur_y_data = []\n",
    "    \n",
    "    if len(cur_x_data) > 0:\n",
    "        if y_df is not None:\n",
    "            wanted_data.append((pd.DataFrame(cur_x_data, columns=x_df.columns), pd.DataFrame(cur_y_data, columns=y_df.columns)))\n",
    "        else:\n",
    "            wanted_data.append((pd.DataFrame(cur_x_data, columns=x_df.columns), None))\n",
    "    \n",
    "    return wanted_data\n",
    "\n",
    "def dummy_and_remove_features(data_df, categories_dict={}, cols_to_remove=[], include_defaults=True, keep_datetime=False):\n",
    "    if include_defaults:\n",
    "        cd = {\n",
    "            'quarter': [1,2,3,4],\n",
    "            'day_of_week': [0,1,2,3,4]\n",
    "        }\n",
    "        \n",
    "        cols = {'spread','momentum_rsi', 'month', 'day', 'minute', 'hour', 'year', 'chikou_span_visual','chikou_span', \n",
    "                'tk_cross_bull_length', 'tk_cross_bear_length', \n",
    "                'tk_price_cross_bull_length', 'tk_price_cross_bear_length', \n",
    "                'senkou_cross_bull_length', 'senkou_cross_bear_length', \n",
    "                'chikou_cross_bull_length', 'chikou_cross_bear_length',\n",
    "                'trend_visual_ichimoku_a','trend_visual_ichimoku_b',}\n",
    "                #'trend_ichimoku_base','trend_ichimoku_conv', 'trend_ichimoku_a', 'trend_ichimoku_b'}\n",
    "        \n",
    "        if not keep_datetime:\n",
    "            cols.add('datetime')\n",
    "        \n",
    "        cd.update(categories_dict) \n",
    "        categories_dict = cd\n",
    "        cols_to_remove = set(cols_to_remove) | cols     # prios keys/vals from 2nd arg of | (or) operand\n",
    "        data_df_cols_set = set(data_df.columns)\n",
    "        \n",
    "        categories_dict = {key: categories_dict[key] for key in categories_dict if key in data_df_cols_set}\n",
    "        cols_to_remove = [col for col in cols_to_remove if col in data_df_cols_set]\n",
    "    \n",
    "    if len(categories_dict) > 0:\n",
    "        catagorical_cols = list(categories_dict.keys())\n",
    "        categories = list(categories_dict.values())\n",
    "\n",
    "        cols_to_dummy = data_df[catagorical_cols]\n",
    "        cols_to_dummy_vals = cols_to_dummy.to_numpy()  \n",
    "\n",
    "        dummy_enc = OneHotEncoder(categories=categories, drop='first')\n",
    "        dummied_vals = dummy_enc.fit_transform(cols_to_dummy_vals).toarray()\n",
    "        dummy_col_names = dummy_enc.get_feature_names(catagorical_cols)\n",
    "\n",
    "        dummied_cols_df = pd.DataFrame(dummied_vals, columns=dummy_col_names, index=data_df.index)\n",
    "        data_df = pd.concat((data_df, dummied_cols_df), axis=1)\n",
    "\n",
    "        cols_to_remove.extend(catagorical_cols)\n",
    "        \n",
    "    data_df = data_df.drop(cols_to_remove, axis=1)\n",
    "    \n",
    "    return data_df\n",
    "\n",
    "def convert_class_labels(y_df, to_ints=True, labels_dict=None, to_numpy=False):\n",
    "    if to_ints:\n",
    "        if labels_dict is None:\n",
    "            unique_labels = np.unique(y_df.to_numpy())\n",
    "            labels_to_int = {unique_labels[i]: i  for i in range(len(unique_labels))}\n",
    "        else:\n",
    "            labels_to_int = {labels_dict[i]: i for i in labels_dict}\n",
    "        \n",
    "        new_labels = []\n",
    "        for v in y_df.to_numpy():\n",
    "            v = v[0]\n",
    "            new_labels.append(labels_to_int[v])\n",
    "        \n",
    "        if not to_numpy:\n",
    "            new_labels = pd.DataFrame(new_labels, columns=y_df.columns)\n",
    "        else:\n",
    "            new_labels = np.array(new_labels)\n",
    "        \n",
    "        if labels_dict is None:\n",
    "            labels_dict = {labels_to_int[label]: label for label in labels_to_int}\n",
    "            \n",
    "        return new_labels, labels_dict\n",
    "    else:\n",
    "        new_labels = []\n",
    "        for v in y_df.to_numpy():\n",
    "            v = v[0]\n",
    "            new_labels.append(labels_dict[v])\n",
    "            \n",
    "        if not to_numpy:\n",
    "            new_labels = pd.DataFrame(new_labels, columns=y_df.columns)\n",
    "        else:\n",
    "            new_labels = np.array(new_labels)\n",
    "        return new_labels, labels_dict\n",
    "\n",
    "def error_rate(y_true_df, y_pred_df):\n",
    "    if y_true_df.shape[0] != y_pred_df.shape[0]:\n",
    "        print(f'y_true_df (rows={y_true_df.shape[0]}) and y_pred_df (rows={y_pred_df.shape[0]}) do not have the same number of rows')\n",
    "        return\n",
    "    d1, d2 = y_true_df.to_numpy(), y_pred_df.to_numpy()\n",
    "    wrong_indices = []\n",
    "    for i in range(len(d1)):\n",
    "        if d1[i] != d2[i]:\n",
    "            wrong_indices.append(i)\n",
    "    return len(wrong_indices)/len(d1), wrong_indices\n",
    "\n",
    "def no_missing_data_idx_range(df, early_ending_cols=[]):\n",
    "    early_ending_cols = [df.columns.get_loc(col_name) for col_name in early_ending_cols]\n",
    "    early_ending_cols = set(early_ending_cols)\n",
    "    data = df.to_numpy()\n",
    "    start_idx = None\n",
    "    end_idx = None\n",
    "    for i in range(len(data)):\n",
    "        missing_data = False\n",
    "        for j in range(len(data[i])):\n",
    "            if j not in early_ending_cols and pd.isnull(data[i][j]):\n",
    "                missing_data=True\n",
    "                start_idx = None\n",
    "                end_idx = None\n",
    "                break\n",
    "            elif j in early_ending_cols and pd.isnull(data[i][j]):\n",
    "                if start_idx and not end_idx:\n",
    "                    end_idx = i-1\n",
    "        if not start_idx and not missing_data:\n",
    "            start_idx = i\n",
    "    if not end_idx:\n",
    "        end_idx = len(data) - 1\n",
    "    return start_idx, end_idx\n",
    "\n",
    "def normalize_data(df, train_data, groups=None, normalization_terms=None):\n",
    "    df = df.copy(deep=True)\n",
    "    \n",
    "    if train_data:\n",
    "        if not groups:\n",
    "            print(f'groups must be specified if train_data is true')\n",
    "            return None\n",
    "        \n",
    "        normalization_terms = {}\n",
    "        normalized = set()\n",
    "        \n",
    "        for group in groups:\n",
    "            min_value = min(df[group].min())\n",
    "            max_value = max(df[group].max())\n",
    "            dict_val = (min_value, max_value)\n",
    "            \n",
    "            for col in group:\n",
    "                df[col] = (df[col] - min_value) / (max_value - min_value)\n",
    "                \n",
    "                normalization_terms[col] = dict_val\n",
    "                normalization_terms[df.columns.get_loc(col)] = dict_val\n",
    "\n",
    "            normalized = normalized.union(group)\n",
    "            \n",
    "        for col in df:\n",
    "            if col not in normalized and df[col].dtype != bool and pd.api.types.is_numeric_dtype(df[col].dtype):\n",
    "                min_value = df[col].min()\n",
    "                max_value = df[col].max()\n",
    "                dict_val = (min_value, max_value)\n",
    "                \n",
    "                if min_value != max_value:\n",
    "                    df[col] = (df[col] - min_value) / (max_value - min_value)\n",
    "                elif min_value > 1 or min_value < 0:\n",
    "                    df[col] = [0] * df.shape[0]\n",
    "                \n",
    "                normalization_terms[col] = dict_val\n",
    "                normalization_terms[df.columns.get_loc(col)] = dict_val\n",
    "                \n",
    "    else:\n",
    "        if not normalization_terms:\n",
    "            print(f'normalization_terms must be specified if train_data is false')\n",
    "            return None\n",
    "        \n",
    "        for col in df:\n",
    "            if col in normalization_terms:\n",
    "                min_value, max_value = normalization_terms[col]\n",
    "                \n",
    "                if min_value != max_value:\n",
    "                    df[col] = (df[col] - min_value) / (max_value - min_value)\n",
    "                elif min_value > 1 or min_value < 0:\n",
    "                    df[col] = [0] * df.shape[0]\n",
    "                \n",
    "    return df, normalization_terms\n",
    "\n",
    "def normalize_data_list(row, normalization_terms):\n",
    "    new_row = []\n",
    "    for col_i in range(len(row)):\n",
    "        if col_i in normalization_terms:\n",
    "            min_value, max_value = normalization_terms[col_i]\n",
    "            \n",
    "            if min_value != max_value:\n",
    "                normalized = (row[col_i] - min_value) / (max_value - min_value)\n",
    "            elif min_value > 1 or min_value < 0:\n",
    "                normalized = 0\n",
    "            \n",
    "            new_row.append(normalized)\n",
    "        else:\n",
    "            new_row.append(row[col_i])\n",
    "    return new_row\n",
    "                \n",
    "def apply_perc_change(df, cols, limit=None):\n",
    "    df = df.copy(deep=True)\n",
    "    for col in cols:\n",
    "        df[col] = df[col].pct_change(limit=limit)\n",
    "    return df\n",
    "\n",
    "def apply_perc_change_list(last_row, cur_row, cols_set):\n",
    "    new_row = []\n",
    "    for col in range(len(last_row)):\n",
    "        if col in cols_set:\n",
    "            pc = (cur_row[col]/last_row[col]) - 1\n",
    "            new_row.append(pc)\n",
    "        else:\n",
    "            new_row.append(cur_row[col])\n",
    "    return new_row\n",
    "\n",
    "def apply_moving_avg(df, cols, window):\n",
    "    df = df.copy(deep=True)\n",
    "    df[cols] = df[cols].rolling(window).mean()\n",
    "    return df\n",
    "\n",
    "def apply_moving_avg_q(q, cols_set):\n",
    "    n_rows, n_cols = len(q), len(q[0])\n",
    "    new_row = []\n",
    "    for col in range(n_cols):\n",
    "        if col in cols_set:\n",
    "            avg = sum([row[col] for row in q]) / n_rows\n",
    "            new_row.append(avg)\n",
    "        else:\n",
    "            new_row.append(q[-1][col])\n",
    "    return new_row\n",
    "\n",
    "def missing_labels_preprocess(x_df, y_df, y_col):\n",
    "    x_df = dummy_and_remove_features(x_df)\n",
    "    if y_df is not None:\n",
    "        res = disregard_rows_with_missing_data(x_df, pd.DataFrame(y_df[y_col]))\n",
    "    else:\n",
    "        res = disregard_rows_with_missing_data(x_df, None)\n",
    "    x, y = res[0]\n",
    "    return x, y\n",
    "\n",
    "def potention_profits(decisons_true, decisons_pred, decisons_true_profits):\n",
    "    if decisons_true.shape[0] != decisons_pred.shape[0] != decisons_true_profits.shape[0]:\n",
    "        print(f'decisons_true (rows={decisons_true.shape[0]}), decisons_pred (rows={decisons_pred.shape[0]}), and '\n",
    "              f'decisons_true_profits (rows={decisons_true_profits.shape[0]}) do not have the same number of rows')\n",
    "        return\n",
    "    d1, d2, d3 = decisons_true.to_numpy(), decisons_pred.to_numpy(), decisons_true_profits.to_numpy()\n",
    "    potential_profits = 0\n",
    "    for i in range(len(d1)):\n",
    "        if d1[i][0] == d2[i][0]:\n",
    "             potential_profits += d3[i][0]\n",
    "    return potential_profits\n",
    "\n",
    "def get_profit(close_price, open_price, pip_value, pip_resolution, in_quote_currency):\n",
    "    pips = (close_price - open_price) / pip_resolution\n",
    "    # calculates profit in the quote currency (right side currency of currency pair) by default\n",
    "    profit = pip_value * pips  # can be negative\n",
    "    if not in_quote_currency:\n",
    "        profit /= close_price\n",
    "    return profit\n",
    "\n",
    "# reference: https://www.mql5.com/en/articles/4830\n",
    "def get_margin(trades, buy_label, sell_label, contract_size, leverage, tradersway_commodity, in_quote_currency, hedged_margin, trade_indices=None):\n",
    "    # *_trade_tups: (lots, open price)\n",
    "    \n",
    "    buy_lots = 0\n",
    "    sell_lots = 0\n",
    "    hedged_volume_margin = 0\n",
    "    uncovered_volume_margin = 0\n",
    "    \n",
    "    multiplier = 1\n",
    "    if contract_size > hedged_margin:\n",
    "        multiplier = contract_size / hedged_margin \n",
    "    \n",
    "    if in_quote_currency:\n",
    "        buy_price_lots = 0\n",
    "        sell_price_lots = 0\n",
    "        \n",
    "        if trade_indices is not None:\n",
    "            for trade_i in trade_indices:\n",
    "                trade = trades[trade_i]\n",
    "                decision_label = trade['decision_label']\n",
    "                if decision_label == buy_label:\n",
    "                    buy_lots += trade['lots']\n",
    "                    buy_price_lots += trade['lots'] * trade['open_price']\n",
    "                elif decision_label == sell_label:\n",
    "                    sell_lots += trade['lots']\n",
    "                    sell_price_lots += trade['lots'] * trade['open_price']\n",
    "        else:\n",
    "            for trade_i in trades:\n",
    "                trade = trades[trade_i]\n",
    "                decision_label = trade['decision_label']\n",
    "                if decision_label == buy_label:\n",
    "                    buy_lots += trade['lots']\n",
    "                    buy_price_lots += trade['lots'] * trade['open_price']\n",
    "                elif decision_label == sell_label:\n",
    "                    sell_lots += trade['lots']\n",
    "                    sell_price_lots += trade['lots'] * trade['open_price']\n",
    "\n",
    "        total_lots = buy_lots + sell_lots\n",
    "        wap = (buy_price_lots + sell_price_lots) / total_lots   # weighted average price\n",
    "\n",
    "        # calculate uncovered volume margin\n",
    "        if buy_lots > sell_lots:\n",
    "            uncovered_lots = buy_lots - sell_lots\n",
    "            uncovered_wap = buy_price_lots / buy_lots    \n",
    "            uncovered_volume_margin = uncovered_wap * uncovered_lots * contract_size / leverage\n",
    "        elif buy_lots < sell_lots:\n",
    "            uncovered_lots = sell_lots - buy_lots\n",
    "            uncovered_wap = sell_price_lots / sell_lots\n",
    "            uncovered_volume_margin = uncovered_wap * uncovered_lots * contract_size / leverage\n",
    "\n",
    "        # calculate hedged volume margin\n",
    "        hedged_volume_margin = wap * min(buy_lots, sell_lots) * contract_size / multiplier / leverage\n",
    "    else:\n",
    "        if trade_indices is not None:\n",
    "            for trade_i in trade_indices:\n",
    "                trade = trades[trade_i]\n",
    "                decision_label = trade['decision_label']\n",
    "                if decision_label == buy_label:\n",
    "                    buy_lots += trade['lots']\n",
    "                elif decision_label == sell_label:\n",
    "                    sell_lots += trade['lots']\n",
    "        else:\n",
    "            for trade_i in trades:\n",
    "                trade = trades[trade_i]\n",
    "                decision_label = trade['decision_label']\n",
    "                if decision_label == buy_label:\n",
    "                    buy_lots += trade['lots']\n",
    "                elif decision_label == sell_label:\n",
    "                    sell_lots += trade['lots']\n",
    "        \n",
    "        # calculate uncovered volume margin\n",
    "        if buy_lots > sell_lots:\n",
    "            uncovered_lots = buy_lots - sell_lots\n",
    "            uncovered_volume_margin = uncovered_lots * contract_size / leverage\n",
    "        elif buy_lots < sell_lots:\n",
    "            uncovered_lots = sell_lots - buy_lots\n",
    "            uncovered_volume_margin = uncovered_lots * contract_size / leverage\n",
    "        \n",
    "        # calculate hedged volume margin\n",
    "        hedged_volume_margin = min(buy_lots, sell_lots) * contract_size / multiplier /leverage\n",
    "\n",
    "    margin = hedged_volume_margin + uncovered_volume_margin\n",
    "    if tradersway_commodity:\n",
    "        margin *= 2   # idk\n",
    "    return margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81.49999999999999"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trades = {\n",
    "#     1: {\n",
    "#         'decision_label': 1,\n",
    "#         'lots': 0.11,\n",
    "#         'open_price': 1.22176,\n",
    "#     },\n",
    "#     2: {\n",
    "#         'decision_label': 1,\n",
    "#         'lots': 0.76,\n",
    "#         'open_price': 1.22175,\n",
    "#     },\n",
    "#     3: {\n",
    "#         'decision_label': 1,\n",
    "#         'lots': 0.14,\n",
    "#         'open_price': 1.22175,\n",
    "#     },\n",
    "#     4: {\n",
    "#         'decision_label': 0,\n",
    "#         'lots': 1.28,\n",
    "#         'open_price': 1.22169,\n",
    "#     },\n",
    "#     5: {\n",
    "#         'decision_label': 0,\n",
    "#         'lots': 0.55,\n",
    "#         'open_price': 1.22167,\n",
    "#     },\n",
    "# }\n",
    "\n",
    "# get_margin(trades, buy_label=1, sell_label=0, contract_size=100000, leverage=1000, tradersway_commodity=False, in_quote_currency=True, hedged_margin=50000)\n",
    "\n",
    "trades = {\n",
    "    1: {\n",
    "        'decision_label': 1,\n",
    "        'lots': 1.14,\n",
    "        'open_price': 1.27019,\n",
    "    },\n",
    "    2: {\n",
    "        'decision_label': 0,\n",
    "        'lots': 0.14,\n",
    "        'open_price': 1.27008,\n",
    "    },\n",
    "    3: {\n",
    "        'decision_label': 0,\n",
    "        'lots': 0.51,\n",
    "        'open_price': 1.27011,\n",
    "    },\n",
    "}\n",
    "\n",
    "get_margin(trades, buy_label=1, sell_label=0, contract_size=100000, leverage=1000, tradersway_commodity=False, in_quote_currency=False, hedged_margin=50000,\n",
    "           trade_indices=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# helper functions for generating labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ichimoku_labels(df, min_profit_percent=0.003, profit_noise_percent=0.003, label_non_signals=False, print_debug=True,\n",
    "                             signals_to_consider=None, contract_size=100_000, lots_per_trade=1, in_quote_currency=True, pip_resolution=0.0001):\n",
    "    \n",
    "    # when min_profit==profit_noise this turns into a binary classification problem (buy or sell, no wait)\n",
    "    if profit_noise_percent == min_profit_percent:\n",
    "        no_waits = True\n",
    "        \n",
    "    pip_value = contract_size * lots_per_trade * pip_resolution   # in quote currency (right side currency of currency pair)\n",
    "    min_profit = min_profit_percent * lots_per_trade * contract_size   # in base currency (left side currency of currency pair)\n",
    "    profit_noise = profit_noise_percent * lots_per_trade * contract_size   # in base currency (left side currency of currency pair) \n",
    "    \n",
    "    data = df.to_numpy()\n",
    "    feature_indices = {df.columns[i]:i for i in range(len(df.columns))}\n",
    "    close_trade_features = []\n",
    "    close_trade_labels = []\n",
    "    \n",
    "    # if any of these columns are equal to 0 then the corresponding signal has occured at that tick\n",
    "    if not signals_to_consider:\n",
    "        signals_to_consider = ['cloud_breakout_bull','cloud_breakout_bear',                       # cloud breakout\n",
    "                               'tk_cross_bull_strength', 'tk_cross_bear_strength',                # Tenkan Sen / Kijun Sen Cross\n",
    "                               'tk_price_cross_bull_strength', 'tk_price_cross_bear_strength',    # price crossing both the Tenkan Sen / Kijun Sen\n",
    "                               'senkou_cross_bull_strength', 'senkou_cross_bear_strength',        # Senkou Span Cross\n",
    "                               'chikou_cross_bull_strength', 'chikou_cross_bear_strength']        # Chikou Span Cross\n",
    "    \n",
    "    # find index at which data becomes consistant (no missing data)\n",
    "    early_ending_cols = []\n",
    "    if 'chikou_span_visual' in df.columns:\n",
    "        early_ending_cols = ['chikou_span_visual']\n",
    "    start_idx, end_idx = no_missing_data_idx_range(df, early_ending_cols=early_ending_cols)\n",
    "    \n",
    "    has_datetimes = True if 'datetime' in df.columns else False\n",
    "    \n",
    "    def get_decision_label(trade, first, current_close_price, decisions_so_far, leftover=False):\n",
    "        decision = 'first' if first else 'second'\n",
    "        label = None\n",
    "        if trade[f'{decision}_decision_best_buy_profit'][0] > trade[f'{decision}_decision_best_sell_profit'][0]:\n",
    "            # add 1 to ticks_till_peak to reserve 0 ticks for 'wait' labels\n",
    "            ticks_till_peak = trade[f'{decision}_decision_best_buy_profit'][1] - trade['trade_open_tick_i'] + 1\n",
    "            label = ['buy', ticks_till_peak, trade[f'{decision}_decision_best_buy_profit'][0], trade[f'{decision}_decision_best_buy_profit'][1]]\n",
    "        elif trade[f'{decision}_decision_best_buy_profit'][0] < trade[f'{decision}_decision_best_sell_profit'][0]:\n",
    "            ticks_till_peak = trade[f'{decision}_decision_best_sell_profit'][1] - trade['trade_open_tick_i'] + 1\n",
    "            label = ['sell', ticks_till_peak, trade[f'{decision}_decision_best_sell_profit'][0], trade[f'{decision}_decision_best_sell_profit'][1]]\n",
    "        \n",
    "        scaled_min_profit = min_profit if not in_quote_currency else min_profit / current_close_price\n",
    "        if label and label[2] < scaled_min_profit and not no_waits and not leftover:\n",
    "            label = ['wait', 0, 0, None]\n",
    "\n",
    "        if trade[f'{decision}_decision_best_buy_profit'][0] == trade[f'{decision}_decision_best_sell_profit'][0]:\n",
    "            if print_debug:\n",
    "                print(f'{decision} decision best buy and sell profit equal, trade: {trade}\\n')\n",
    "            \n",
    "            if not no_waits:\n",
    "                label = ['wait', 0, 0, None]\n",
    "            else:\n",
    "                decisions_so_far = decisions_so_far[-11:]\n",
    "                num_buys = decisions_so_far.count('buy')\n",
    "                num_sells = len(decisions_so_far) - num_buys\n",
    "                if num_buys > num_sells:\n",
    "                    ticks_till_peak = trade[f'{decision}_decision_best_buy_profit'][1] - trade['trade_open_tick_i'] + 1\n",
    "                    label = ['buy', ticks_till_peak, trade[f'{decision}_decision_best_buy_profit'][0], trade[f'{decision}_decision_best_buy_profit'][1]]\n",
    "                else:\n",
    "                    ticks_till_peak = trade[f'{decision}_decision_best_sell_profit'][1] - trade['trade_open_tick_i'] + 1\n",
    "                    label = ['sell', ticks_till_peak, trade[f'{decision}_decision_best_sell_profit'][0], trade[f'{decision}_decision_best_sell_profit'][1]]\n",
    "                \n",
    "        return label\n",
    "    \n",
    "    # now simulate hedged trades to determine labels\n",
    "    labels_dict = {}   # 6 labels per label: (1) decision (buy, sell, wait) (str), (2) ticks till best profit (int), and (3) the profit (float) x2 for each decision\n",
    "    trades = {}\n",
    "    pending_order = None\n",
    "    pending_close = None\n",
    "    decisions_so_far = []\n",
    "    for i, row in enumerate(data[start_idx:]):\n",
    "        i += start_idx\n",
    "        \n",
    "        if pending_order is not None:\n",
    "            pending_order_i, causes = pending_order\n",
    "            open_price = data[i][feature_indices['Open']]\n",
    "            signal_datetime = None if not has_datetimes else data[pending_order_i][feature_indices['datetime']]                \n",
    "            trades[pending_order_i] = {\n",
    "                'signal_datetime': signal_datetime,\n",
    "                'open_price': open_price,\n",
    "                'trade_open_tick_i': i,\n",
    "                'causes': causes,\n",
    "                'consider_profit': False,\n",
    "                'first_decision_best_buy_profit': None, # should be tuple of size 2 where 1st elem is the profit and 2nd is the number of bars to get to that profit\n",
    "                'first_decision_best_sell_profit': None,\n",
    "                'second_decision_best_buy_profit': None,\n",
    "                'second_decision_best_sell_profit': None,\n",
    "                'first_decision_done': False,\n",
    "                'second_decision_done': False,\n",
    "                'first_decision_done_tick_dt': None,\n",
    "                'second_decision_done_tick_dt': None,\n",
    "                'first_decision_done_tick_i': None,\n",
    "                'second_decision_done_tick_i': None\n",
    "            }\n",
    "            pending_order = None\n",
    "        \n",
    "        closed_trades = []\n",
    "        for trade_i in trades:\n",
    "            trade = trades[trade_i]\n",
    "            trade_open_price = trade['open_price']\n",
    "            close_price = row[feature_indices['Close']]\n",
    "            last_close_price = data[i-1][feature_indices['Close']] if i-1 != trade_i else trade_open_price\n",
    "            buy_profit = get_profit(close_price, trade_open_price, pip_value, pip_resolution, in_quote_currency)\n",
    "            sell_profit = buy_profit * -1\n",
    "            \n",
    "            scaled_profit_noise = profit_noise if not in_quote_currency else profit_noise / close_price\n",
    "            if abs(buy_profit) >= scaled_profit_noise:\n",
    "                trade['consider_profit'] = True\n",
    "                    \n",
    "            if not trade['first_decision_done']:\n",
    "                if not trade['first_decision_best_buy_profit'] or trade['first_decision_best_buy_profit'][0] < buy_profit:\n",
    "                    trade['first_decision_best_buy_profit'] = (buy_profit, i, f'debug notes: ({close_price} - {trade_open_price}) / {pip_resolution} * {pip_value}')\n",
    "                \n",
    "                if not trade['first_decision_best_sell_profit'] or trade['first_decision_best_sell_profit'][0] < sell_profit:\n",
    "                    trade['first_decision_best_sell_profit'] = (sell_profit, i, f'debug notes: ({close_price} - {trade_open_price}) / {pip_resolution} * {pip_value}')\n",
    "                \n",
    "                # test for end of 1st decision: see if current close price crossed the intial price at which the trade was opened at\n",
    "                # note: only look for crosses after profit has exceeded profit_noise (small amounts of profit w/ respect to lots_per_trade and in_quote_currency)\n",
    "                if trade['consider_profit'] and \\\n",
    "                        ((last_close_price < trade_open_price and close_price >= trade_open_price) \\\n",
    "                        or (last_close_price > trade_open_price and close_price <= trade_open_price)): \n",
    "                    label = get_decision_label(trade, current_close_price=close_price, first=True, decisions_so_far=decisions_so_far)\n",
    "                    trade['first_decision_done'] = True\n",
    "                    first_decision_done_tick_dt = None if not has_datetimes else data[i][feature_indices['datetime']]  \n",
    "                    trade['first_decision_done_tick_dt'] = first_decision_done_tick_dt\n",
    "                    trade['first_decision_done_tick_i'] = i\n",
    "                    labels_dict[trade_i] = {'first_decision': label}\n",
    "                    decisions_so_far.append(label[0])\n",
    "                    \n",
    "                    trade['consider_profit'] = False\n",
    "                    \n",
    "                    # at this point trade['second_decision_best_buy_profit'] should be None\n",
    "                    trade['second_decision_best_buy_profit'] = (buy_profit, i, f'debug notes: ({close_price} - {trade_open_price}) / {pip_resolution} * {pip_value}')\n",
    "                    trade['second_decision_best_sell_profit'] = (sell_profit, i,f'debug notes: ({close_price} - {trade_open_price}) / {pip_resolution} * {pip_value}')\n",
    "                       \n",
    "            elif not trade['second_decision_done']:\n",
    "                if trade['second_decision_best_buy_profit'][0] < buy_profit:\n",
    "                    trade['second_decision_best_buy_profit'] = (buy_profit, i,f'debug notes: ({close_price} - {trade_open_price}) / {pip_resolution} * {pip_value}')\n",
    "                \n",
    "                if trade['second_decision_best_sell_profit'][0] < sell_profit:\n",
    "                    trade['second_decision_best_sell_profit'] = (sell_profit, i,f'debug notes: ({close_price} - {trade_open_price}) / {pip_resolution} * {pip_value}')\n",
    "                \n",
    "                # test for end of 2nd decision: see if current close price crossed the intial price at which the trade was opened at again\n",
    "                # note: only look for crosses after profit has exceeded profit_noise (small amounts of profit w/ respect to lots_per_trade and in_quote_currency)\n",
    "                if trade['consider_profit'] and \\\n",
    "                        ((last_close_price < trade_open_price and close_price >= trade_open_price) \\\n",
    "                        or (last_close_price > trade_open_price and close_price <= trade_open_price)): \n",
    "                    label = get_decision_label(trade, current_close_price=close_price, first=False, decisions_so_far=decisions_so_far)\n",
    "                    trade['second_decision_done'] = True\n",
    "                    second_decision_done_tick_dt = None if not has_datetimes else data[i][feature_indices['datetime']]\n",
    "                    trade['second_decision_done_tick_dt'] = second_decision_done_tick_dt\n",
    "                    trade['second_decision_done_tick_i'] = i\n",
    "                    labels_dict[trade_i]['second_decision'] = label\n",
    "                    decisions_so_far.append(label[0])\n",
    "                    \n",
    "                    labels_dict[trade_i]['causes'] = ','.join(trade['causes'])\n",
    "                    closed_trades.append(trade_i)\n",
    "        \n",
    "        pending_close = None\n",
    "        \n",
    "        for trade_i in closed_trades:\n",
    "            del trades[trade_i]\n",
    "            \n",
    "        causes = []\n",
    "        for sig in signals_to_consider:\n",
    "            sig_i = feature_indices[sig]\n",
    "            if int(row[sig_i]) != 0:\n",
    "                causes.append(sig)\n",
    "        \n",
    "        if len(causes) > 0:\n",
    "            pending_order = (i, causes)\n",
    "            pending_close = (i, causes)\n",
    "    \n",
    "    # leftover open trades\n",
    "    for trade_i in trades:\n",
    "        trade = trades[trade_i]\n",
    "        \n",
    "        if not trade['first_decision_done']:\n",
    "            label = get_decision_label(trade, current_close_price=data[-1][feature_indices['Close']], first=True, leftover=True, decisions_so_far=decisions_so_far)\n",
    "            labels_dict[trade_i] = {'first_decision': label}\n",
    "                    \n",
    "        elif not trade['second_decision_done']:\n",
    "            label = get_decision_label(trade, current_close_price=data[-1][feature_indices['Close']], first=False, leftover=True, decisions_so_far=decisions_so_far)\n",
    "            labels_dict[trade_i]['second_decision'] = label\n",
    "                    \n",
    "        labels_dict[trade_i]['causes'] = ','.join(trade['causes'])\n",
    "\n",
    "    first_decision_labels_count = 4\n",
    "    second_decision_labels_count = 4\n",
    "    for i in labels_dict:\n",
    "        entry = labels_dict[i]\n",
    "        if 'first_decision' in entry:\n",
    "            if not first_decision_labels_count:\n",
    "                first_decision_labels_count = len(entry['first_decision'])\n",
    "            elif first_decision_labels_count != len(entry['first_decision']):\n",
    "                print(f'number of 1st decision labels are not equal for each row (row {i}: {entry[\"first_decision\"]}, ' \n",
    "                      f'changed from {first_decision_labels_count} to {len(entry[\"first_decision\"])},)!')\n",
    "                return None\n",
    "        if 'second_decision' in entry:\n",
    "            if not second_decision_labels_count:\n",
    "                second_decision_labels_count = len(entry['second_decision'])\n",
    "            elif second_decision_labels_count != len(entry['second_decision']):\n",
    "                print(f'number of 2nd decision labels are not equal for each row (row {i}: {entry[\"second_decision\"]}, ' \n",
    "                      f'changed from {second_decision_labels_count} to {len(entry[\"second_decision\"])})!')\n",
    "                return None\n",
    "    \n",
    "    labels = []\n",
    "    for i in range(len(data)):\n",
    "        if i in labels_dict:\n",
    "            entry = labels_dict[i] \n",
    "            causes_label = entry['causes']\n",
    "            \n",
    "            first_decision_labels = [None] * first_decision_labels_count\n",
    "            second_decision_labels = [None] * second_decision_labels_count\n",
    "            if 'first_decision' in entry:\n",
    "                first_decision_labels = entry['first_decision']\n",
    "            if 'second_decision' in entry:\n",
    "                second_decision_labels = entry['second_decision']\n",
    "                            \n",
    "            label = [*first_decision_labels, *second_decision_labels, causes_label]\n",
    "\n",
    "            labels.append(label)\n",
    "        \n",
    "        # just assign 'wait' labels to rows of data where no ichimoku signlas occured if label_non_signals==True\n",
    "        elif label_non_signals:\n",
    "            labels.append(['wait', 0, 0, None,'wait', 0, 0, None, None])\n",
    "            \n",
    "        # otherwise just put None labels\n",
    "        else:\n",
    "            labels.append([None] * (first_decision_labels_count + second_decision_labels_count + 1)) # +1 for causes label\n",
    "    \n",
    "    label_names = ['first_decision','ticks_till_best_profit_first_decision', 'best_profit_first_decision', 'profit_peak_first_decision',\n",
    "                   'second_decision','ticks_till_best_profit_second_decision', 'best_profit_second_decision', 'profit_peak_second_decision',\n",
    "                   'causes']\n",
    "    \n",
    "    labels_df = pd.DataFrame(labels, columns=label_names)\n",
    "    return labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def compare_labels_dataframes(df1, df2, print_each_line=True, only_show_diff_rows=True):\n",
    "#     data1 = tuple(df1.itertuples())\n",
    "#     data2 = tuple(df2.itertuples())\n",
    "    \n",
    "#     if len(data1) != len(data2):\n",
    "#         print('dataframes do not have equal number of rows')\n",
    "    \n",
    "#     diff_rows = 0\n",
    "#     for row1, row2 in zip(data1, data2):\n",
    "#         row1 = tuple(row1)\n",
    "#         row2 = tuple(row2)\n",
    "#         all_equal = True\n",
    "#         diff_cols = []\n",
    "        \n",
    "#         for i in range(max(len(row1), len(row2))):\n",
    "#             if pd.isnull(row1[i]) and pd.isnull(row2[i]):\n",
    "#                 continue\n",
    "#             elif isinstance(row1[i], float) and isinstance(row2[i], float):\n",
    "#                 if round(row1[i], 3) != round(row2[i], 3):\n",
    "#                     all_equal = False\n",
    "#                     diff_cols.append(i)\n",
    "#             else:\n",
    "#                 if row1[i] != row2[i]:\n",
    "#                     all_equal = False\n",
    "#                     diff_cols.append(i)\n",
    "            \n",
    "#         if not all_equal:\n",
    "#             diff_rows += 1\n",
    "        \n",
    "#         if print_each_line and (not only_show_diff_rows or (only_show_diff_rows and not all_equal)):\n",
    "#             print(f'df1 row: {row1}')\n",
    "#             print(f'df2 row: {row2}')\n",
    "#             print(f'df1 row types: {[type(x) for x in row1]}')\n",
    "#             print(f'df2 row types: {[type(x) for x in row2]}')\n",
    "#             print(f'same rows: {all_equal}')\n",
    "#             print(f'different column indices: {diff_cols}\\n')\n",
    "\n",
    "#     print(f'number of diff rows: {diff_rows}')\n",
    "    \n",
    "# d1 = generate_ichimoku_labels(data_with_ichi_2)\n",
    "# # d1.to_csv('./test1.csv')\n",
    "# d2 = generate_ichimoku_labels(data_with_ichi_2, min_profit_percent=0.0001, profit_noise_percent=0)\n",
    "# # d2.to_csv('./test2.csv')\n",
    "# d3 = generate_ichimoku_labels(data_with_ichi_2, label_non_signals=True)\n",
    "# # d3.to_csv('./test3.csv')\n",
    "# compare_labels_dataframes(d1, d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### try out different models w/ diff hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "XGBoost param tuning guide:\n",
    "https://towardsdatascience.com/fine-tuning-xgboost-in-python-like-a-boss-b4543ed8b1e\n",
    "\"\"\"\n",
    "\n",
    "contract_size = 100_000   # size of 1 lot is typically 100,000 (100 for gold, becuase 1 lot = 100 oz of gold)\n",
    "in_quote_currency = True\n",
    "pip_resolution = 0.0001\n",
    "labels_dict = {1: 'buy', 0: 'sell'}\n",
    "\n",
    "profit_percentages = [(pp/1000,pp/1000) for pp in range(1,101,3)]\n",
    "\n",
    "param_grid = {\n",
    "    'ichi_settings': [(9,26,52),(8,22,24),(9,30,60)],\n",
    "    'labeling_params': [{\n",
    "        'label_non_signals': [False],\n",
    "        'profit_percentages': profit_percentages,\n",
    "        'lots_per_trade': [0.2],\n",
    "    }],\n",
    "    'xgboost_params': [{\n",
    "        'n_estimators': [3000],\n",
    "        'max_depth': [2],\n",
    "        'learning_rate': [0.1],\n",
    "        'subsample': [1],\n",
    "        'colsample_bytree': [1],\n",
    "        'gamma': [1]\n",
    "    }]\n",
    "}\n",
    "\n",
    "param_grid = ParameterGrid(param_grid)\n",
    "param_grid = random.sample(list(param_grid), len(param_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filepath = gi.download_mt5_data(\"EURUSD\", 'H1', '2012-01-02', '2020-12-18')\n",
    "train_split = 0.7\n",
    "results = []\n",
    "best_params_first_decision = None\n",
    "best_score_first_decision = None\n",
    "best_params_second_decision = None\n",
    "best_score_second_decision = None\n",
    "num_class = 3 # buy, sell, wait\n",
    "signals_to_consider = ['cloud_breakout_bull','cloud_breakout_bear',                       # cloud breakout\n",
    "                       'tk_cross_bull_strength', 'tk_cross_bear_strength',                # Tenkan Sen / Kijun Sen Cross\n",
    "                       'tk_price_cross_bull_strength', 'tk_price_cross_bear_strength',    # price crossing both the Tenkan Sen / Kijun Sen\n",
    "                       'senkou_cross_bull_strength', 'senkou_cross_bear_strength',        # Senkou Span Cross\n",
    "                       'chikou_cross_bull_strength', 'chikou_cross_bear_strength']        # Chikou Span Cross\n",
    "pc_cols = ['Open','High','Low','Close','Volume',\n",
    "           'trend_ichimoku_base','trend_ichimoku_conv',\n",
    "           'trend_ichimoku_a', 'trend_ichimoku_b']\n",
    "\n",
    "start_time = time.time()\n",
    "for i, params in enumerate(param_grid):\n",
    "    ichi_settings = params['ichi_settings']\n",
    "    labeling_params = params['labeling_params']\n",
    "    xgboost_params = params['xgboost_params']\n",
    "    \n",
    "    labeling_params = ParameterGrid(labeling_params)\n",
    "    labeling_params = random.sample(list(labeling_params), len(labeling_params))\n",
    "    xgboost_params = ParameterGrid(xgboost_params)\n",
    "    xgboost_params = random.sample(list(xgboost_params), len(xgboost_params))\n",
    "        \n",
    "    model_config = {\n",
    "        'current_model':'ichi_cloud',\n",
    "        'ichi_cloud':{\n",
    "            'indicators': {\n",
    "                'ichimoku': {\n",
    "                    'tenkan_period': ichi_settings[0],\n",
    "                    'kijun_period': ichi_settings[1],\n",
    "                    'chikou_period': ichi_settings[1],\n",
    "                    'senkou_b_period': ichi_settings[2]\n",
    "                },\n",
    "                'rsi': {\n",
    "                    'periods': 14\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # load in and split data\n",
    "    \n",
    "    data_with_ta_indicators = gi.add_indicators_to_raw(filepath=filepath, \n",
    "                                                       save_to_disk=False, \n",
    "                                                       config=model_config, \n",
    "                                                       has_headers=True,\n",
    "                                                       datetime_col='datetime')\n",
    "    data_with_ichi_signals = add_features(data_with_ta_indicators)\n",
    "    start_idx, end_idx = no_missing_data_idx_range(data_with_ichi_signals, early_ending_cols=['chikou_span_visual'])\n",
    "    data_with_ichi_signals = data_with_ichi_signals[start_idx:].reset_index(drop=True)\n",
    "    \n",
    "    if train_split > 1:\n",
    "        print(f'train_split ({train_split}) is greater than 1, stopping.')\n",
    "    \n",
    "    train_p = train_split\n",
    "    num_rows = len(data_with_ichi_signals)\n",
    "    train_data_count = int(train_p * num_rows)\n",
    "    \n",
    "    train_data_orig = data_with_ichi_signals.iloc[:train_data_count]\n",
    "    validation_data_orig = data_with_ichi_signals.iloc[train_data_count:]\n",
    "    \n",
    "    for j, label_params in enumerate(labeling_params):\n",
    "        label_non_signals = label_params['label_non_signals']\n",
    "        min_profit_percent, profit_noise_percent = label_params['profit_percentages']\n",
    "        lots_per_trade = label_params['lots_per_trade']\n",
    "    \n",
    "        # generate labels for data\n",
    "\n",
    "        train_data_labels = generate_ichimoku_labels(train_data_orig, label_non_signals=label_non_signals, min_profit_percent=min_profit_percent, \n",
    "                                                     profit_noise_percent=profit_noise_percent, signals_to_consider=signals_to_consider, \n",
    "                                                     contract_size=contract_size, lots_per_trade=lots_per_trade,\n",
    "                                                     in_quote_currency=in_quote_currency,pip_resolution=pip_resolution, print_debug=False)\n",
    "        validation_data_labels = generate_ichimoku_labels(validation_data_orig, label_non_signals=label_non_signals, min_profit_percent=min_profit_percent, \n",
    "                                                          profit_noise_percent=profit_noise_percent, signals_to_consider=signals_to_consider, \n",
    "                                                          contract_size=contract_size, lots_per_trade=lots_per_trade,\n",
    "                                                          in_quote_currency=in_quote_currency, pip_resolution=pip_resolution, print_debug=False)\n",
    "        \n",
    "        train_data = apply_perc_change(train_data_orig, cols=pc_cols, limit=1)\n",
    "        start_idx, end_idx = no_missing_data_idx_range(train_data, early_ending_cols=['chikou_span_visual'])\n",
    "        train_data = train_data.iloc[start_idx:end_idx+1]\n",
    "        train_data_labels = train_data_labels.iloc[start_idx:end_idx+1]\n",
    "        \n",
    "        validation_data = apply_perc_change(validation_data_orig, cols=pc_cols, limit=1)\n",
    "        start_idx, end_idx = no_missing_data_idx_range(validation_data, early_ending_cols=['chikou_span_visual'])\n",
    "        validation_data = validation_data.iloc[start_idx:end_idx+1]\n",
    "        validation_data_labels = validation_data_labels.iloc[start_idx:end_idx+1]\n",
    "\n",
    "        x_train_first_decisions, y_train_first_decisions = missing_labels_preprocess(train_data, train_data_labels, 'first_decision')\n",
    "        x_valid_first_decisions, y_valid_first_decisions = missing_labels_preprocess(validation_data, validation_data_labels, 'first_decision')\n",
    "        x_train_first_decisions_profits, y_train_first_decisions_profits = missing_labels_preprocess(train_data, train_data_labels, \n",
    "                                                                                                     'best_profit_first_decision')\n",
    "        x_valid_first_decisions_profits, y_valid_first_decisions_profits = missing_labels_preprocess(validation_data, validation_data_labels, \n",
    "                                                                                                     'best_profit_first_decision')\n",
    "\n",
    "        x_train_second_decisions, y_train_second_decisions = missing_labels_preprocess(train_data, train_data_labels, 'second_decision')\n",
    "        x_valid_second_decisions, y_valid_second_decisions = missing_labels_preprocess(validation_data, validation_data_labels, 'second_decision')\n",
    "        x_train_second_decisions_profits, y_train_second_decisions_profits = missing_labels_preprocess(train_data, train_data_labels, \n",
    "                                                                                                       'best_profit_second_decision')\n",
    "        x_valid_second_decisions_profits, y_valid_second_decisions_profits = missing_labels_preprocess(validation_data, validation_data_labels, \n",
    "                                                                                                       'best_profit_second_decision')\n",
    "\n",
    "        # generate predictions w/ XGBoost model\n",
    "        for k, xgb_params in enumerate(xgboost_params):\n",
    "            n_estimators = xgb_params['n_estimators']\n",
    "            max_depth = xgb_params['max_depth']\n",
    "            learning_rate = xgb_params['learning_rate']\n",
    "            subsample = xgb_params['subsample']\n",
    "            colsample_bytree = xgb_params['colsample_bytree']\n",
    "            gamma = xgb_params['gamma']\n",
    "            \n",
    "            if min_profit_percent==profit_noise_percent:\n",
    "                # binrary classification problem (buy or sell)\n",
    "                error_metric_name = 'error'\n",
    "                xgb_params = {'max_depth':max_depth, 'learning_rate':learning_rate, 'objective':'binary:logistic', 'eval_metric': error_metric_name, \n",
    "                              'gamma':gamma, 'colsample_bytree':colsample_bytree, 'subsample':subsample}\n",
    "            else:\n",
    "                # multi-class classification problem (buy, sell, or wiat)\n",
    "                error_metric_name = 'merror'\n",
    "                xgb_params = {'max_depth':max_depth, 'learning_rate':learning_rate, 'objective':'multi:softmax', 'num_class': num_class,\n",
    "                              'eval_metric': error_metric_name, 'gamma':gamma, 'colsample_bytree':colsample_bytree, 'subsample':subsample}\n",
    "            \n",
    "            ### first decisions\n",
    "\n",
    "            y_train_true, labels_dict = convert_class_labels(y_train_first_decisions, labels_dict=labels_dict)\n",
    "            y_valid_true, labels_dict = convert_class_labels(y_valid_first_decisions, labels_dict=labels_dict)\n",
    "\n",
    "            dtrain = xgb.DMatrix(x_train_first_decisions, label=y_train_true)\n",
    "            dvalidation = [(xgb.DMatrix(x_train_first_decisions, label=y_train_true),'train'), \n",
    "                           (xgb.DMatrix(x_valid_first_decisions, label=y_valid_true),'validation')]\n",
    "            dtest = xgb.DMatrix(x_valid_first_decisions)\n",
    "            \n",
    "            evals_result = {}\n",
    "            decision_predictor = xgb.train(xgb_params, dtrain, num_boost_round=n_estimators, evals=dvalidation, \n",
    "                                           evals_result=evals_result, verbose_eval=False)\n",
    "            \n",
    "            train_error = evals_result['train'][error_metric_name][-1]\n",
    "            train_accuracy_first_decision = 1 - train_error\n",
    "            \n",
    "            validation_error = evals_result['validation'][error_metric_name][-1]\n",
    "            validation_accuracy_first_decision = 1 - validation_error\n",
    "            \n",
    "            y_test_probs = decision_predictor.predict(dtest)\n",
    "            y_test_preds = np.around(y_test_probs)\n",
    "            y_test_preds = pd.DataFrame(y_test_preds, columns=y_valid_true.columns)\n",
    "            y_test_preds = convert_class_labels(y_test_preds, to_ints=False, labels_dict=labels_dict)[0]\n",
    "            p_profits_first_decision = potention_profits(y_valid_first_decisions, y_test_preds, y_valid_first_decisions_profits)\n",
    "\n",
    "            ### second decisions\n",
    "\n",
    "            y_train_true, labels_dict = convert_class_labels(y_train_second_decisions, labels_dict=labels_dict)\n",
    "            y_valid_true, labels_dict = convert_class_labels(y_valid_second_decisions, labels_dict=labels_dict)\n",
    "\n",
    "            dtrain = xgb.DMatrix(x_train_second_decisions, label=y_train_true)\n",
    "            dvalidation = [(xgb.DMatrix(x_train_second_decisions, label=y_train_true),'train'), \n",
    "                           (xgb.DMatrix(x_valid_second_decisions, label=y_valid_true),'validation')]\n",
    "            dtest = xgb.DMatrix(x_valid_second_decisions)\n",
    "            \n",
    "            evals_result = {}\n",
    "            decision_predictor = xgb.train(xgb_params, dtrain, num_boost_round=n_estimators, evals=dvalidation, \n",
    "                                           evals_result=evals_result, verbose_eval=False)\n",
    "\n",
    "            train_error = evals_result['train'][error_metric_name][-1]\n",
    "            train_accuracy_second_decision = 1 - train_error\n",
    "            \n",
    "            validation_error = evals_result['validation'][error_metric_name][-1]\n",
    "            validation_accuracy_second_decision = 1 - validation_error\n",
    "            \n",
    "            y_test_probs = decision_predictor.predict(dtest)\n",
    "            y_test_preds = np.around(y_test_probs)\n",
    "            y_test_preds = pd.DataFrame(y_test_preds, columns=y_valid_true.columns)\n",
    "            y_test_preds = convert_class_labels(y_test_preds, to_ints=False, labels_dict=labels_dict)[0]\n",
    "            p_profits_second_decision = potention_profits(y_valid_second_decisions, y_test_preds, y_valid_second_decisions_profits)\n",
    "        \n",
    "            all_params = {\n",
    "                'tenkan_period': ichi_settings[0],\n",
    "                'kijun_period': ichi_settings[1],\n",
    "                'chikou_period': ichi_settings[1],\n",
    "                'senkou_b_period': ichi_settings[2],\n",
    "                'label_non_signals': label_non_signals,\n",
    "                'min_profit_percent': min_profit_percent,\n",
    "                'profit_noise_percent': profit_noise_percent,\n",
    "                'lots_per_trade': lots_per_trade,\n",
    "                'n_estimators': n_estimators,\n",
    "                'max_depth': max_depth,\n",
    "                'learning_rate': learning_rate,\n",
    "                'subsample': subsample,\n",
    "                'colsample_bytree': colsample_bytree,\n",
    "                'gamma': gamma,\n",
    "                'train_accuracy_first_decision': train_accuracy_first_decision,\n",
    "                'validation_accuracy_first_decision': validation_accuracy_first_decision,\n",
    "                'train_accuracy_second_decision': train_accuracy_second_decision,\n",
    "                'validation_accuracy_second_decision': validation_accuracy_second_decision,\n",
    "                'potention_profits_first_decision': p_profits_first_decision,\n",
    "                'potention_profits_second_decision': p_profits_second_decision\n",
    "            }\n",
    "            \n",
    "            first_decision_score = validation_accuracy_first_decision\n",
    "            second_decision_score = validation_accuracy_second_decision\n",
    "            \n",
    "            if not best_score_first_decision or first_decision_score > best_score_first_decision:\n",
    "                best_score_first_decision = first_decision_score\n",
    "                best_params_first_decision = all_params\n",
    "                \n",
    "            if not best_score_second_decision or second_decision_score > best_score_second_decision:\n",
    "                best_score_second_decision = second_decision_score\n",
    "                best_params_second_decision = all_params\n",
    "            \n",
    "            results.append(all_params)\n",
    "            \n",
    "            print('--------------------------------------------------------------------')\n",
    "            print(f'{k+1}/{len(xgboost_params)} xgb params evaulated')\n",
    "            print('--------------------------------------------------------------------\\n')\n",
    "            print(f'last params evaluated:')\n",
    "            print(f'{all_params}\\n')\n",
    "            print(f'best first decision params evaluated:')\n",
    "            print(f'{best_params_first_decision}\\n')\n",
    "            print(f'best second decision params evaluated:')\n",
    "            print(f'{best_params_second_decision}\\n')\n",
    "\n",
    "        print('--------------------------------------------------------------------')\n",
    "        print(f'{j+1}/{len(labeling_params)} labeling params evaulated')\n",
    "        print('--------------------------------------------------------------------\\n')\n",
    "        \n",
    "        results_sorted = sorted(results, key=lambda d: d['validation_accuracy_first_decision'], reverse=True)\n",
    "        results_sorted_df = pd.DataFrame(results_sorted)\n",
    "        results_sorted_df.to_csv('../my_stuff/grid_search_results.csv')\n",
    "        \n",
    "    print('--------------------------------------------------------------------')\n",
    "    print(f'{i+1}/{len(param_grid)} ichimoku settings evaulated')\n",
    "    print('--------------------------------------------------------------------\\n')\n",
    "    \n",
    "results_sorted = sorted(results, key=lambda d: d['validation_accuracy_first_decision'], reverse=True)\n",
    "results_sorted_df = pd.DataFrame(results_sorted)\n",
    "results_sorted_df.to_csv('../my_stuff/grid_search_results.csv')\n",
    "print(f'runtime: {(time.time()-start_time)/60} min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train model for backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 60431 rows of tick data from C:\\GitHub Repos\\ForexMachine\\Data\\.cache\\mt5_EURUSD_h1_ticks_2011-01-01T00;00UTC_to_2020-10-01T00;00UTC.csv\n",
      "saved 60431 rows of EURUSD h1 tick data to C:\\GitHub Repos\\ForexMachine\\Data\\RawData\\mt5_EURUSD_h1_ticks_2011-01-01T00;00UTC_to_2020-10-01T00;00UTC.csv, done.\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "\n",
    "label_non_signals = False\n",
    "min_profit_percent, profit_noise_percent = 0.01, 0.01\n",
    "contract_size = 100_000   # size of 1 lot is typically 100,000 (100 for gold, becuase 1 lot = 100 oz of gold)\n",
    "lots_per_trade = 0.2  \n",
    "currency_side = 'right'\n",
    "in_quote_currency = True if currency_side == 'right' else False\n",
    "pip_resolution = 0.0001\n",
    "\n",
    "labels_dict = {1: 'buy', 0: 'sell'}\n",
    "n_estimators = 3000\n",
    "max_depth = 2\n",
    "learning_rate = 0.1\n",
    "subsample = 1\n",
    "colsample_bytree = 1\n",
    "gamma = 1\n",
    "tenkan_period = 9\n",
    "kijun_period = 30\n",
    "senkou_b_period = 60\n",
    "model_config = {\n",
    "    'current_model':'ichi_cloud',\n",
    "    'ichi_cloud':{\n",
    "        'indicators': {\n",
    "            'ichimoku': {\n",
    "                'tenkan_period': tenkan_period,\n",
    "                'kijun_period': kijun_period,\n",
    "                'chikou_period': kijun_period,\n",
    "                'senkou_b_period': senkou_b_period\n",
    "            },\n",
    "            'rsi': {\n",
    "                'periods': 14\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "signals_to_consider = ['cloud_breakout_bull','cloud_breakout_bear',                       # cloud breakout\n",
    "                       'tk_cross_bull_strength', 'tk_cross_bear_strength',                # Tenkan Sen / Kijun Sen Cross\n",
    "                       'tk_price_cross_bull_strength', 'tk_price_cross_bear_strength',    # price crossing both the Tenkan Sen / Kijun Sen\n",
    "                       'senkou_cross_bull_strength', 'senkou_cross_bear_strength',        # Senkou Span Cross\n",
    "                       'chikou_cross_bull_strength', 'chikou_cross_bear_strength']        # Chikou Span Cross\n",
    "sigs_for_filename = 'cb-tk-tkp-sen-chi'\n",
    "\n",
    "# get data\n",
    "\n",
    "cur_pair = 'EURUSD'\n",
    "timeframe = 'H1'\n",
    "tick_data_filepath = gi.download_mt5_data(cur_pair, timeframe, '2011-01-01', '2020-10-01')\n",
    "data_with_indicators = gi.add_indicators_to_raw(filepath=tick_data_filepath, \n",
    "                                                save_to_disk=True, \n",
    "                                                config=model_config, \n",
    "                                                has_headers=True,\n",
    "                                                datetime_col='datetime')\n",
    "train_data = add_features(data_with_indicators)\n",
    "\n",
    "train_data_labels = generate_ichimoku_labels(train_data, label_non_signals=label_non_signals, min_profit_percent=min_profit_percent, \n",
    "                                             profit_noise_percent=profit_noise_percent, signals_to_consider=signals_to_consider, \n",
    "                                             contract_size=contract_size, lots_per_trade=lots_per_trade,\n",
    "                                             in_quote_currency=in_quote_currency,pip_resolution=pip_resolution)\n",
    "\n",
    "pc_cols = ['Open','High','Low','Close','Volume',\n",
    "           'trend_ichimoku_base','trend_ichimoku_conv',\n",
    "           'trend_ichimoku_a', 'trend_ichimoku_b']\n",
    "train_data = apply_perc_change(train_data, cols=pc_cols, limit=1)\n",
    "start_idx, end_idx = no_missing_data_idx_range(train_data)\n",
    "train_data = train_data.iloc[start_idx:end_idx+1]\n",
    "train_data_labels = train_data_labels.iloc[start_idx:end_idx+1]\n",
    "\n",
    "x_train_first_decisions, y_train_first_decisions = missing_labels_preprocess(train_data, train_data_labels, 'first_decision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-error:0.45984\n",
      "[1]\ttrain-error:0.45647\n",
      "[2]\ttrain-error:0.45815\n",
      "[3]\ttrain-error:0.45771\n",
      "[4]\ttrain-error:0.45984\n",
      "[5]\ttrain-error:0.45771\n",
      "[6]\ttrain-error:0.45771\n",
      "[7]\ttrain-error:0.45753\n",
      "[8]\ttrain-error:0.45771\n",
      "[9]\ttrain-error:0.45584\n",
      "[10]\ttrain-error:0.45771\n",
      "[11]\ttrain-error:0.45815\n",
      "[12]\ttrain-error:0.45735\n",
      "[13]\ttrain-error:0.45718\n",
      "[14]\ttrain-error:0.45744\n",
      "[15]\ttrain-error:0.45744\n",
      "[16]\ttrain-error:0.45744\n",
      "[17]\ttrain-error:0.45620\n",
      "[18]\ttrain-error:0.45593\n",
      "[19]\ttrain-error:0.45522\n",
      "[20]\ttrain-error:0.45238\n",
      "[21]\ttrain-error:0.45274\n",
      "[22]\ttrain-error:0.45354\n",
      "[23]\ttrain-error:0.45327\n",
      "[24]\ttrain-error:0.45336\n",
      "[25]\ttrain-error:0.45425\n",
      "[26]\ttrain-error:0.45363\n",
      "[27]\ttrain-error:0.45300\n",
      "[28]\ttrain-error:0.45150\n",
      "[29]\ttrain-error:0.45123\n",
      "[30]\ttrain-error:0.44732\n",
      "[31]\ttrain-error:0.44697\n",
      "[32]\ttrain-error:0.44510\n",
      "[33]\ttrain-error:0.44555\n",
      "[34]\ttrain-error:0.44484\n",
      "[35]\ttrain-error:0.44413\n",
      "[36]\ttrain-error:0.44457\n",
      "[37]\ttrain-error:0.44448\n",
      "[38]\ttrain-error:0.44546\n",
      "[39]\ttrain-error:0.44413\n",
      "[40]\ttrain-error:0.44280\n",
      "[41]\ttrain-error:0.44164\n",
      "[42]\ttrain-error:0.44262\n",
      "[43]\ttrain-error:0.44102\n",
      "[44]\ttrain-error:0.44049\n",
      "[45]\ttrain-error:0.44040\n",
      "[46]\ttrain-error:0.43916\n",
      "[47]\ttrain-error:0.43738\n",
      "[48]\ttrain-error:0.43747\n",
      "[49]\ttrain-error:0.43747\n",
      "[50]\ttrain-error:0.43650\n",
      "[51]\ttrain-error:0.43659\n",
      "[52]\ttrain-error:0.43729\n",
      "[53]\ttrain-error:0.43712\n",
      "[54]\ttrain-error:0.43756\n",
      "[55]\ttrain-error:0.43694\n",
      "[56]\ttrain-error:0.43694\n",
      "[57]\ttrain-error:0.43588\n",
      "[58]\ttrain-error:0.43499\n",
      "[59]\ttrain-error:0.43463\n",
      "[60]\ttrain-error:0.43383\n",
      "[61]\ttrain-error:0.43401\n",
      "[62]\ttrain-error:0.43330\n",
      "[63]\ttrain-error:0.43401\n",
      "[64]\ttrain-error:0.43357\n",
      "[65]\ttrain-error:0.43339\n",
      "[66]\ttrain-error:0.43304\n",
      "[67]\ttrain-error:0.43197\n",
      "[68]\ttrain-error:0.43153\n",
      "[69]\ttrain-error:0.43055\n",
      "[70]\ttrain-error:0.43046\n",
      "[71]\ttrain-error:0.42993\n",
      "[72]\ttrain-error:0.43002\n",
      "[73]\ttrain-error:0.42993\n",
      "[74]\ttrain-error:0.42922\n",
      "[75]\ttrain-error:0.42895\n",
      "[76]\ttrain-error:0.42824\n",
      "[77]\ttrain-error:0.42815\n",
      "[78]\ttrain-error:0.42682\n",
      "[79]\ttrain-error:0.42700\n",
      "[80]\ttrain-error:0.42602\n",
      "[81]\ttrain-error:0.42416\n",
      "[82]\ttrain-error:0.42540\n",
      "[83]\ttrain-error:0.42513\n",
      "[84]\ttrain-error:0.42496\n",
      "[85]\ttrain-error:0.42389\n",
      "[86]\ttrain-error:0.42398\n",
      "[87]\ttrain-error:0.42265\n",
      "[88]\ttrain-error:0.42150\n",
      "[89]\ttrain-error:0.42398\n",
      "[90]\ttrain-error:0.42372\n",
      "[91]\ttrain-error:0.42372\n",
      "[92]\ttrain-error:0.42363\n",
      "[93]\ttrain-error:0.42363\n",
      "[94]\ttrain-error:0.42256\n",
      "[95]\ttrain-error:0.42229\n",
      "[96]\ttrain-error:0.42229\n",
      "[97]\ttrain-error:0.42167\n",
      "[98]\ttrain-error:0.42123\n",
      "[99]\ttrain-error:0.42052\n",
      "[100]\ttrain-error:0.42008\n",
      "[101]\ttrain-error:0.42008\n",
      "[102]\ttrain-error:0.41937\n",
      "[103]\ttrain-error:0.41928\n",
      "[104]\ttrain-error:0.41901\n",
      "[105]\ttrain-error:0.41946\n",
      "[106]\ttrain-error:0.41874\n",
      "[107]\ttrain-error:0.41874\n",
      "[108]\ttrain-error:0.41812\n",
      "[109]\ttrain-error:0.41830\n",
      "[110]\ttrain-error:0.41803\n",
      "[111]\ttrain-error:0.41821\n",
      "[112]\ttrain-error:0.41803\n",
      "[113]\ttrain-error:0.41795\n",
      "[114]\ttrain-error:0.41768\n",
      "[115]\ttrain-error:0.41688\n",
      "[116]\ttrain-error:0.41715\n",
      "[117]\ttrain-error:0.41741\n",
      "[118]\ttrain-error:0.41733\n",
      "[119]\ttrain-error:0.41715\n",
      "[120]\ttrain-error:0.41715\n",
      "[121]\ttrain-error:0.41697\n",
      "[122]\ttrain-error:0.41653\n",
      "[123]\ttrain-error:0.41653\n",
      "[124]\ttrain-error:0.41635\n",
      "[125]\ttrain-error:0.41599\n",
      "[126]\ttrain-error:0.41564\n",
      "[127]\ttrain-error:0.41511\n",
      "[128]\ttrain-error:0.41475\n",
      "[129]\ttrain-error:0.41333\n",
      "[130]\ttrain-error:0.41351\n",
      "[131]\ttrain-error:0.41378\n",
      "[132]\ttrain-error:0.41360\n",
      "[133]\ttrain-error:0.41191\n",
      "[134]\ttrain-error:0.41209\n",
      "[135]\ttrain-error:0.41182\n",
      "[136]\ttrain-error:0.41191\n",
      "[137]\ttrain-error:0.41218\n",
      "[138]\ttrain-error:0.41200\n",
      "[139]\ttrain-error:0.41191\n",
      "[140]\ttrain-error:0.41173\n",
      "[141]\ttrain-error:0.41093\n",
      "[142]\ttrain-error:0.41076\n",
      "[143]\ttrain-error:0.41085\n",
      "[144]\ttrain-error:0.41076\n",
      "[145]\ttrain-error:0.41058\n",
      "[146]\ttrain-error:0.41067\n",
      "[147]\ttrain-error:0.40996\n",
      "[148]\ttrain-error:0.40880\n",
      "[149]\ttrain-error:0.40943\n",
      "[150]\ttrain-error:0.40934\n",
      "[151]\ttrain-error:0.40845\n",
      "[152]\ttrain-error:0.40889\n",
      "[153]\ttrain-error:0.40898\n",
      "[154]\ttrain-error:0.40747\n",
      "[155]\ttrain-error:0.40783\n",
      "[156]\ttrain-error:0.40721\n",
      "[157]\ttrain-error:0.40818\n",
      "[158]\ttrain-error:0.40809\n",
      "[159]\ttrain-error:0.40809\n",
      "[160]\ttrain-error:0.40765\n",
      "[161]\ttrain-error:0.40730\n",
      "[162]\ttrain-error:0.40738\n",
      "[163]\ttrain-error:0.40588\n",
      "[164]\ttrain-error:0.40490\n",
      "[165]\ttrain-error:0.40428\n",
      "[166]\ttrain-error:0.40472\n",
      "[167]\ttrain-error:0.40490\n",
      "[168]\ttrain-error:0.40481\n",
      "[169]\ttrain-error:0.40463\n",
      "[170]\ttrain-error:0.40463\n",
      "[171]\ttrain-error:0.40525\n",
      "[172]\ttrain-error:0.40508\n",
      "[173]\ttrain-error:0.40517\n",
      "[174]\ttrain-error:0.40454\n",
      "[175]\ttrain-error:0.40428\n",
      "[176]\ttrain-error:0.40419\n",
      "[177]\ttrain-error:0.40383\n",
      "[178]\ttrain-error:0.40383\n",
      "[179]\ttrain-error:0.40348\n",
      "[180]\ttrain-error:0.40303\n",
      "[181]\ttrain-error:0.40303\n",
      "[182]\ttrain-error:0.40206\n",
      "[183]\ttrain-error:0.40197\n",
      "[184]\ttrain-error:0.40197\n",
      "[185]\ttrain-error:0.40188\n",
      "[186]\ttrain-error:0.40037\n",
      "[187]\ttrain-error:0.40073\n",
      "[188]\ttrain-error:0.40073\n",
      "[189]\ttrain-error:0.40055\n",
      "[190]\ttrain-error:0.40046\n",
      "[191]\ttrain-error:0.40020\n",
      "[192]\ttrain-error:0.39975\n",
      "[193]\ttrain-error:0.39993\n",
      "[194]\ttrain-error:0.39975\n",
      "[195]\ttrain-error:0.39957\n",
      "[196]\ttrain-error:0.39966\n",
      "[197]\ttrain-error:0.39966\n",
      "[198]\ttrain-error:0.39975\n",
      "[199]\ttrain-error:0.39966\n",
      "[200]\ttrain-error:0.39975\n",
      "[201]\ttrain-error:0.40002\n",
      "[202]\ttrain-error:0.39975\n",
      "[203]\ttrain-error:0.39975\n",
      "[204]\ttrain-error:0.40028\n",
      "[205]\ttrain-error:0.39984\n",
      "[206]\ttrain-error:0.40011\n",
      "[207]\ttrain-error:0.39948\n",
      "[208]\ttrain-error:0.39753\n",
      "[209]\ttrain-error:0.39744\n",
      "[210]\ttrain-error:0.39700\n",
      "[211]\ttrain-error:0.39665\n",
      "[212]\ttrain-error:0.39665\n",
      "[213]\ttrain-error:0.39673\n",
      "[214]\ttrain-error:0.39620\n",
      "[215]\ttrain-error:0.39629\n",
      "[216]\ttrain-error:0.39593\n",
      "[217]\ttrain-error:0.39576\n",
      "[218]\ttrain-error:0.39620\n",
      "[219]\ttrain-error:0.39576\n",
      "[220]\ttrain-error:0.39549\n",
      "[221]\ttrain-error:0.39505\n",
      "[222]\ttrain-error:0.39398\n",
      "[223]\ttrain-error:0.39425\n",
      "[224]\ttrain-error:0.39301\n",
      "[225]\ttrain-error:0.39327\n",
      "[226]\ttrain-error:0.39354\n",
      "[227]\ttrain-error:0.39336\n",
      "[228]\ttrain-error:0.39372\n",
      "[229]\ttrain-error:0.39292\n",
      "[230]\ttrain-error:0.39247\n",
      "[231]\ttrain-error:0.39265\n",
      "[232]\ttrain-error:0.39265\n",
      "[233]\ttrain-error:0.39221\n",
      "[234]\ttrain-error:0.39114\n",
      "[235]\ttrain-error:0.39088\n",
      "[236]\ttrain-error:0.39061\n",
      "[237]\ttrain-error:0.38990\n",
      "[238]\ttrain-error:0.38981\n",
      "[239]\ttrain-error:0.38954\n",
      "[240]\ttrain-error:0.38937\n",
      "[241]\ttrain-error:0.38954\n",
      "[242]\ttrain-error:0.38954\n",
      "[243]\ttrain-error:0.38928\n",
      "[244]\ttrain-error:0.38928\n",
      "[245]\ttrain-error:0.38910\n",
      "[246]\ttrain-error:0.38883\n",
      "[247]\ttrain-error:0.38875\n",
      "[248]\ttrain-error:0.38875\n",
      "[249]\ttrain-error:0.38910\n",
      "[250]\ttrain-error:0.38795\n",
      "[251]\ttrain-error:0.38795\n",
      "[252]\ttrain-error:0.38804\n",
      "[253]\ttrain-error:0.38777\n",
      "[254]\ttrain-error:0.38777\n",
      "[255]\ttrain-error:0.38812\n",
      "[256]\ttrain-error:0.38724\n",
      "[257]\ttrain-error:0.38653\n",
      "[258]\ttrain-error:0.38599\n",
      "[259]\ttrain-error:0.38591\n",
      "[260]\ttrain-error:0.38599\n",
      "[261]\ttrain-error:0.38653\n",
      "[262]\ttrain-error:0.38644\n",
      "[263]\ttrain-error:0.38671\n",
      "[264]\ttrain-error:0.38635\n",
      "[265]\ttrain-error:0.38555\n",
      "[266]\ttrain-error:0.38511\n",
      "[267]\ttrain-error:0.38431\n",
      "[268]\ttrain-error:0.38395\n",
      "[269]\ttrain-error:0.38378\n",
      "[270]\ttrain-error:0.38449\n",
      "[271]\ttrain-error:0.38360\n",
      "[272]\ttrain-error:0.38378\n",
      "[273]\ttrain-error:0.38324\n",
      "[274]\ttrain-error:0.38218\n",
      "[275]\ttrain-error:0.38200\n",
      "[276]\ttrain-error:0.38147\n",
      "[277]\ttrain-error:0.38067\n",
      "[278]\ttrain-error:0.38005\n",
      "[279]\ttrain-error:0.37996\n",
      "[280]\ttrain-error:0.38022\n",
      "[281]\ttrain-error:0.37996\n",
      "[282]\ttrain-error:0.37996\n",
      "[283]\ttrain-error:0.37943\n",
      "[284]\ttrain-error:0.37934\n",
      "[285]\ttrain-error:0.37951\n",
      "[286]\ttrain-error:0.37907\n",
      "[287]\ttrain-error:0.37916\n",
      "[288]\ttrain-error:0.37916\n",
      "[289]\ttrain-error:0.37960\n",
      "[290]\ttrain-error:0.38040\n",
      "[291]\ttrain-error:0.38049\n",
      "[292]\ttrain-error:0.38058\n",
      "[293]\ttrain-error:0.38049\n",
      "[294]\ttrain-error:0.38014\n",
      "[295]\ttrain-error:0.38014\n",
      "[296]\ttrain-error:0.38005\n",
      "[297]\ttrain-error:0.38005\n",
      "[298]\ttrain-error:0.37934\n",
      "[299]\ttrain-error:0.37898\n",
      "[300]\ttrain-error:0.37916\n",
      "[301]\ttrain-error:0.37943\n",
      "[302]\ttrain-error:0.37925\n",
      "[303]\ttrain-error:0.37845\n",
      "[304]\ttrain-error:0.37818\n",
      "[305]\ttrain-error:0.37774\n",
      "[306]\ttrain-error:0.37792\n",
      "[307]\ttrain-error:0.37801\n",
      "[308]\ttrain-error:0.37810\n",
      "[309]\ttrain-error:0.37801\n",
      "[310]\ttrain-error:0.37801\n",
      "[311]\ttrain-error:0.37756\n",
      "[312]\ttrain-error:0.37739\n",
      "[313]\ttrain-error:0.37739\n",
      "[314]\ttrain-error:0.37747\n",
      "[315]\ttrain-error:0.37747\n",
      "[316]\ttrain-error:0.37721\n",
      "[317]\ttrain-error:0.37623\n",
      "[318]\ttrain-error:0.37623\n",
      "[319]\ttrain-error:0.37667\n",
      "[320]\ttrain-error:0.37694\n",
      "[321]\ttrain-error:0.37650\n",
      "[322]\ttrain-error:0.37650\n",
      "[323]\ttrain-error:0.37632\n",
      "[324]\ttrain-error:0.37561\n",
      "[325]\ttrain-error:0.37552\n",
      "[326]\ttrain-error:0.37667\n",
      "[327]\ttrain-error:0.37605\n",
      "[328]\ttrain-error:0.37543\n",
      "[329]\ttrain-error:0.37455\n",
      "[330]\ttrain-error:0.37437\n",
      "[331]\ttrain-error:0.37392\n",
      "[332]\ttrain-error:0.37304\n",
      "[333]\ttrain-error:0.37259\n",
      "[334]\ttrain-error:0.37250\n",
      "[335]\ttrain-error:0.37215\n",
      "[336]\ttrain-error:0.37197\n",
      "[337]\ttrain-error:0.37188\n",
      "[338]\ttrain-error:0.37215\n",
      "[339]\ttrain-error:0.37171\n",
      "[340]\ttrain-error:0.37144\n",
      "[341]\ttrain-error:0.37179\n",
      "[342]\ttrain-error:0.37126\n",
      "[343]\ttrain-error:0.37135\n",
      "[344]\ttrain-error:0.37135\n",
      "[345]\ttrain-error:0.37153\n",
      "[346]\ttrain-error:0.37171\n",
      "[347]\ttrain-error:0.37241\n",
      "[348]\ttrain-error:0.37241\n",
      "[349]\ttrain-error:0.37233\n",
      "[350]\ttrain-error:0.37144\n",
      "[351]\ttrain-error:0.37117\n",
      "[352]\ttrain-error:0.37135\n",
      "[353]\ttrain-error:0.37126\n",
      "[354]\ttrain-error:0.37126\n",
      "[355]\ttrain-error:0.37162\n",
      "[356]\ttrain-error:0.37153\n",
      "[357]\ttrain-error:0.37046\n",
      "[358]\ttrain-error:0.37091\n",
      "[359]\ttrain-error:0.37135\n",
      "[360]\ttrain-error:0.37135\n",
      "[361]\ttrain-error:0.37091\n",
      "[362]\ttrain-error:0.37073\n",
      "[363]\ttrain-error:0.37020\n",
      "[364]\ttrain-error:0.36966\n",
      "[365]\ttrain-error:0.36966\n",
      "[366]\ttrain-error:0.36966\n",
      "[367]\ttrain-error:0.36975\n",
      "[368]\ttrain-error:0.36966\n",
      "[369]\ttrain-error:0.36966\n",
      "[370]\ttrain-error:0.37011\n",
      "[371]\ttrain-error:0.37002\n",
      "[372]\ttrain-error:0.36984\n",
      "[373]\ttrain-error:0.36993\n",
      "[374]\ttrain-error:0.36957\n",
      "[375]\ttrain-error:0.36975\n",
      "[376]\ttrain-error:0.36940\n",
      "[377]\ttrain-error:0.36913\n",
      "[378]\ttrain-error:0.36940\n",
      "[379]\ttrain-error:0.36966\n",
      "[380]\ttrain-error:0.36904\n",
      "[381]\ttrain-error:0.36895\n",
      "[382]\ttrain-error:0.36886\n",
      "[383]\ttrain-error:0.36860\n",
      "[384]\ttrain-error:0.36904\n",
      "[385]\ttrain-error:0.36904\n",
      "[386]\ttrain-error:0.36878\n",
      "[387]\ttrain-error:0.36860\n",
      "[388]\ttrain-error:0.36807\n",
      "[389]\ttrain-error:0.36824\n",
      "[390]\ttrain-error:0.36798\n",
      "[391]\ttrain-error:0.36771\n",
      "[392]\ttrain-error:0.36745\n",
      "[393]\ttrain-error:0.36709\n",
      "[394]\ttrain-error:0.36700\n",
      "[395]\ttrain-error:0.36718\n",
      "[396]\ttrain-error:0.36665\n",
      "[397]\ttrain-error:0.36727\n",
      "[398]\ttrain-error:0.36727\n",
      "[399]\ttrain-error:0.36709\n",
      "[400]\ttrain-error:0.36682\n",
      "[401]\ttrain-error:0.36691\n",
      "[402]\ttrain-error:0.36620\n",
      "[403]\ttrain-error:0.36700\n",
      "[404]\ttrain-error:0.36762\n",
      "[405]\ttrain-error:0.36798\n",
      "[406]\ttrain-error:0.36798\n",
      "[407]\ttrain-error:0.36816\n",
      "[408]\ttrain-error:0.36816\n",
      "[409]\ttrain-error:0.36718\n",
      "[410]\ttrain-error:0.36727\n",
      "[411]\ttrain-error:0.36753\n",
      "[412]\ttrain-error:0.36647\n",
      "[413]\ttrain-error:0.36647\n",
      "[414]\ttrain-error:0.36638\n",
      "[415]\ttrain-error:0.36647\n",
      "[416]\ttrain-error:0.36629\n",
      "[417]\ttrain-error:0.36567\n",
      "[418]\ttrain-error:0.36514\n",
      "[419]\ttrain-error:0.36567\n",
      "[420]\ttrain-error:0.36585\n",
      "[421]\ttrain-error:0.36585\n",
      "[422]\ttrain-error:0.36558\n",
      "[423]\ttrain-error:0.36585\n",
      "[424]\ttrain-error:0.36576\n",
      "[425]\ttrain-error:0.36594\n",
      "[426]\ttrain-error:0.36469\n",
      "[427]\ttrain-error:0.36381\n",
      "[428]\ttrain-error:0.36363\n",
      "[429]\ttrain-error:0.36345\n",
      "[430]\ttrain-error:0.36398\n",
      "[431]\ttrain-error:0.36390\n",
      "[432]\ttrain-error:0.36416\n",
      "[433]\ttrain-error:0.36390\n",
      "[434]\ttrain-error:0.36363\n",
      "[435]\ttrain-error:0.36292\n",
      "[436]\ttrain-error:0.36283\n",
      "[437]\ttrain-error:0.36150\n",
      "[438]\ttrain-error:0.36230\n",
      "[439]\ttrain-error:0.36221\n",
      "[440]\ttrain-error:0.36203\n",
      "[441]\ttrain-error:0.36203\n",
      "[442]\ttrain-error:0.36194\n",
      "[443]\ttrain-error:0.36185\n",
      "[444]\ttrain-error:0.36185\n",
      "[445]\ttrain-error:0.36168\n",
      "[446]\ttrain-error:0.36292\n",
      "[447]\ttrain-error:0.36239\n",
      "[448]\ttrain-error:0.36292\n",
      "[449]\ttrain-error:0.36283\n",
      "[450]\ttrain-error:0.36212\n",
      "[451]\ttrain-error:0.36168\n",
      "[452]\ttrain-error:0.36168\n",
      "[453]\ttrain-error:0.36105\n",
      "[454]\ttrain-error:0.36061\n",
      "[455]\ttrain-error:0.35990\n",
      "[456]\ttrain-error:0.36043\n",
      "[457]\ttrain-error:0.35981\n",
      "[458]\ttrain-error:0.36026\n",
      "[459]\ttrain-error:0.36061\n",
      "[460]\ttrain-error:0.36034\n",
      "[461]\ttrain-error:0.36043\n",
      "[462]\ttrain-error:0.36017\n",
      "[463]\ttrain-error:0.35999\n",
      "[464]\ttrain-error:0.35972\n",
      "[465]\ttrain-error:0.35990\n",
      "[466]\ttrain-error:0.35990\n",
      "[467]\ttrain-error:0.35999\n",
      "[468]\ttrain-error:0.35981\n",
      "[469]\ttrain-error:0.35963\n",
      "[470]\ttrain-error:0.35990\n",
      "[471]\ttrain-error:0.35990\n",
      "[472]\ttrain-error:0.35875\n",
      "[473]\ttrain-error:0.35866\n",
      "[474]\ttrain-error:0.35839\n",
      "[475]\ttrain-error:0.35848\n",
      "[476]\ttrain-error:0.35813\n",
      "[477]\ttrain-error:0.35706\n",
      "[478]\ttrain-error:0.35733\n",
      "[479]\ttrain-error:0.35715\n",
      "[480]\ttrain-error:0.35679\n",
      "[481]\ttrain-error:0.35617\n",
      "[482]\ttrain-error:0.35582\n",
      "[483]\ttrain-error:0.35600\n",
      "[484]\ttrain-error:0.35546\n",
      "[485]\ttrain-error:0.35546\n",
      "[486]\ttrain-error:0.35573\n",
      "[487]\ttrain-error:0.35546\n",
      "[488]\ttrain-error:0.35520\n",
      "[489]\ttrain-error:0.35529\n",
      "[490]\ttrain-error:0.35511\n",
      "[491]\ttrain-error:0.35484\n",
      "[492]\ttrain-error:0.35493\n",
      "[493]\ttrain-error:0.35466\n",
      "[494]\ttrain-error:0.35466\n",
      "[495]\ttrain-error:0.35422\n",
      "[496]\ttrain-error:0.35440\n",
      "[497]\ttrain-error:0.35440\n",
      "[498]\ttrain-error:0.35440\n",
      "[499]\ttrain-error:0.35449\n",
      "[500]\ttrain-error:0.35431\n",
      "[501]\ttrain-error:0.35440\n",
      "[502]\ttrain-error:0.35422\n",
      "[503]\ttrain-error:0.35395\n",
      "[504]\ttrain-error:0.35395\n",
      "[505]\ttrain-error:0.35395\n",
      "[506]\ttrain-error:0.35404\n",
      "[507]\ttrain-error:0.35422\n",
      "[508]\ttrain-error:0.35395\n",
      "[509]\ttrain-error:0.35386\n",
      "[510]\ttrain-error:0.35378\n",
      "[511]\ttrain-error:0.35386\n",
      "[512]\ttrain-error:0.35404\n",
      "[513]\ttrain-error:0.35315\n",
      "[514]\ttrain-error:0.35289\n",
      "[515]\ttrain-error:0.35271\n",
      "[516]\ttrain-error:0.35262\n",
      "[517]\ttrain-error:0.35245\n",
      "[518]\ttrain-error:0.35245\n",
      "[519]\ttrain-error:0.35245\n",
      "[520]\ttrain-error:0.35245\n",
      "[521]\ttrain-error:0.35245\n",
      "[522]\ttrain-error:0.35245\n",
      "[523]\ttrain-error:0.35245\n",
      "[524]\ttrain-error:0.35245\n",
      "[525]\ttrain-error:0.35245\n",
      "[526]\ttrain-error:0.35245\n",
      "[527]\ttrain-error:0.35245\n",
      "[528]\ttrain-error:0.35245\n",
      "[529]\ttrain-error:0.35245\n",
      "[530]\ttrain-error:0.35245\n",
      "[531]\ttrain-error:0.35245\n",
      "[532]\ttrain-error:0.35245\n",
      "[533]\ttrain-error:0.35245\n",
      "[534]\ttrain-error:0.35245\n",
      "[535]\ttrain-error:0.35245\n",
      "[536]\ttrain-error:0.35245\n",
      "[537]\ttrain-error:0.35245\n",
      "[538]\ttrain-error:0.35245\n",
      "[539]\ttrain-error:0.35245\n",
      "[540]\ttrain-error:0.35245\n",
      "[541]\ttrain-error:0.35245\n",
      "[542]\ttrain-error:0.35245\n",
      "[543]\ttrain-error:0.35245\n",
      "[544]\ttrain-error:0.35245\n",
      "[545]\ttrain-error:0.35245\n",
      "[546]\ttrain-error:0.35245\n",
      "[547]\ttrain-error:0.35245\n",
      "[548]\ttrain-error:0.35245\n",
      "[549]\ttrain-error:0.35245\n",
      "[550]\ttrain-error:0.35245\n",
      "[551]\ttrain-error:0.35245\n",
      "[552]\ttrain-error:0.35245\n",
      "[553]\ttrain-error:0.35245\n",
      "[554]\ttrain-error:0.35245\n",
      "[555]\ttrain-error:0.35245\n",
      "[556]\ttrain-error:0.35245\n",
      "[557]\ttrain-error:0.35245\n",
      "[558]\ttrain-error:0.35245\n",
      "[559]\ttrain-error:0.35245\n",
      "[560]\ttrain-error:0.35245\n",
      "[561]\ttrain-error:0.35245\n",
      "[562]\ttrain-error:0.35245\n",
      "[563]\ttrain-error:0.35245\n",
      "[564]\ttrain-error:0.35245\n",
      "[565]\ttrain-error:0.35245\n",
      "[566]\ttrain-error:0.35245\n",
      "[567]\ttrain-error:0.35245\n",
      "[568]\ttrain-error:0.35245\n",
      "[569]\ttrain-error:0.35245\n",
      "[570]\ttrain-error:0.35245\n",
      "[571]\ttrain-error:0.35245\n",
      "[572]\ttrain-error:0.35245\n",
      "[573]\ttrain-error:0.35245\n",
      "[574]\ttrain-error:0.35245\n",
      "[575]\ttrain-error:0.35245\n",
      "[576]\ttrain-error:0.35245\n",
      "[577]\ttrain-error:0.35245\n",
      "[578]\ttrain-error:0.35245\n",
      "[579]\ttrain-error:0.35245\n",
      "[580]\ttrain-error:0.35245\n",
      "[581]\ttrain-error:0.35245\n",
      "[582]\ttrain-error:0.35245\n",
      "[583]\ttrain-error:0.35245\n",
      "[584]\ttrain-error:0.35245\n",
      "[585]\ttrain-error:0.35245\n",
      "[586]\ttrain-error:0.35245\n",
      "[587]\ttrain-error:0.35245\n",
      "[588]\ttrain-error:0.35245\n",
      "[589]\ttrain-error:0.35245\n",
      "[590]\ttrain-error:0.35245\n",
      "[591]\ttrain-error:0.35245\n",
      "[592]\ttrain-error:0.35245\n",
      "[593]\ttrain-error:0.35245\n",
      "[594]\ttrain-error:0.35245\n",
      "[595]\ttrain-error:0.35245\n",
      "[596]\ttrain-error:0.35245\n",
      "[597]\ttrain-error:0.35245\n",
      "[598]\ttrain-error:0.35245\n",
      "[599]\ttrain-error:0.35245\n",
      "[600]\ttrain-error:0.35245\n",
      "[601]\ttrain-error:0.35245\n",
      "[602]\ttrain-error:0.35245\n",
      "[603]\ttrain-error:0.35245\n",
      "[604]\ttrain-error:0.35245\n",
      "[605]\ttrain-error:0.35245\n",
      "[606]\ttrain-error:0.35245\n",
      "[607]\ttrain-error:0.35245\n",
      "[608]\ttrain-error:0.35245\n",
      "[609]\ttrain-error:0.35245\n",
      "[610]\ttrain-error:0.35245\n",
      "[611]\ttrain-error:0.35245\n",
      "[612]\ttrain-error:0.35245\n",
      "[613]\ttrain-error:0.35245\n",
      "[614]\ttrain-error:0.35245\n",
      "[615]\ttrain-error:0.35245\n",
      "[616]\ttrain-error:0.35245\n",
      "[617]\ttrain-error:0.35245\n",
      "[618]\ttrain-error:0.35245\n",
      "[619]\ttrain-error:0.35245\n",
      "[620]\ttrain-error:0.35245\n",
      "[621]\ttrain-error:0.35245\n",
      "[622]\ttrain-error:0.35245\n",
      "[623]\ttrain-error:0.35245\n",
      "[624]\ttrain-error:0.35245\n",
      "[625]\ttrain-error:0.35245\n",
      "[626]\ttrain-error:0.35245\n",
      "[627]\ttrain-error:0.35245\n",
      "[628]\ttrain-error:0.35245\n",
      "[629]\ttrain-error:0.35245\n",
      "[630]\ttrain-error:0.35245\n",
      "[631]\ttrain-error:0.35245\n",
      "[632]\ttrain-error:0.35245\n",
      "[633]\ttrain-error:0.35245\n",
      "[634]\ttrain-error:0.35245\n",
      "[635]\ttrain-error:0.35245\n",
      "[636]\ttrain-error:0.35245\n",
      "[637]\ttrain-error:0.35245\n",
      "[638]\ttrain-error:0.35245\n",
      "[639]\ttrain-error:0.35245\n",
      "[640]\ttrain-error:0.35245\n",
      "[641]\ttrain-error:0.35245\n",
      "[642]\ttrain-error:0.35245\n",
      "[643]\ttrain-error:0.35245\n",
      "[644]\ttrain-error:0.35245\n",
      "[645]\ttrain-error:0.35245\n",
      "[646]\ttrain-error:0.35245\n",
      "[647]\ttrain-error:0.35245\n",
      "[648]\ttrain-error:0.35245\n",
      "[649]\ttrain-error:0.35245\n",
      "[650]\ttrain-error:0.35245\n",
      "[651]\ttrain-error:0.35245\n",
      "[652]\ttrain-error:0.35245\n",
      "[653]\ttrain-error:0.35245\n",
      "[654]\ttrain-error:0.35245\n",
      "[655]\ttrain-error:0.35245\n",
      "[656]\ttrain-error:0.35245\n",
      "[657]\ttrain-error:0.35245\n",
      "[658]\ttrain-error:0.35245\n",
      "[659]\ttrain-error:0.35245\n",
      "[660]\ttrain-error:0.35245\n",
      "[661]\ttrain-error:0.35245\n",
      "[662]\ttrain-error:0.35245\n",
      "[663]\ttrain-error:0.35245\n",
      "[664]\ttrain-error:0.35245\n",
      "[665]\ttrain-error:0.35245\n",
      "[666]\ttrain-error:0.35245\n",
      "[667]\ttrain-error:0.35245\n",
      "[668]\ttrain-error:0.35245\n",
      "[669]\ttrain-error:0.35245\n",
      "[670]\ttrain-error:0.35245\n",
      "[671]\ttrain-error:0.35245\n",
      "[672]\ttrain-error:0.35245\n",
      "[673]\ttrain-error:0.35245\n",
      "[674]\ttrain-error:0.35245\n",
      "[675]\ttrain-error:0.35245\n",
      "[676]\ttrain-error:0.35245\n",
      "[677]\ttrain-error:0.35245\n",
      "[678]\ttrain-error:0.35245\n",
      "[679]\ttrain-error:0.35245\n",
      "[680]\ttrain-error:0.35245\n",
      "[681]\ttrain-error:0.35245\n",
      "[682]\ttrain-error:0.35245\n",
      "[683]\ttrain-error:0.35245\n",
      "[684]\ttrain-error:0.35245\n",
      "[685]\ttrain-error:0.35245\n",
      "[686]\ttrain-error:0.35245\n",
      "[687]\ttrain-error:0.35245\n",
      "[688]\ttrain-error:0.35245\n",
      "[689]\ttrain-error:0.35245\n",
      "[690]\ttrain-error:0.35245\n",
      "[691]\ttrain-error:0.35245\n",
      "[692]\ttrain-error:0.35245\n",
      "[693]\ttrain-error:0.35245\n",
      "[694]\ttrain-error:0.35245\n",
      "[695]\ttrain-error:0.35245\n",
      "[696]\ttrain-error:0.35245\n",
      "[697]\ttrain-error:0.35245\n",
      "[698]\ttrain-error:0.35245\n",
      "[699]\ttrain-error:0.35245\n",
      "[700]\ttrain-error:0.35245\n",
      "[701]\ttrain-error:0.35245\n",
      "[702]\ttrain-error:0.35245\n",
      "[703]\ttrain-error:0.35245\n",
      "[704]\ttrain-error:0.35245\n",
      "[705]\ttrain-error:0.35245\n",
      "[706]\ttrain-error:0.35245\n",
      "[707]\ttrain-error:0.35245\n",
      "[708]\ttrain-error:0.35245\n",
      "[709]\ttrain-error:0.35245\n",
      "[710]\ttrain-error:0.35245\n",
      "[711]\ttrain-error:0.35245\n",
      "[712]\ttrain-error:0.35245\n",
      "[713]\ttrain-error:0.35245\n",
      "[714]\ttrain-error:0.35245\n",
      "[715]\ttrain-error:0.35245\n",
      "[716]\ttrain-error:0.35245\n",
      "[717]\ttrain-error:0.35245\n",
      "[718]\ttrain-error:0.35245\n",
      "[719]\ttrain-error:0.35245\n",
      "[720]\ttrain-error:0.35245\n",
      "[721]\ttrain-error:0.35245\n",
      "[722]\ttrain-error:0.35245\n",
      "[723]\ttrain-error:0.35245\n",
      "[724]\ttrain-error:0.35245\n",
      "[725]\ttrain-error:0.35245\n",
      "[726]\ttrain-error:0.35245\n",
      "[727]\ttrain-error:0.35245\n",
      "[728]\ttrain-error:0.35245\n",
      "[729]\ttrain-error:0.35245\n",
      "[730]\ttrain-error:0.35245\n",
      "[731]\ttrain-error:0.35245\n",
      "[732]\ttrain-error:0.35245\n",
      "[733]\ttrain-error:0.35245\n",
      "[734]\ttrain-error:0.35245\n",
      "[735]\ttrain-error:0.35245\n",
      "[736]\ttrain-error:0.35245\n",
      "[737]\ttrain-error:0.35245\n",
      "[738]\ttrain-error:0.35245\n",
      "[739]\ttrain-error:0.35245\n",
      "[740]\ttrain-error:0.35245\n",
      "[741]\ttrain-error:0.35245\n",
      "[742]\ttrain-error:0.35245\n",
      "[743]\ttrain-error:0.35245\n",
      "[744]\ttrain-error:0.35245\n",
      "[745]\ttrain-error:0.35245\n",
      "[746]\ttrain-error:0.35245\n",
      "[747]\ttrain-error:0.35245\n",
      "[748]\ttrain-error:0.35245\n",
      "[749]\ttrain-error:0.35245\n",
      "[750]\ttrain-error:0.35245\n",
      "[751]\ttrain-error:0.35245\n",
      "[752]\ttrain-error:0.35245\n",
      "[753]\ttrain-error:0.35245\n",
      "[754]\ttrain-error:0.35245\n",
      "[755]\ttrain-error:0.35245\n",
      "[756]\ttrain-error:0.35245\n",
      "[757]\ttrain-error:0.35245\n",
      "[758]\ttrain-error:0.35245\n",
      "[759]\ttrain-error:0.35245\n",
      "[760]\ttrain-error:0.35245\n",
      "[761]\ttrain-error:0.35245\n",
      "[762]\ttrain-error:0.35245\n",
      "[763]\ttrain-error:0.35245\n",
      "[764]\ttrain-error:0.35245\n",
      "[765]\ttrain-error:0.35245\n",
      "[766]\ttrain-error:0.35245\n",
      "[767]\ttrain-error:0.35245\n",
      "[768]\ttrain-error:0.35245\n",
      "[769]\ttrain-error:0.35245\n",
      "[770]\ttrain-error:0.35245\n",
      "[771]\ttrain-error:0.35245\n",
      "[772]\ttrain-error:0.35245\n",
      "[773]\ttrain-error:0.35245\n",
      "[774]\ttrain-error:0.35245\n",
      "[775]\ttrain-error:0.35245\n",
      "[776]\ttrain-error:0.35245\n",
      "[777]\ttrain-error:0.35245\n",
      "[778]\ttrain-error:0.35245\n",
      "[779]\ttrain-error:0.35245\n",
      "[780]\ttrain-error:0.35245\n",
      "[781]\ttrain-error:0.35245\n",
      "[782]\ttrain-error:0.35245\n",
      "[783]\ttrain-error:0.35245\n",
      "[784]\ttrain-error:0.35245\n",
      "[785]\ttrain-error:0.35245\n",
      "[786]\ttrain-error:0.35245\n",
      "[787]\ttrain-error:0.35245\n",
      "[788]\ttrain-error:0.35245\n",
      "[789]\ttrain-error:0.35245\n",
      "[790]\ttrain-error:0.35245\n",
      "[791]\ttrain-error:0.35245\n",
      "[792]\ttrain-error:0.35245\n",
      "[793]\ttrain-error:0.35245\n",
      "[794]\ttrain-error:0.35245\n",
      "[795]\ttrain-error:0.35245\n",
      "[796]\ttrain-error:0.35245\n",
      "[797]\ttrain-error:0.35245\n",
      "[798]\ttrain-error:0.35245\n",
      "[799]\ttrain-error:0.35245\n",
      "[800]\ttrain-error:0.35245\n",
      "[801]\ttrain-error:0.35245\n",
      "[802]\ttrain-error:0.35245\n",
      "[803]\ttrain-error:0.35245\n",
      "[804]\ttrain-error:0.35245\n",
      "[805]\ttrain-error:0.35245\n",
      "[806]\ttrain-error:0.35245\n",
      "[807]\ttrain-error:0.35245\n",
      "[808]\ttrain-error:0.35245\n",
      "[809]\ttrain-error:0.35245\n",
      "[810]\ttrain-error:0.35245\n",
      "[811]\ttrain-error:0.35245\n",
      "[812]\ttrain-error:0.35245\n",
      "[813]\ttrain-error:0.35245\n",
      "[814]\ttrain-error:0.35245\n",
      "[815]\ttrain-error:0.35245\n",
      "[816]\ttrain-error:0.35245\n",
      "[817]\ttrain-error:0.35245\n",
      "[818]\ttrain-error:0.35245\n",
      "[819]\ttrain-error:0.35245\n",
      "[820]\ttrain-error:0.35245\n",
      "[821]\ttrain-error:0.35245\n",
      "[822]\ttrain-error:0.35245\n",
      "[823]\ttrain-error:0.35245\n",
      "[824]\ttrain-error:0.35245\n",
      "[825]\ttrain-error:0.35245\n",
      "[826]\ttrain-error:0.35245\n",
      "[827]\ttrain-error:0.35245\n",
      "[828]\ttrain-error:0.35245\n",
      "[829]\ttrain-error:0.35245\n",
      "[830]\ttrain-error:0.35245\n",
      "[831]\ttrain-error:0.35245\n",
      "[832]\ttrain-error:0.35245\n",
      "[833]\ttrain-error:0.35245\n",
      "[834]\ttrain-error:0.35245\n",
      "[835]\ttrain-error:0.35245\n",
      "[836]\ttrain-error:0.35245\n",
      "[837]\ttrain-error:0.35245\n",
      "[838]\ttrain-error:0.35245\n",
      "[839]\ttrain-error:0.35245\n",
      "[840]\ttrain-error:0.35245\n",
      "[841]\ttrain-error:0.35245\n",
      "[842]\ttrain-error:0.35245\n",
      "[843]\ttrain-error:0.35245\n",
      "[844]\ttrain-error:0.35245\n",
      "[845]\ttrain-error:0.35245\n",
      "[846]\ttrain-error:0.35245\n",
      "[847]\ttrain-error:0.35245\n",
      "[848]\ttrain-error:0.35245\n",
      "[849]\ttrain-error:0.35245\n",
      "[850]\ttrain-error:0.35245\n",
      "[851]\ttrain-error:0.35245\n",
      "[852]\ttrain-error:0.35245\n",
      "[853]\ttrain-error:0.35245\n",
      "[854]\ttrain-error:0.35245\n",
      "[855]\ttrain-error:0.35245\n",
      "[856]\ttrain-error:0.35245\n",
      "[857]\ttrain-error:0.35245\n",
      "[858]\ttrain-error:0.35245\n",
      "[859]\ttrain-error:0.35245\n",
      "[860]\ttrain-error:0.35245\n",
      "[861]\ttrain-error:0.35245\n",
      "[862]\ttrain-error:0.35245\n",
      "[863]\ttrain-error:0.35245\n",
      "[864]\ttrain-error:0.35245\n",
      "[865]\ttrain-error:0.35245\n",
      "[866]\ttrain-error:0.35245\n",
      "[867]\ttrain-error:0.35245\n",
      "[868]\ttrain-error:0.35245\n",
      "[869]\ttrain-error:0.35245\n",
      "[870]\ttrain-error:0.35245\n",
      "[871]\ttrain-error:0.35245\n",
      "[872]\ttrain-error:0.35245\n",
      "[873]\ttrain-error:0.35245\n",
      "[874]\ttrain-error:0.35245\n",
      "[875]\ttrain-error:0.35245\n",
      "[876]\ttrain-error:0.35245\n",
      "[877]\ttrain-error:0.35245\n",
      "[878]\ttrain-error:0.35245\n",
      "[879]\ttrain-error:0.35245\n",
      "[880]\ttrain-error:0.35245\n",
      "[881]\ttrain-error:0.35245\n",
      "[882]\ttrain-error:0.35245\n",
      "[883]\ttrain-error:0.35245\n",
      "[884]\ttrain-error:0.35245\n",
      "[885]\ttrain-error:0.35245\n",
      "[886]\ttrain-error:0.35245\n",
      "[887]\ttrain-error:0.35245\n",
      "[888]\ttrain-error:0.35245\n",
      "[889]\ttrain-error:0.35245\n",
      "[890]\ttrain-error:0.35245\n",
      "[891]\ttrain-error:0.35245\n",
      "[892]\ttrain-error:0.35245\n",
      "[893]\ttrain-error:0.35245\n",
      "[894]\ttrain-error:0.35245\n",
      "[895]\ttrain-error:0.35245\n",
      "[896]\ttrain-error:0.35245\n",
      "[897]\ttrain-error:0.35245\n",
      "[898]\ttrain-error:0.35245\n",
      "[899]\ttrain-error:0.35245\n",
      "[900]\ttrain-error:0.35245\n",
      "[901]\ttrain-error:0.35245\n",
      "[902]\ttrain-error:0.35245\n",
      "[903]\ttrain-error:0.35245\n",
      "[904]\ttrain-error:0.35245\n",
      "[905]\ttrain-error:0.35245\n",
      "[906]\ttrain-error:0.35245\n",
      "[907]\ttrain-error:0.35245\n",
      "[908]\ttrain-error:0.35245\n",
      "[909]\ttrain-error:0.35245\n",
      "[910]\ttrain-error:0.35245\n",
      "[911]\ttrain-error:0.35245\n",
      "[912]\ttrain-error:0.35245\n",
      "[913]\ttrain-error:0.35245\n",
      "[914]\ttrain-error:0.35245\n",
      "[915]\ttrain-error:0.35245\n",
      "[916]\ttrain-error:0.35245\n",
      "[917]\ttrain-error:0.35245\n",
      "[918]\ttrain-error:0.35245\n",
      "[919]\ttrain-error:0.35245\n",
      "[920]\ttrain-error:0.35245\n",
      "[921]\ttrain-error:0.35245\n",
      "[922]\ttrain-error:0.35245\n",
      "[923]\ttrain-error:0.35245\n",
      "[924]\ttrain-error:0.35245\n",
      "[925]\ttrain-error:0.35245\n",
      "[926]\ttrain-error:0.35245\n",
      "[927]\ttrain-error:0.35245\n",
      "[928]\ttrain-error:0.35245\n",
      "[929]\ttrain-error:0.35245\n",
      "[930]\ttrain-error:0.35245\n",
      "[931]\ttrain-error:0.35245\n",
      "[932]\ttrain-error:0.35245\n",
      "[933]\ttrain-error:0.35245\n",
      "[934]\ttrain-error:0.35245\n",
      "[935]\ttrain-error:0.35245\n",
      "[936]\ttrain-error:0.35245\n",
      "[937]\ttrain-error:0.35245\n",
      "[938]\ttrain-error:0.35245\n",
      "[939]\ttrain-error:0.35245\n",
      "[940]\ttrain-error:0.35245\n",
      "[941]\ttrain-error:0.35245\n",
      "[942]\ttrain-error:0.35245\n",
      "[943]\ttrain-error:0.35245\n",
      "[944]\ttrain-error:0.35245\n",
      "[945]\ttrain-error:0.35245\n",
      "[946]\ttrain-error:0.35245\n",
      "[947]\ttrain-error:0.35245\n",
      "[948]\ttrain-error:0.35245\n",
      "[949]\ttrain-error:0.35245\n",
      "[950]\ttrain-error:0.35245\n",
      "[951]\ttrain-error:0.35245\n",
      "[952]\ttrain-error:0.35245\n",
      "[953]\ttrain-error:0.35245\n",
      "[954]\ttrain-error:0.35245\n",
      "[955]\ttrain-error:0.35245\n",
      "[956]\ttrain-error:0.35245\n",
      "[957]\ttrain-error:0.35245\n",
      "[958]\ttrain-error:0.35245\n",
      "[959]\ttrain-error:0.35245\n",
      "[960]\ttrain-error:0.35245\n",
      "[961]\ttrain-error:0.35245\n",
      "[962]\ttrain-error:0.35245\n",
      "[963]\ttrain-error:0.35245\n",
      "[964]\ttrain-error:0.35245\n",
      "[965]\ttrain-error:0.35245\n",
      "[966]\ttrain-error:0.35245\n",
      "[967]\ttrain-error:0.35245\n",
      "[968]\ttrain-error:0.35245\n",
      "[969]\ttrain-error:0.35245\n",
      "[970]\ttrain-error:0.35245\n",
      "[971]\ttrain-error:0.35245\n",
      "[972]\ttrain-error:0.35245\n",
      "[973]\ttrain-error:0.35245\n",
      "[974]\ttrain-error:0.35245\n",
      "[975]\ttrain-error:0.35245\n",
      "[976]\ttrain-error:0.35245\n",
      "[977]\ttrain-error:0.35245\n",
      "[978]\ttrain-error:0.35245\n",
      "[979]\ttrain-error:0.35245\n",
      "[980]\ttrain-error:0.35245\n",
      "[981]\ttrain-error:0.35245\n",
      "[982]\ttrain-error:0.35245\n",
      "[983]\ttrain-error:0.35245\n",
      "[984]\ttrain-error:0.35245\n",
      "[985]\ttrain-error:0.35245\n",
      "[986]\ttrain-error:0.35245\n",
      "[987]\ttrain-error:0.35245\n",
      "[988]\ttrain-error:0.35245\n",
      "[989]\ttrain-error:0.35245\n",
      "[990]\ttrain-error:0.35245\n",
      "[991]\ttrain-error:0.35245\n",
      "[992]\ttrain-error:0.35245\n",
      "[993]\ttrain-error:0.35245\n",
      "[994]\ttrain-error:0.35245\n",
      "[995]\ttrain-error:0.35245\n",
      "[996]\ttrain-error:0.35245\n",
      "[997]\ttrain-error:0.35245\n",
      "[998]\ttrain-error:0.35245\n",
      "[999]\ttrain-error:0.35245\n",
      "[1000]\ttrain-error:0.35245\n",
      "[1001]\ttrain-error:0.35245\n",
      "[1002]\ttrain-error:0.35245\n",
      "[1003]\ttrain-error:0.35245\n",
      "[1004]\ttrain-error:0.35245\n",
      "[1005]\ttrain-error:0.35245\n",
      "[1006]\ttrain-error:0.35245\n",
      "[1007]\ttrain-error:0.35245\n",
      "[1008]\ttrain-error:0.35245\n",
      "[1009]\ttrain-error:0.35245\n",
      "[1010]\ttrain-error:0.35245\n",
      "[1011]\ttrain-error:0.35245\n",
      "[1012]\ttrain-error:0.35245\n",
      "[1013]\ttrain-error:0.35245\n",
      "[1014]\ttrain-error:0.35245\n",
      "[1015]\ttrain-error:0.35245\n",
      "[1016]\ttrain-error:0.35245\n",
      "[1017]\ttrain-error:0.35245\n",
      "[1018]\ttrain-error:0.35245\n",
      "[1019]\ttrain-error:0.35245\n",
      "[1020]\ttrain-error:0.35245\n",
      "[1021]\ttrain-error:0.35245\n",
      "[1022]\ttrain-error:0.35245\n",
      "[1023]\ttrain-error:0.35245\n",
      "[1024]\ttrain-error:0.35245\n",
      "[1025]\ttrain-error:0.35245\n",
      "[1026]\ttrain-error:0.35245\n",
      "[1027]\ttrain-error:0.35245\n",
      "[1028]\ttrain-error:0.35245\n",
      "[1029]\ttrain-error:0.35245\n",
      "[1030]\ttrain-error:0.35245\n",
      "[1031]\ttrain-error:0.35245\n",
      "[1032]\ttrain-error:0.35245\n",
      "[1033]\ttrain-error:0.35245\n",
      "[1034]\ttrain-error:0.35245\n",
      "[1035]\ttrain-error:0.35245\n",
      "[1036]\ttrain-error:0.35245\n",
      "[1037]\ttrain-error:0.35245\n",
      "[1038]\ttrain-error:0.35245\n",
      "[1039]\ttrain-error:0.35245\n",
      "[1040]\ttrain-error:0.35245\n",
      "[1041]\ttrain-error:0.35245\n",
      "[1042]\ttrain-error:0.35245\n",
      "[1043]\ttrain-error:0.35245\n",
      "[1044]\ttrain-error:0.35245\n",
      "[1045]\ttrain-error:0.35245\n",
      "[1046]\ttrain-error:0.35245\n",
      "[1047]\ttrain-error:0.35245\n",
      "[1048]\ttrain-error:0.35245\n",
      "[1049]\ttrain-error:0.35245\n",
      "[1050]\ttrain-error:0.35245\n",
      "[1051]\ttrain-error:0.35245\n",
      "[1052]\ttrain-error:0.35245\n",
      "[1053]\ttrain-error:0.35245\n",
      "[1054]\ttrain-error:0.35245\n",
      "[1055]\ttrain-error:0.35245\n",
      "[1056]\ttrain-error:0.35245\n",
      "[1057]\ttrain-error:0.35245\n",
      "[1058]\ttrain-error:0.35245\n",
      "[1059]\ttrain-error:0.35245\n",
      "[1060]\ttrain-error:0.35245\n",
      "[1061]\ttrain-error:0.35245\n",
      "[1062]\ttrain-error:0.35245\n",
      "[1063]\ttrain-error:0.35245\n",
      "[1064]\ttrain-error:0.35245\n",
      "[1065]\ttrain-error:0.35245\n",
      "[1066]\ttrain-error:0.35245\n",
      "[1067]\ttrain-error:0.35245\n",
      "[1068]\ttrain-error:0.35245\n",
      "[1069]\ttrain-error:0.35245\n",
      "[1070]\ttrain-error:0.35245\n",
      "[1071]\ttrain-error:0.35245\n",
      "[1072]\ttrain-error:0.35245\n",
      "[1073]\ttrain-error:0.35245\n",
      "[1074]\ttrain-error:0.35245\n",
      "[1075]\ttrain-error:0.35245\n",
      "[1076]\ttrain-error:0.35245\n",
      "[1077]\ttrain-error:0.35245\n",
      "[1078]\ttrain-error:0.35245\n",
      "[1079]\ttrain-error:0.35245\n",
      "[1080]\ttrain-error:0.35245\n",
      "[1081]\ttrain-error:0.35245\n",
      "[1082]\ttrain-error:0.35245\n",
      "[1083]\ttrain-error:0.35245\n",
      "[1084]\ttrain-error:0.35245\n",
      "[1085]\ttrain-error:0.35245\n",
      "[1086]\ttrain-error:0.35245\n",
      "[1087]\ttrain-error:0.35245\n",
      "[1088]\ttrain-error:0.35245\n",
      "[1089]\ttrain-error:0.35245\n",
      "[1090]\ttrain-error:0.35245\n",
      "[1091]\ttrain-error:0.35245\n",
      "[1092]\ttrain-error:0.35245\n",
      "[1093]\ttrain-error:0.35245\n",
      "[1094]\ttrain-error:0.35245\n",
      "[1095]\ttrain-error:0.35245\n",
      "[1096]\ttrain-error:0.35245\n",
      "[1097]\ttrain-error:0.35245\n",
      "[1098]\ttrain-error:0.35245\n",
      "[1099]\ttrain-error:0.35245\n",
      "[1100]\ttrain-error:0.35245\n",
      "[1101]\ttrain-error:0.35245\n",
      "[1102]\ttrain-error:0.35245\n",
      "[1103]\ttrain-error:0.35245\n",
      "[1104]\ttrain-error:0.35245\n",
      "[1105]\ttrain-error:0.35245\n",
      "[1106]\ttrain-error:0.35245\n",
      "[1107]\ttrain-error:0.35245\n",
      "[1108]\ttrain-error:0.35245\n",
      "[1109]\ttrain-error:0.35245\n",
      "[1110]\ttrain-error:0.35245\n",
      "[1111]\ttrain-error:0.35245\n",
      "[1112]\ttrain-error:0.35245\n",
      "[1113]\ttrain-error:0.35245\n",
      "[1114]\ttrain-error:0.35245\n",
      "[1115]\ttrain-error:0.35245\n",
      "[1116]\ttrain-error:0.35245\n",
      "[1117]\ttrain-error:0.35245\n",
      "[1118]\ttrain-error:0.35245\n",
      "[1119]\ttrain-error:0.35245\n",
      "[1120]\ttrain-error:0.35245\n",
      "[1121]\ttrain-error:0.35245\n",
      "[1122]\ttrain-error:0.35245\n",
      "[1123]\ttrain-error:0.35245\n",
      "[1124]\ttrain-error:0.35245\n",
      "[1125]\ttrain-error:0.35245\n",
      "[1126]\ttrain-error:0.35245\n",
      "[1127]\ttrain-error:0.35245\n",
      "[1128]\ttrain-error:0.35245\n",
      "[1129]\ttrain-error:0.35245\n",
      "[1130]\ttrain-error:0.35245\n",
      "[1131]\ttrain-error:0.35245\n",
      "[1132]\ttrain-error:0.35245\n",
      "[1133]\ttrain-error:0.35245\n",
      "[1134]\ttrain-error:0.35245\n",
      "[1135]\ttrain-error:0.35245\n",
      "[1136]\ttrain-error:0.35245\n",
      "[1137]\ttrain-error:0.35245\n",
      "[1138]\ttrain-error:0.35245\n",
      "[1139]\ttrain-error:0.35245\n",
      "[1140]\ttrain-error:0.35245\n",
      "[1141]\ttrain-error:0.35245\n",
      "[1142]\ttrain-error:0.35245\n",
      "[1143]\ttrain-error:0.35245\n",
      "[1144]\ttrain-error:0.35245\n",
      "[1145]\ttrain-error:0.35245\n",
      "[1146]\ttrain-error:0.35245\n",
      "[1147]\ttrain-error:0.35245\n",
      "[1148]\ttrain-error:0.35245\n",
      "[1149]\ttrain-error:0.35245\n",
      "[1150]\ttrain-error:0.35245\n",
      "[1151]\ttrain-error:0.35245\n",
      "[1152]\ttrain-error:0.35245\n",
      "[1153]\ttrain-error:0.35245\n",
      "[1154]\ttrain-error:0.35245\n",
      "[1155]\ttrain-error:0.35245\n",
      "[1156]\ttrain-error:0.35245\n",
      "[1157]\ttrain-error:0.35245\n",
      "[1158]\ttrain-error:0.35245\n",
      "[1159]\ttrain-error:0.35245\n",
      "[1160]\ttrain-error:0.35245\n",
      "[1161]\ttrain-error:0.35245\n",
      "[1162]\ttrain-error:0.35245\n",
      "[1163]\ttrain-error:0.35245\n",
      "[1164]\ttrain-error:0.35245\n",
      "[1165]\ttrain-error:0.35245\n",
      "[1166]\ttrain-error:0.35245\n",
      "[1167]\ttrain-error:0.35245\n",
      "[1168]\ttrain-error:0.35245\n",
      "[1169]\ttrain-error:0.35245\n",
      "[1170]\ttrain-error:0.35245\n",
      "[1171]\ttrain-error:0.35245\n",
      "[1172]\ttrain-error:0.35245\n",
      "[1173]\ttrain-error:0.35245\n",
      "[1174]\ttrain-error:0.35245\n",
      "[1175]\ttrain-error:0.35245\n",
      "[1176]\ttrain-error:0.35245\n",
      "[1177]\ttrain-error:0.35245\n",
      "[1178]\ttrain-error:0.35245\n",
      "[1179]\ttrain-error:0.35245\n",
      "[1180]\ttrain-error:0.35245\n",
      "[1181]\ttrain-error:0.35245\n",
      "[1182]\ttrain-error:0.35245\n",
      "[1183]\ttrain-error:0.35245\n",
      "[1184]\ttrain-error:0.35245\n",
      "[1185]\ttrain-error:0.35245\n",
      "[1186]\ttrain-error:0.35245\n",
      "[1187]\ttrain-error:0.35245\n",
      "[1188]\ttrain-error:0.35245\n",
      "[1189]\ttrain-error:0.35245\n",
      "[1190]\ttrain-error:0.35245\n",
      "[1191]\ttrain-error:0.35245\n",
      "[1192]\ttrain-error:0.35245\n",
      "[1193]\ttrain-error:0.35245\n",
      "[1194]\ttrain-error:0.35245\n",
      "[1195]\ttrain-error:0.35245\n",
      "[1196]\ttrain-error:0.35245\n",
      "[1197]\ttrain-error:0.35245\n",
      "[1198]\ttrain-error:0.35245\n",
      "[1199]\ttrain-error:0.35245\n",
      "[1200]\ttrain-error:0.35245\n",
      "[1201]\ttrain-error:0.35245\n",
      "[1202]\ttrain-error:0.35245\n",
      "[1203]\ttrain-error:0.35245\n",
      "[1204]\ttrain-error:0.35245\n",
      "[1205]\ttrain-error:0.35245\n",
      "[1206]\ttrain-error:0.35245\n",
      "[1207]\ttrain-error:0.35245\n",
      "[1208]\ttrain-error:0.35245\n",
      "[1209]\ttrain-error:0.35245\n",
      "[1210]\ttrain-error:0.35245\n",
      "[1211]\ttrain-error:0.35245\n",
      "[1212]\ttrain-error:0.35245\n",
      "[1213]\ttrain-error:0.35245\n",
      "[1214]\ttrain-error:0.35245\n",
      "[1215]\ttrain-error:0.35245\n",
      "[1216]\ttrain-error:0.35245\n",
      "[1217]\ttrain-error:0.35245\n",
      "[1218]\ttrain-error:0.35245\n",
      "[1219]\ttrain-error:0.35245\n",
      "[1220]\ttrain-error:0.35245\n",
      "[1221]\ttrain-error:0.35245\n",
      "[1222]\ttrain-error:0.35245\n",
      "[1223]\ttrain-error:0.35245\n",
      "[1224]\ttrain-error:0.35245\n",
      "[1225]\ttrain-error:0.35245\n",
      "[1226]\ttrain-error:0.35245\n",
      "[1227]\ttrain-error:0.35245\n",
      "[1228]\ttrain-error:0.35245\n",
      "[1229]\ttrain-error:0.35245\n",
      "[1230]\ttrain-error:0.35245\n",
      "[1231]\ttrain-error:0.35245\n",
      "[1232]\ttrain-error:0.35245\n",
      "[1233]\ttrain-error:0.35245\n",
      "[1234]\ttrain-error:0.35245\n",
      "[1235]\ttrain-error:0.35245\n",
      "[1236]\ttrain-error:0.35245\n",
      "[1237]\ttrain-error:0.35245\n",
      "[1238]\ttrain-error:0.35245\n",
      "[1239]\ttrain-error:0.35245\n",
      "[1240]\ttrain-error:0.35245\n",
      "[1241]\ttrain-error:0.35245\n",
      "[1242]\ttrain-error:0.35245\n",
      "[1243]\ttrain-error:0.35245\n",
      "[1244]\ttrain-error:0.35245\n",
      "[1245]\ttrain-error:0.35245\n",
      "[1246]\ttrain-error:0.35245\n",
      "[1247]\ttrain-error:0.35245\n",
      "[1248]\ttrain-error:0.35245\n",
      "[1249]\ttrain-error:0.35245\n",
      "[1250]\ttrain-error:0.35245\n",
      "[1251]\ttrain-error:0.35245\n",
      "[1252]\ttrain-error:0.35245\n",
      "[1253]\ttrain-error:0.35245\n",
      "[1254]\ttrain-error:0.35245\n",
      "[1255]\ttrain-error:0.35245\n",
      "[1256]\ttrain-error:0.35245\n",
      "[1257]\ttrain-error:0.35245\n",
      "[1258]\ttrain-error:0.35245\n",
      "[1259]\ttrain-error:0.35245\n",
      "[1260]\ttrain-error:0.35245\n",
      "[1261]\ttrain-error:0.35245\n",
      "[1262]\ttrain-error:0.35245\n",
      "[1263]\ttrain-error:0.35245\n",
      "[1264]\ttrain-error:0.35245\n",
      "[1265]\ttrain-error:0.35245\n",
      "[1266]\ttrain-error:0.35245\n",
      "[1267]\ttrain-error:0.35245\n",
      "[1268]\ttrain-error:0.35245\n",
      "[1269]\ttrain-error:0.35245\n",
      "[1270]\ttrain-error:0.35245\n",
      "[1271]\ttrain-error:0.35245\n",
      "[1272]\ttrain-error:0.35245\n",
      "[1273]\ttrain-error:0.35245\n",
      "[1274]\ttrain-error:0.35245\n",
      "[1275]\ttrain-error:0.35245\n",
      "[1276]\ttrain-error:0.35245\n",
      "[1277]\ttrain-error:0.35245\n",
      "[1278]\ttrain-error:0.35245\n",
      "[1279]\ttrain-error:0.35245\n",
      "[1280]\ttrain-error:0.35245\n",
      "[1281]\ttrain-error:0.35245\n",
      "[1282]\ttrain-error:0.35245\n",
      "[1283]\ttrain-error:0.35245\n",
      "[1284]\ttrain-error:0.35245\n",
      "[1285]\ttrain-error:0.35245\n",
      "[1286]\ttrain-error:0.35245\n",
      "[1287]\ttrain-error:0.35245\n",
      "[1288]\ttrain-error:0.35245\n",
      "[1289]\ttrain-error:0.35245\n",
      "[1290]\ttrain-error:0.35245\n",
      "[1291]\ttrain-error:0.35245\n",
      "[1292]\ttrain-error:0.35245\n",
      "[1293]\ttrain-error:0.35245\n",
      "[1294]\ttrain-error:0.35245\n",
      "[1295]\ttrain-error:0.35245\n",
      "[1296]\ttrain-error:0.35245\n",
      "[1297]\ttrain-error:0.35245\n",
      "[1298]\ttrain-error:0.35245\n",
      "[1299]\ttrain-error:0.35245\n",
      "[1300]\ttrain-error:0.35245\n",
      "[1301]\ttrain-error:0.35245\n",
      "[1302]\ttrain-error:0.35245\n",
      "[1303]\ttrain-error:0.35245\n",
      "[1304]\ttrain-error:0.35245\n",
      "[1305]\ttrain-error:0.35245\n",
      "[1306]\ttrain-error:0.35245\n",
      "[1307]\ttrain-error:0.35245\n",
      "[1308]\ttrain-error:0.35245\n",
      "[1309]\ttrain-error:0.35245\n",
      "[1310]\ttrain-error:0.35245\n",
      "[1311]\ttrain-error:0.35245\n",
      "[1312]\ttrain-error:0.35245\n",
      "[1313]\ttrain-error:0.35245\n",
      "[1314]\ttrain-error:0.35245\n",
      "[1315]\ttrain-error:0.35245\n",
      "[1316]\ttrain-error:0.35245\n",
      "[1317]\ttrain-error:0.35245\n",
      "[1318]\ttrain-error:0.35245\n",
      "[1319]\ttrain-error:0.35245\n",
      "[1320]\ttrain-error:0.35245\n",
      "[1321]\ttrain-error:0.35245\n",
      "[1322]\ttrain-error:0.35245\n",
      "[1323]\ttrain-error:0.35245\n",
      "[1324]\ttrain-error:0.35245\n",
      "[1325]\ttrain-error:0.35245\n",
      "[1326]\ttrain-error:0.35245\n",
      "[1327]\ttrain-error:0.35245\n",
      "[1328]\ttrain-error:0.35245\n",
      "[1329]\ttrain-error:0.35245\n",
      "[1330]\ttrain-error:0.35245\n",
      "[1331]\ttrain-error:0.35245\n",
      "[1332]\ttrain-error:0.35245\n",
      "[1333]\ttrain-error:0.35245\n",
      "[1334]\ttrain-error:0.35245\n",
      "[1335]\ttrain-error:0.35245\n",
      "[1336]\ttrain-error:0.35245\n",
      "[1337]\ttrain-error:0.35245\n",
      "[1338]\ttrain-error:0.35245\n",
      "[1339]\ttrain-error:0.35245\n",
      "[1340]\ttrain-error:0.35245\n",
      "[1341]\ttrain-error:0.35245\n",
      "[1342]\ttrain-error:0.35245\n",
      "[1343]\ttrain-error:0.35245\n",
      "[1344]\ttrain-error:0.35245\n",
      "[1345]\ttrain-error:0.35245\n",
      "[1346]\ttrain-error:0.35245\n",
      "[1347]\ttrain-error:0.35245\n",
      "[1348]\ttrain-error:0.35245\n",
      "[1349]\ttrain-error:0.35245\n",
      "[1350]\ttrain-error:0.35245\n",
      "[1351]\ttrain-error:0.35245\n",
      "[1352]\ttrain-error:0.35245\n",
      "[1353]\ttrain-error:0.35245\n",
      "[1354]\ttrain-error:0.35245\n",
      "[1355]\ttrain-error:0.35245\n",
      "[1356]\ttrain-error:0.35245\n",
      "[1357]\ttrain-error:0.35245\n",
      "[1358]\ttrain-error:0.35245\n",
      "[1359]\ttrain-error:0.35245\n",
      "[1360]\ttrain-error:0.35245\n",
      "[1361]\ttrain-error:0.35245\n",
      "[1362]\ttrain-error:0.35245\n",
      "[1363]\ttrain-error:0.35245\n",
      "[1364]\ttrain-error:0.35245\n",
      "[1365]\ttrain-error:0.35245\n",
      "[1366]\ttrain-error:0.35245\n",
      "[1367]\ttrain-error:0.35245\n",
      "[1368]\ttrain-error:0.35245\n",
      "[1369]\ttrain-error:0.35245\n",
      "[1370]\ttrain-error:0.35245\n",
      "[1371]\ttrain-error:0.35245\n",
      "[1372]\ttrain-error:0.35245\n",
      "[1373]\ttrain-error:0.35245\n",
      "[1374]\ttrain-error:0.35245\n",
      "[1375]\ttrain-error:0.35245\n",
      "[1376]\ttrain-error:0.35245\n",
      "[1377]\ttrain-error:0.35245\n",
      "[1378]\ttrain-error:0.35245\n",
      "[1379]\ttrain-error:0.35245\n",
      "[1380]\ttrain-error:0.35245\n",
      "[1381]\ttrain-error:0.35245\n",
      "[1382]\ttrain-error:0.35245\n",
      "[1383]\ttrain-error:0.35245\n",
      "[1384]\ttrain-error:0.35245\n",
      "[1385]\ttrain-error:0.35245\n",
      "[1386]\ttrain-error:0.35245\n",
      "[1387]\ttrain-error:0.35245\n",
      "[1388]\ttrain-error:0.35245\n",
      "[1389]\ttrain-error:0.35245\n",
      "[1390]\ttrain-error:0.35245\n",
      "[1391]\ttrain-error:0.35245\n",
      "[1392]\ttrain-error:0.35245\n",
      "[1393]\ttrain-error:0.35245\n",
      "[1394]\ttrain-error:0.35245\n",
      "[1395]\ttrain-error:0.35245\n",
      "[1396]\ttrain-error:0.35245\n",
      "[1397]\ttrain-error:0.35245\n",
      "[1398]\ttrain-error:0.35245\n",
      "[1399]\ttrain-error:0.35245\n",
      "[1400]\ttrain-error:0.35245\n",
      "[1401]\ttrain-error:0.35245\n",
      "[1402]\ttrain-error:0.35245\n",
      "[1403]\ttrain-error:0.35245\n",
      "[1404]\ttrain-error:0.35245\n",
      "[1405]\ttrain-error:0.35245\n",
      "[1406]\ttrain-error:0.35245\n",
      "[1407]\ttrain-error:0.35245\n",
      "[1408]\ttrain-error:0.35245\n",
      "[1409]\ttrain-error:0.35245\n",
      "[1410]\ttrain-error:0.35245\n",
      "[1411]\ttrain-error:0.35245\n",
      "[1412]\ttrain-error:0.35245\n",
      "[1413]\ttrain-error:0.35245\n",
      "[1414]\ttrain-error:0.35245\n",
      "[1415]\ttrain-error:0.35245\n",
      "[1416]\ttrain-error:0.35245\n",
      "[1417]\ttrain-error:0.35245\n",
      "[1418]\ttrain-error:0.35245\n",
      "[1419]\ttrain-error:0.35245\n",
      "[1420]\ttrain-error:0.35245\n",
      "[1421]\ttrain-error:0.35245\n",
      "[1422]\ttrain-error:0.35245\n",
      "[1423]\ttrain-error:0.35245\n",
      "[1424]\ttrain-error:0.35245\n",
      "[1425]\ttrain-error:0.35245\n",
      "[1426]\ttrain-error:0.35245\n",
      "[1427]\ttrain-error:0.35245\n",
      "[1428]\ttrain-error:0.35245\n",
      "[1429]\ttrain-error:0.35245\n",
      "[1430]\ttrain-error:0.35245\n",
      "[1431]\ttrain-error:0.35245\n",
      "[1432]\ttrain-error:0.35245\n",
      "[1433]\ttrain-error:0.35245\n",
      "[1434]\ttrain-error:0.35245\n",
      "[1435]\ttrain-error:0.35245\n",
      "[1436]\ttrain-error:0.35245\n",
      "[1437]\ttrain-error:0.35245\n",
      "[1438]\ttrain-error:0.35245\n",
      "[1439]\ttrain-error:0.35245\n",
      "[1440]\ttrain-error:0.35245\n",
      "[1441]\ttrain-error:0.35245\n",
      "[1442]\ttrain-error:0.35245\n",
      "[1443]\ttrain-error:0.35245\n",
      "[1444]\ttrain-error:0.35245\n",
      "[1445]\ttrain-error:0.35245\n",
      "[1446]\ttrain-error:0.35245\n",
      "[1447]\ttrain-error:0.35245\n",
      "[1448]\ttrain-error:0.35245\n",
      "[1449]\ttrain-error:0.35245\n",
      "[1450]\ttrain-error:0.35245\n",
      "[1451]\ttrain-error:0.35245\n",
      "[1452]\ttrain-error:0.35245\n",
      "[1453]\ttrain-error:0.35245\n",
      "[1454]\ttrain-error:0.35245\n",
      "[1455]\ttrain-error:0.35245\n",
      "[1456]\ttrain-error:0.35245\n",
      "[1457]\ttrain-error:0.35245\n",
      "[1458]\ttrain-error:0.35245\n",
      "[1459]\ttrain-error:0.35245\n",
      "[1460]\ttrain-error:0.35245\n",
      "[1461]\ttrain-error:0.35245\n",
      "[1462]\ttrain-error:0.35245\n",
      "[1463]\ttrain-error:0.35245\n",
      "[1464]\ttrain-error:0.35245\n",
      "[1465]\ttrain-error:0.35245\n",
      "[1466]\ttrain-error:0.35245\n",
      "[1467]\ttrain-error:0.35245\n",
      "[1468]\ttrain-error:0.35245\n",
      "[1469]\ttrain-error:0.35245\n",
      "[1470]\ttrain-error:0.35245\n",
      "[1471]\ttrain-error:0.35245\n",
      "[1472]\ttrain-error:0.35245\n",
      "[1473]\ttrain-error:0.35245\n",
      "[1474]\ttrain-error:0.35245\n",
      "[1475]\ttrain-error:0.35245\n",
      "[1476]\ttrain-error:0.35245\n",
      "[1477]\ttrain-error:0.35245\n",
      "[1478]\ttrain-error:0.35245\n",
      "[1479]\ttrain-error:0.35245\n",
      "[1480]\ttrain-error:0.35245\n",
      "[1481]\ttrain-error:0.35245\n",
      "[1482]\ttrain-error:0.35245\n",
      "[1483]\ttrain-error:0.35245\n",
      "[1484]\ttrain-error:0.35245\n",
      "[1485]\ttrain-error:0.35245\n",
      "[1486]\ttrain-error:0.35245\n",
      "[1487]\ttrain-error:0.35245\n",
      "[1488]\ttrain-error:0.35245\n",
      "[1489]\ttrain-error:0.35245\n",
      "[1490]\ttrain-error:0.35245\n",
      "[1491]\ttrain-error:0.35245\n",
      "[1492]\ttrain-error:0.35245\n",
      "[1493]\ttrain-error:0.35245\n",
      "[1494]\ttrain-error:0.35245\n",
      "[1495]\ttrain-error:0.35245\n",
      "[1496]\ttrain-error:0.35245\n",
      "[1497]\ttrain-error:0.35245\n",
      "[1498]\ttrain-error:0.35245\n",
      "[1499]\ttrain-error:0.35245\n",
      "[1500]\ttrain-error:0.35245\n",
      "[1501]\ttrain-error:0.35245\n",
      "[1502]\ttrain-error:0.35245\n",
      "[1503]\ttrain-error:0.35245\n",
      "[1504]\ttrain-error:0.35245\n",
      "[1505]\ttrain-error:0.35245\n",
      "[1506]\ttrain-error:0.35245\n",
      "[1507]\ttrain-error:0.35245\n",
      "[1508]\ttrain-error:0.35245\n",
      "[1509]\ttrain-error:0.35245\n",
      "[1510]\ttrain-error:0.35245\n",
      "[1511]\ttrain-error:0.35245\n",
      "[1512]\ttrain-error:0.35245\n",
      "[1513]\ttrain-error:0.35245\n",
      "[1514]\ttrain-error:0.35245\n",
      "[1515]\ttrain-error:0.35245\n",
      "[1516]\ttrain-error:0.35245\n",
      "[1517]\ttrain-error:0.35245\n",
      "[1518]\ttrain-error:0.35245\n",
      "[1519]\ttrain-error:0.35245\n",
      "[1520]\ttrain-error:0.35245\n",
      "[1521]\ttrain-error:0.35245\n",
      "[1522]\ttrain-error:0.35245\n",
      "[1523]\ttrain-error:0.35245\n",
      "[1524]\ttrain-error:0.35245\n",
      "[1525]\ttrain-error:0.35245\n",
      "[1526]\ttrain-error:0.35245\n",
      "[1527]\ttrain-error:0.35245\n",
      "[1528]\ttrain-error:0.35245\n",
      "[1529]\ttrain-error:0.35245\n",
      "[1530]\ttrain-error:0.35245\n",
      "[1531]\ttrain-error:0.35245\n",
      "[1532]\ttrain-error:0.35245\n",
      "[1533]\ttrain-error:0.35245\n",
      "[1534]\ttrain-error:0.35245\n",
      "[1535]\ttrain-error:0.35245\n",
      "[1536]\ttrain-error:0.35245\n",
      "[1537]\ttrain-error:0.35245\n",
      "[1538]\ttrain-error:0.35245\n",
      "[1539]\ttrain-error:0.35245\n",
      "[1540]\ttrain-error:0.35245\n",
      "[1541]\ttrain-error:0.35245\n",
      "[1542]\ttrain-error:0.35245\n",
      "[1543]\ttrain-error:0.35245\n",
      "[1544]\ttrain-error:0.35245\n",
      "[1545]\ttrain-error:0.35245\n",
      "[1546]\ttrain-error:0.35245\n",
      "[1547]\ttrain-error:0.35245\n",
      "[1548]\ttrain-error:0.35245\n",
      "[1549]\ttrain-error:0.35245\n",
      "[1550]\ttrain-error:0.35245\n",
      "[1551]\ttrain-error:0.35245\n",
      "[1552]\ttrain-error:0.35245\n",
      "[1553]\ttrain-error:0.35245\n",
      "[1554]\ttrain-error:0.35245\n",
      "[1555]\ttrain-error:0.35245\n",
      "[1556]\ttrain-error:0.35245\n",
      "[1557]\ttrain-error:0.35245\n",
      "[1558]\ttrain-error:0.35245\n",
      "[1559]\ttrain-error:0.35245\n",
      "[1560]\ttrain-error:0.35245\n",
      "[1561]\ttrain-error:0.35245\n",
      "[1562]\ttrain-error:0.35245\n",
      "[1563]\ttrain-error:0.35245\n",
      "[1564]\ttrain-error:0.35245\n",
      "[1565]\ttrain-error:0.35245\n",
      "[1566]\ttrain-error:0.35245\n",
      "[1567]\ttrain-error:0.35245\n",
      "[1568]\ttrain-error:0.35245\n",
      "[1569]\ttrain-error:0.35245\n",
      "[1570]\ttrain-error:0.35245\n",
      "[1571]\ttrain-error:0.35245\n",
      "[1572]\ttrain-error:0.35245\n",
      "[1573]\ttrain-error:0.35245\n",
      "[1574]\ttrain-error:0.35245\n",
      "[1575]\ttrain-error:0.35245\n",
      "[1576]\ttrain-error:0.35245\n",
      "[1577]\ttrain-error:0.35245\n",
      "[1578]\ttrain-error:0.35245\n",
      "[1579]\ttrain-error:0.35245\n",
      "[1580]\ttrain-error:0.35245\n",
      "[1581]\ttrain-error:0.35245\n",
      "[1582]\ttrain-error:0.35245\n",
      "[1583]\ttrain-error:0.35245\n",
      "[1584]\ttrain-error:0.35245\n",
      "[1585]\ttrain-error:0.35245\n",
      "[1586]\ttrain-error:0.35245\n",
      "[1587]\ttrain-error:0.35245\n",
      "[1588]\ttrain-error:0.35245\n",
      "[1589]\ttrain-error:0.35245\n",
      "[1590]\ttrain-error:0.35245\n",
      "[1591]\ttrain-error:0.35245\n",
      "[1592]\ttrain-error:0.35245\n",
      "[1593]\ttrain-error:0.35245\n",
      "[1594]\ttrain-error:0.35245\n",
      "[1595]\ttrain-error:0.35245\n",
      "[1596]\ttrain-error:0.35245\n",
      "[1597]\ttrain-error:0.35245\n",
      "[1598]\ttrain-error:0.35245\n",
      "[1599]\ttrain-error:0.35245\n",
      "[1600]\ttrain-error:0.35245\n",
      "[1601]\ttrain-error:0.35245\n",
      "[1602]\ttrain-error:0.35245\n",
      "[1603]\ttrain-error:0.35245\n",
      "[1604]\ttrain-error:0.35245\n",
      "[1605]\ttrain-error:0.35245\n",
      "[1606]\ttrain-error:0.35245\n",
      "[1607]\ttrain-error:0.35245\n",
      "[1608]\ttrain-error:0.35245\n",
      "[1609]\ttrain-error:0.35245\n",
      "[1610]\ttrain-error:0.35245\n",
      "[1611]\ttrain-error:0.35245\n",
      "[1612]\ttrain-error:0.35245\n",
      "[1613]\ttrain-error:0.35245\n",
      "[1614]\ttrain-error:0.35245\n",
      "[1615]\ttrain-error:0.35245\n",
      "[1616]\ttrain-error:0.35245\n",
      "[1617]\ttrain-error:0.35245\n",
      "[1618]\ttrain-error:0.35245\n",
      "[1619]\ttrain-error:0.35245\n",
      "[1620]\ttrain-error:0.35245\n",
      "[1621]\ttrain-error:0.35245\n",
      "[1622]\ttrain-error:0.35245\n",
      "[1623]\ttrain-error:0.35245\n",
      "[1624]\ttrain-error:0.35245\n",
      "[1625]\ttrain-error:0.35245\n",
      "[1626]\ttrain-error:0.35245\n",
      "[1627]\ttrain-error:0.35245\n",
      "[1628]\ttrain-error:0.35245\n",
      "[1629]\ttrain-error:0.35245\n",
      "[1630]\ttrain-error:0.35245\n",
      "[1631]\ttrain-error:0.35245\n",
      "[1632]\ttrain-error:0.35245\n",
      "[1633]\ttrain-error:0.35245\n",
      "[1634]\ttrain-error:0.35245\n",
      "[1635]\ttrain-error:0.35245\n",
      "[1636]\ttrain-error:0.35245\n",
      "[1637]\ttrain-error:0.35245\n",
      "[1638]\ttrain-error:0.35245\n",
      "[1639]\ttrain-error:0.35245\n",
      "[1640]\ttrain-error:0.35245\n",
      "[1641]\ttrain-error:0.35245\n",
      "[1642]\ttrain-error:0.35245\n",
      "[1643]\ttrain-error:0.35245\n",
      "[1644]\ttrain-error:0.35245\n",
      "[1645]\ttrain-error:0.35245\n",
      "[1646]\ttrain-error:0.35245\n",
      "[1647]\ttrain-error:0.35245\n",
      "[1648]\ttrain-error:0.35245\n",
      "[1649]\ttrain-error:0.35245\n",
      "[1650]\ttrain-error:0.35245\n",
      "[1651]\ttrain-error:0.35245\n",
      "[1652]\ttrain-error:0.35245\n",
      "[1653]\ttrain-error:0.35245\n",
      "[1654]\ttrain-error:0.35245\n",
      "[1655]\ttrain-error:0.35245\n",
      "[1656]\ttrain-error:0.35245\n",
      "[1657]\ttrain-error:0.35245\n",
      "[1658]\ttrain-error:0.35245\n",
      "[1659]\ttrain-error:0.35245\n",
      "[1660]\ttrain-error:0.35245\n",
      "[1661]\ttrain-error:0.35245\n",
      "[1662]\ttrain-error:0.35245\n",
      "[1663]\ttrain-error:0.35245\n",
      "[1664]\ttrain-error:0.35245\n",
      "[1665]\ttrain-error:0.35245\n",
      "[1666]\ttrain-error:0.35245\n",
      "[1667]\ttrain-error:0.35245\n",
      "[1668]\ttrain-error:0.35245\n",
      "[1669]\ttrain-error:0.35245\n",
      "[1670]\ttrain-error:0.35245\n",
      "[1671]\ttrain-error:0.35245\n",
      "[1672]\ttrain-error:0.35245\n",
      "[1673]\ttrain-error:0.35245\n",
      "[1674]\ttrain-error:0.35245\n",
      "[1675]\ttrain-error:0.35245\n",
      "[1676]\ttrain-error:0.35245\n",
      "[1677]\ttrain-error:0.35245\n",
      "[1678]\ttrain-error:0.35245\n",
      "[1679]\ttrain-error:0.35245\n",
      "[1680]\ttrain-error:0.35245\n",
      "[1681]\ttrain-error:0.35245\n",
      "[1682]\ttrain-error:0.35245\n",
      "[1683]\ttrain-error:0.35245\n",
      "[1684]\ttrain-error:0.35245\n",
      "[1685]\ttrain-error:0.35245\n",
      "[1686]\ttrain-error:0.35245\n",
      "[1687]\ttrain-error:0.35245\n",
      "[1688]\ttrain-error:0.35245\n",
      "[1689]\ttrain-error:0.35245\n",
      "[1690]\ttrain-error:0.35245\n",
      "[1691]\ttrain-error:0.35245\n",
      "[1692]\ttrain-error:0.35245\n",
      "[1693]\ttrain-error:0.35245\n",
      "[1694]\ttrain-error:0.35245\n",
      "[1695]\ttrain-error:0.35245\n",
      "[1696]\ttrain-error:0.35245\n",
      "[1697]\ttrain-error:0.35245\n",
      "[1698]\ttrain-error:0.35245\n",
      "[1699]\ttrain-error:0.35245\n",
      "[1700]\ttrain-error:0.35245\n",
      "[1701]\ttrain-error:0.35245\n",
      "[1702]\ttrain-error:0.35245\n",
      "[1703]\ttrain-error:0.35245\n",
      "[1704]\ttrain-error:0.35245\n",
      "[1705]\ttrain-error:0.35245\n",
      "[1706]\ttrain-error:0.35245\n",
      "[1707]\ttrain-error:0.35245\n",
      "[1708]\ttrain-error:0.35245\n",
      "[1709]\ttrain-error:0.35245\n",
      "[1710]\ttrain-error:0.35245\n",
      "[1711]\ttrain-error:0.35245\n",
      "[1712]\ttrain-error:0.35245\n",
      "[1713]\ttrain-error:0.35245\n",
      "[1714]\ttrain-error:0.35245\n",
      "[1715]\ttrain-error:0.35245\n",
      "[1716]\ttrain-error:0.35245\n",
      "[1717]\ttrain-error:0.35245\n",
      "[1718]\ttrain-error:0.35245\n",
      "[1719]\ttrain-error:0.35245\n",
      "[1720]\ttrain-error:0.35245\n",
      "[1721]\ttrain-error:0.35245\n",
      "[1722]\ttrain-error:0.35245\n",
      "[1723]\ttrain-error:0.35245\n",
      "[1724]\ttrain-error:0.35245\n",
      "[1725]\ttrain-error:0.35245\n",
      "[1726]\ttrain-error:0.35245\n",
      "[1727]\ttrain-error:0.35245\n",
      "[1728]\ttrain-error:0.35245\n",
      "[1729]\ttrain-error:0.35245\n",
      "[1730]\ttrain-error:0.35245\n",
      "[1731]\ttrain-error:0.35245\n",
      "[1732]\ttrain-error:0.35245\n",
      "[1733]\ttrain-error:0.35245\n",
      "[1734]\ttrain-error:0.35245\n",
      "[1735]\ttrain-error:0.35245\n",
      "[1736]\ttrain-error:0.35245\n",
      "[1737]\ttrain-error:0.35245\n",
      "[1738]\ttrain-error:0.35245\n",
      "[1739]\ttrain-error:0.35245\n",
      "[1740]\ttrain-error:0.35245\n",
      "[1741]\ttrain-error:0.35245\n",
      "[1742]\ttrain-error:0.35245\n",
      "[1743]\ttrain-error:0.35245\n",
      "[1744]\ttrain-error:0.35245\n",
      "[1745]\ttrain-error:0.35245\n",
      "[1746]\ttrain-error:0.35245\n",
      "[1747]\ttrain-error:0.35245\n",
      "[1748]\ttrain-error:0.35245\n",
      "[1749]\ttrain-error:0.35245\n",
      "[1750]\ttrain-error:0.35245\n",
      "[1751]\ttrain-error:0.35245\n",
      "[1752]\ttrain-error:0.35245\n",
      "[1753]\ttrain-error:0.35245\n",
      "[1754]\ttrain-error:0.35245\n",
      "[1755]\ttrain-error:0.35245\n",
      "[1756]\ttrain-error:0.35245\n",
      "[1757]\ttrain-error:0.35245\n",
      "[1758]\ttrain-error:0.35245\n",
      "[1759]\ttrain-error:0.35245\n",
      "[1760]\ttrain-error:0.35245\n",
      "[1761]\ttrain-error:0.35245\n",
      "[1762]\ttrain-error:0.35245\n",
      "[1763]\ttrain-error:0.35245\n",
      "[1764]\ttrain-error:0.35245\n",
      "[1765]\ttrain-error:0.35245\n",
      "[1766]\ttrain-error:0.35245\n",
      "[1767]\ttrain-error:0.35245\n",
      "[1768]\ttrain-error:0.35245\n",
      "[1769]\ttrain-error:0.35245\n",
      "[1770]\ttrain-error:0.35245\n",
      "[1771]\ttrain-error:0.35245\n",
      "[1772]\ttrain-error:0.35245\n",
      "[1773]\ttrain-error:0.35245\n",
      "[1774]\ttrain-error:0.35245\n",
      "[1775]\ttrain-error:0.35245\n",
      "[1776]\ttrain-error:0.35245\n",
      "[1777]\ttrain-error:0.35245\n",
      "[1778]\ttrain-error:0.35245\n",
      "[1779]\ttrain-error:0.35245\n",
      "[1780]\ttrain-error:0.35245\n",
      "[1781]\ttrain-error:0.35245\n",
      "[1782]\ttrain-error:0.35245\n",
      "[1783]\ttrain-error:0.35245\n",
      "[1784]\ttrain-error:0.35245\n",
      "[1785]\ttrain-error:0.35245\n",
      "[1786]\ttrain-error:0.35245\n",
      "[1787]\ttrain-error:0.35245\n",
      "[1788]\ttrain-error:0.35245\n",
      "[1789]\ttrain-error:0.35245\n",
      "[1790]\ttrain-error:0.35245\n",
      "[1791]\ttrain-error:0.35245\n",
      "[1792]\ttrain-error:0.35245\n",
      "[1793]\ttrain-error:0.35245\n",
      "[1794]\ttrain-error:0.35245\n",
      "[1795]\ttrain-error:0.35245\n",
      "[1796]\ttrain-error:0.35245\n",
      "[1797]\ttrain-error:0.35245\n",
      "[1798]\ttrain-error:0.35245\n",
      "[1799]\ttrain-error:0.35245\n",
      "[1800]\ttrain-error:0.35245\n",
      "[1801]\ttrain-error:0.35245\n",
      "[1802]\ttrain-error:0.35245\n",
      "[1803]\ttrain-error:0.35245\n",
      "[1804]\ttrain-error:0.35245\n",
      "[1805]\ttrain-error:0.35245\n",
      "[1806]\ttrain-error:0.35245\n",
      "[1807]\ttrain-error:0.35245\n",
      "[1808]\ttrain-error:0.35245\n",
      "[1809]\ttrain-error:0.35245\n",
      "[1810]\ttrain-error:0.35245\n",
      "[1811]\ttrain-error:0.35245\n",
      "[1812]\ttrain-error:0.35245\n",
      "[1813]\ttrain-error:0.35245\n",
      "[1814]\ttrain-error:0.35245\n",
      "[1815]\ttrain-error:0.35245\n",
      "[1816]\ttrain-error:0.35245\n",
      "[1817]\ttrain-error:0.35245\n",
      "[1818]\ttrain-error:0.35245\n",
      "[1819]\ttrain-error:0.35245\n",
      "[1820]\ttrain-error:0.35245\n",
      "[1821]\ttrain-error:0.35245\n",
      "[1822]\ttrain-error:0.35245\n",
      "[1823]\ttrain-error:0.35245\n",
      "[1824]\ttrain-error:0.35245\n",
      "[1825]\ttrain-error:0.35245\n",
      "[1826]\ttrain-error:0.35245\n",
      "[1827]\ttrain-error:0.35245\n",
      "[1828]\ttrain-error:0.35245\n",
      "[1829]\ttrain-error:0.35245\n",
      "[1830]\ttrain-error:0.35245\n",
      "[1831]\ttrain-error:0.35245\n",
      "[1832]\ttrain-error:0.35245\n",
      "[1833]\ttrain-error:0.35245\n",
      "[1834]\ttrain-error:0.35245\n",
      "[1835]\ttrain-error:0.35245\n",
      "[1836]\ttrain-error:0.35245\n",
      "[1837]\ttrain-error:0.35245\n",
      "[1838]\ttrain-error:0.35245\n",
      "[1839]\ttrain-error:0.35245\n",
      "[1840]\ttrain-error:0.35245\n",
      "[1841]\ttrain-error:0.35245\n",
      "[1842]\ttrain-error:0.35245\n",
      "[1843]\ttrain-error:0.35245\n",
      "[1844]\ttrain-error:0.35245\n",
      "[1845]\ttrain-error:0.35245\n",
      "[1846]\ttrain-error:0.35245\n",
      "[1847]\ttrain-error:0.35245\n",
      "[1848]\ttrain-error:0.35245\n",
      "[1849]\ttrain-error:0.35245\n",
      "[1850]\ttrain-error:0.35245\n",
      "[1851]\ttrain-error:0.35245\n",
      "[1852]\ttrain-error:0.35245\n",
      "[1853]\ttrain-error:0.35245\n",
      "[1854]\ttrain-error:0.35245\n",
      "[1855]\ttrain-error:0.35245\n",
      "[1856]\ttrain-error:0.35245\n",
      "[1857]\ttrain-error:0.35245\n",
      "[1858]\ttrain-error:0.35245\n",
      "[1859]\ttrain-error:0.35245\n",
      "[1860]\ttrain-error:0.35245\n",
      "[1861]\ttrain-error:0.35245\n",
      "[1862]\ttrain-error:0.35245\n",
      "[1863]\ttrain-error:0.35245\n",
      "[1864]\ttrain-error:0.35245\n",
      "[1865]\ttrain-error:0.35245\n",
      "[1866]\ttrain-error:0.35245\n",
      "[1867]\ttrain-error:0.35245\n",
      "[1868]\ttrain-error:0.35245\n",
      "[1869]\ttrain-error:0.35245\n",
      "[1870]\ttrain-error:0.35245\n",
      "[1871]\ttrain-error:0.35245\n",
      "[1872]\ttrain-error:0.35245\n",
      "[1873]\ttrain-error:0.35245\n",
      "[1874]\ttrain-error:0.35245\n",
      "[1875]\ttrain-error:0.35245\n",
      "[1876]\ttrain-error:0.35245\n",
      "[1877]\ttrain-error:0.35245\n",
      "[1878]\ttrain-error:0.35245\n",
      "[1879]\ttrain-error:0.35245\n",
      "[1880]\ttrain-error:0.35245\n",
      "[1881]\ttrain-error:0.35245\n",
      "[1882]\ttrain-error:0.35245\n",
      "[1883]\ttrain-error:0.35245\n",
      "[1884]\ttrain-error:0.35245\n",
      "[1885]\ttrain-error:0.35245\n",
      "[1886]\ttrain-error:0.35245\n",
      "[1887]\ttrain-error:0.35245\n",
      "[1888]\ttrain-error:0.35245\n",
      "[1889]\ttrain-error:0.35245\n",
      "[1890]\ttrain-error:0.35245\n",
      "[1891]\ttrain-error:0.35245\n",
      "[1892]\ttrain-error:0.35245\n",
      "[1893]\ttrain-error:0.35245\n",
      "[1894]\ttrain-error:0.35245\n",
      "[1895]\ttrain-error:0.35245\n",
      "[1896]\ttrain-error:0.35245\n",
      "[1897]\ttrain-error:0.35245\n",
      "[1898]\ttrain-error:0.35245\n",
      "[1899]\ttrain-error:0.35245\n",
      "[1900]\ttrain-error:0.35245\n",
      "[1901]\ttrain-error:0.35245\n",
      "[1902]\ttrain-error:0.35245\n",
      "[1903]\ttrain-error:0.35245\n",
      "[1904]\ttrain-error:0.35245\n",
      "[1905]\ttrain-error:0.35245\n",
      "[1906]\ttrain-error:0.35245\n",
      "[1907]\ttrain-error:0.35245\n",
      "[1908]\ttrain-error:0.35245\n",
      "[1909]\ttrain-error:0.35245\n",
      "[1910]\ttrain-error:0.35245\n",
      "[1911]\ttrain-error:0.35245\n",
      "[1912]\ttrain-error:0.35245\n",
      "[1913]\ttrain-error:0.35245\n",
      "[1914]\ttrain-error:0.35245\n",
      "[1915]\ttrain-error:0.35245\n",
      "[1916]\ttrain-error:0.35245\n",
      "[1917]\ttrain-error:0.35245\n",
      "[1918]\ttrain-error:0.35245\n",
      "[1919]\ttrain-error:0.35245\n",
      "[1920]\ttrain-error:0.35245\n",
      "[1921]\ttrain-error:0.35245\n",
      "[1922]\ttrain-error:0.35245\n",
      "[1923]\ttrain-error:0.35245\n",
      "[1924]\ttrain-error:0.35245\n",
      "[1925]\ttrain-error:0.35245\n",
      "[1926]\ttrain-error:0.35245\n",
      "[1927]\ttrain-error:0.35245\n",
      "[1928]\ttrain-error:0.35245\n",
      "[1929]\ttrain-error:0.35245\n",
      "[1930]\ttrain-error:0.35245\n",
      "[1931]\ttrain-error:0.35245\n",
      "[1932]\ttrain-error:0.35245\n",
      "[1933]\ttrain-error:0.35245\n",
      "[1934]\ttrain-error:0.35245\n",
      "[1935]\ttrain-error:0.35245\n",
      "[1936]\ttrain-error:0.35245\n",
      "[1937]\ttrain-error:0.35245\n",
      "[1938]\ttrain-error:0.35245\n",
      "[1939]\ttrain-error:0.35245\n",
      "[1940]\ttrain-error:0.35245\n",
      "[1941]\ttrain-error:0.35245\n",
      "[1942]\ttrain-error:0.35245\n",
      "[1943]\ttrain-error:0.35245\n",
      "[1944]\ttrain-error:0.35245\n",
      "[1945]\ttrain-error:0.35245\n",
      "[1946]\ttrain-error:0.35245\n",
      "[1947]\ttrain-error:0.35245\n",
      "[1948]\ttrain-error:0.35245\n",
      "[1949]\ttrain-error:0.35245\n",
      "[1950]\ttrain-error:0.35245\n",
      "[1951]\ttrain-error:0.35245\n",
      "[1952]\ttrain-error:0.35245\n",
      "[1953]\ttrain-error:0.35245\n",
      "[1954]\ttrain-error:0.35245\n",
      "[1955]\ttrain-error:0.35245\n",
      "[1956]\ttrain-error:0.35245\n",
      "[1957]\ttrain-error:0.35245\n",
      "[1958]\ttrain-error:0.35245\n",
      "[1959]\ttrain-error:0.35245\n",
      "[1960]\ttrain-error:0.35245\n",
      "[1961]\ttrain-error:0.35245\n",
      "[1962]\ttrain-error:0.35245\n",
      "[1963]\ttrain-error:0.35245\n",
      "[1964]\ttrain-error:0.35245\n",
      "[1965]\ttrain-error:0.35245\n",
      "[1966]\ttrain-error:0.35245\n",
      "[1967]\ttrain-error:0.35245\n",
      "[1968]\ttrain-error:0.35245\n",
      "[1969]\ttrain-error:0.35245\n",
      "[1970]\ttrain-error:0.35245\n",
      "[1971]\ttrain-error:0.35245\n",
      "[1972]\ttrain-error:0.35245\n",
      "[1973]\ttrain-error:0.35245\n",
      "[1974]\ttrain-error:0.35245\n",
      "[1975]\ttrain-error:0.35245\n",
      "[1976]\ttrain-error:0.35245\n",
      "[1977]\ttrain-error:0.35245\n",
      "[1978]\ttrain-error:0.35245\n",
      "[1979]\ttrain-error:0.35245\n",
      "[1980]\ttrain-error:0.35245\n",
      "[1981]\ttrain-error:0.35245\n",
      "[1982]\ttrain-error:0.35245\n",
      "[1983]\ttrain-error:0.35245\n",
      "[1984]\ttrain-error:0.35245\n",
      "[1985]\ttrain-error:0.35245\n",
      "[1986]\ttrain-error:0.35245\n",
      "[1987]\ttrain-error:0.35245\n",
      "[1988]\ttrain-error:0.35245\n",
      "[1989]\ttrain-error:0.35245\n",
      "[1990]\ttrain-error:0.35245\n",
      "[1991]\ttrain-error:0.35245\n",
      "[1992]\ttrain-error:0.35245\n",
      "[1993]\ttrain-error:0.35245\n",
      "[1994]\ttrain-error:0.35245\n",
      "[1995]\ttrain-error:0.35245\n",
      "[1996]\ttrain-error:0.35245\n",
      "[1997]\ttrain-error:0.35245\n",
      "[1998]\ttrain-error:0.35245\n",
      "[1999]\ttrain-error:0.35245\n",
      "[2000]\ttrain-error:0.35245\n",
      "[2001]\ttrain-error:0.35245\n",
      "[2002]\ttrain-error:0.35245\n",
      "[2003]\ttrain-error:0.35245\n",
      "[2004]\ttrain-error:0.35245\n",
      "[2005]\ttrain-error:0.35245\n",
      "[2006]\ttrain-error:0.35245\n",
      "[2007]\ttrain-error:0.35245\n",
      "[2008]\ttrain-error:0.35245\n",
      "[2009]\ttrain-error:0.35245\n",
      "[2010]\ttrain-error:0.35245\n",
      "[2011]\ttrain-error:0.35245\n",
      "[2012]\ttrain-error:0.35245\n",
      "[2013]\ttrain-error:0.35245\n",
      "[2014]\ttrain-error:0.35245\n",
      "[2015]\ttrain-error:0.35245\n",
      "[2016]\ttrain-error:0.35245\n",
      "[2017]\ttrain-error:0.35245\n",
      "[2018]\ttrain-error:0.35245\n",
      "[2019]\ttrain-error:0.35245\n",
      "[2020]\ttrain-error:0.35245\n",
      "[2021]\ttrain-error:0.35245\n",
      "[2022]\ttrain-error:0.35245\n",
      "[2023]\ttrain-error:0.35245\n",
      "[2024]\ttrain-error:0.35245\n",
      "[2025]\ttrain-error:0.35245\n",
      "[2026]\ttrain-error:0.35245\n",
      "[2027]\ttrain-error:0.35245\n",
      "[2028]\ttrain-error:0.35245\n",
      "[2029]\ttrain-error:0.35245\n",
      "[2030]\ttrain-error:0.35245\n",
      "[2031]\ttrain-error:0.35245\n",
      "[2032]\ttrain-error:0.35245\n",
      "[2033]\ttrain-error:0.35245\n",
      "[2034]\ttrain-error:0.35245\n",
      "[2035]\ttrain-error:0.35245\n",
      "[2036]\ttrain-error:0.35245\n",
      "[2037]\ttrain-error:0.35245\n",
      "[2038]\ttrain-error:0.35245\n",
      "[2039]\ttrain-error:0.35245\n",
      "[2040]\ttrain-error:0.35245\n",
      "[2041]\ttrain-error:0.35245\n",
      "[2042]\ttrain-error:0.35245\n",
      "[2043]\ttrain-error:0.35245\n",
      "[2044]\ttrain-error:0.35245\n",
      "[2045]\ttrain-error:0.35245\n",
      "[2046]\ttrain-error:0.35245\n",
      "[2047]\ttrain-error:0.35245\n",
      "[2048]\ttrain-error:0.35245\n",
      "[2049]\ttrain-error:0.35245\n",
      "[2050]\ttrain-error:0.35245\n",
      "[2051]\ttrain-error:0.35245\n",
      "[2052]\ttrain-error:0.35245\n",
      "[2053]\ttrain-error:0.35245\n",
      "[2054]\ttrain-error:0.35245\n",
      "[2055]\ttrain-error:0.35245\n",
      "[2056]\ttrain-error:0.35245\n",
      "[2057]\ttrain-error:0.35245\n",
      "[2058]\ttrain-error:0.35245\n",
      "[2059]\ttrain-error:0.35245\n",
      "[2060]\ttrain-error:0.35245\n",
      "[2061]\ttrain-error:0.35245\n",
      "[2062]\ttrain-error:0.35245\n",
      "[2063]\ttrain-error:0.35245\n",
      "[2064]\ttrain-error:0.35245\n",
      "[2065]\ttrain-error:0.35245\n",
      "[2066]\ttrain-error:0.35245\n",
      "[2067]\ttrain-error:0.35245\n",
      "[2068]\ttrain-error:0.35245\n",
      "[2069]\ttrain-error:0.35245\n",
      "[2070]\ttrain-error:0.35245\n",
      "[2071]\ttrain-error:0.35245\n",
      "[2072]\ttrain-error:0.35245\n",
      "[2073]\ttrain-error:0.35245\n",
      "[2074]\ttrain-error:0.35245\n",
      "[2075]\ttrain-error:0.35245\n",
      "[2076]\ttrain-error:0.35245\n",
      "[2077]\ttrain-error:0.35245\n",
      "[2078]\ttrain-error:0.35245\n",
      "[2079]\ttrain-error:0.35245\n",
      "[2080]\ttrain-error:0.35245\n",
      "[2081]\ttrain-error:0.35245\n",
      "[2082]\ttrain-error:0.35245\n",
      "[2083]\ttrain-error:0.35245\n",
      "[2084]\ttrain-error:0.35245\n",
      "[2085]\ttrain-error:0.35245\n",
      "[2086]\ttrain-error:0.35245\n",
      "[2087]\ttrain-error:0.35245\n",
      "[2088]\ttrain-error:0.35245\n",
      "[2089]\ttrain-error:0.35245\n",
      "[2090]\ttrain-error:0.35245\n",
      "[2091]\ttrain-error:0.35245\n",
      "[2092]\ttrain-error:0.35245\n",
      "[2093]\ttrain-error:0.35245\n",
      "[2094]\ttrain-error:0.35245\n",
      "[2095]\ttrain-error:0.35245\n",
      "[2096]\ttrain-error:0.35245\n",
      "[2097]\ttrain-error:0.35245\n",
      "[2098]\ttrain-error:0.35245\n",
      "[2099]\ttrain-error:0.35245\n",
      "[2100]\ttrain-error:0.35245\n",
      "[2101]\ttrain-error:0.35245\n",
      "[2102]\ttrain-error:0.35245\n",
      "[2103]\ttrain-error:0.35245\n",
      "[2104]\ttrain-error:0.35245\n",
      "[2105]\ttrain-error:0.35245\n",
      "[2106]\ttrain-error:0.35245\n",
      "[2107]\ttrain-error:0.35245\n",
      "[2108]\ttrain-error:0.35245\n",
      "[2109]\ttrain-error:0.35245\n",
      "[2110]\ttrain-error:0.35245\n",
      "[2111]\ttrain-error:0.35245\n",
      "[2112]\ttrain-error:0.35245\n",
      "[2113]\ttrain-error:0.35245\n",
      "[2114]\ttrain-error:0.35245\n",
      "[2115]\ttrain-error:0.35245\n",
      "[2116]\ttrain-error:0.35245\n",
      "[2117]\ttrain-error:0.35245\n",
      "[2118]\ttrain-error:0.35245\n",
      "[2119]\ttrain-error:0.35245\n",
      "[2120]\ttrain-error:0.35245\n",
      "[2121]\ttrain-error:0.35245\n",
      "[2122]\ttrain-error:0.35245\n",
      "[2123]\ttrain-error:0.35245\n",
      "[2124]\ttrain-error:0.35245\n",
      "[2125]\ttrain-error:0.35245\n",
      "[2126]\ttrain-error:0.35245\n",
      "[2127]\ttrain-error:0.35245\n",
      "[2128]\ttrain-error:0.35245\n",
      "[2129]\ttrain-error:0.35245\n",
      "[2130]\ttrain-error:0.35245\n",
      "[2131]\ttrain-error:0.35245\n",
      "[2132]\ttrain-error:0.35245\n",
      "[2133]\ttrain-error:0.35245\n",
      "[2134]\ttrain-error:0.35245\n",
      "[2135]\ttrain-error:0.35245\n",
      "[2136]\ttrain-error:0.35245\n",
      "[2137]\ttrain-error:0.35245\n",
      "[2138]\ttrain-error:0.35245\n",
      "[2139]\ttrain-error:0.35245\n",
      "[2140]\ttrain-error:0.35245\n",
      "[2141]\ttrain-error:0.35245\n",
      "[2142]\ttrain-error:0.35245\n",
      "[2143]\ttrain-error:0.35245\n",
      "[2144]\ttrain-error:0.35245\n",
      "[2145]\ttrain-error:0.35245\n",
      "[2146]\ttrain-error:0.35245\n",
      "[2147]\ttrain-error:0.35245\n",
      "[2148]\ttrain-error:0.35245\n",
      "[2149]\ttrain-error:0.35245\n",
      "[2150]\ttrain-error:0.35245\n",
      "[2151]\ttrain-error:0.35245\n",
      "[2152]\ttrain-error:0.35245\n",
      "[2153]\ttrain-error:0.35245\n",
      "[2154]\ttrain-error:0.35245\n",
      "[2155]\ttrain-error:0.35245\n",
      "[2156]\ttrain-error:0.35245\n",
      "[2157]\ttrain-error:0.35245\n",
      "[2158]\ttrain-error:0.35245\n",
      "[2159]\ttrain-error:0.35245\n",
      "[2160]\ttrain-error:0.35245\n",
      "[2161]\ttrain-error:0.35245\n",
      "[2162]\ttrain-error:0.35245\n",
      "[2163]\ttrain-error:0.35245\n",
      "[2164]\ttrain-error:0.35245\n",
      "[2165]\ttrain-error:0.35245\n",
      "[2166]\ttrain-error:0.35245\n",
      "[2167]\ttrain-error:0.35245\n",
      "[2168]\ttrain-error:0.35245\n",
      "[2169]\ttrain-error:0.35245\n",
      "[2170]\ttrain-error:0.35245\n",
      "[2171]\ttrain-error:0.35245\n",
      "[2172]\ttrain-error:0.35245\n",
      "[2173]\ttrain-error:0.35245\n",
      "[2174]\ttrain-error:0.35245\n",
      "[2175]\ttrain-error:0.35245\n",
      "[2176]\ttrain-error:0.35245\n",
      "[2177]\ttrain-error:0.35245\n",
      "[2178]\ttrain-error:0.35245\n",
      "[2179]\ttrain-error:0.35245\n",
      "[2180]\ttrain-error:0.35245\n",
      "[2181]\ttrain-error:0.35245\n",
      "[2182]\ttrain-error:0.35245\n",
      "[2183]\ttrain-error:0.35245\n",
      "[2184]\ttrain-error:0.35245\n",
      "[2185]\ttrain-error:0.35245\n",
      "[2186]\ttrain-error:0.35245\n",
      "[2187]\ttrain-error:0.35245\n",
      "[2188]\ttrain-error:0.35245\n",
      "[2189]\ttrain-error:0.35245\n",
      "[2190]\ttrain-error:0.35245\n",
      "[2191]\ttrain-error:0.35245\n",
      "[2192]\ttrain-error:0.35245\n",
      "[2193]\ttrain-error:0.35245\n",
      "[2194]\ttrain-error:0.35245\n",
      "[2195]\ttrain-error:0.35245\n",
      "[2196]\ttrain-error:0.35245\n",
      "[2197]\ttrain-error:0.35245\n",
      "[2198]\ttrain-error:0.35245\n",
      "[2199]\ttrain-error:0.35245\n",
      "[2200]\ttrain-error:0.35245\n",
      "[2201]\ttrain-error:0.35245\n",
      "[2202]\ttrain-error:0.35245\n",
      "[2203]\ttrain-error:0.35245\n",
      "[2204]\ttrain-error:0.35245\n",
      "[2205]\ttrain-error:0.35245\n",
      "[2206]\ttrain-error:0.35245\n",
      "[2207]\ttrain-error:0.35245\n",
      "[2208]\ttrain-error:0.35245\n",
      "[2209]\ttrain-error:0.35245\n",
      "[2210]\ttrain-error:0.35245\n",
      "[2211]\ttrain-error:0.35245\n",
      "[2212]\ttrain-error:0.35245\n",
      "[2213]\ttrain-error:0.35245\n",
      "[2214]\ttrain-error:0.35245\n",
      "[2215]\ttrain-error:0.35245\n",
      "[2216]\ttrain-error:0.35245\n",
      "[2217]\ttrain-error:0.35245\n",
      "[2218]\ttrain-error:0.35245\n",
      "[2219]\ttrain-error:0.35245\n",
      "[2220]\ttrain-error:0.35245\n",
      "[2221]\ttrain-error:0.35245\n",
      "[2222]\ttrain-error:0.35245\n",
      "[2223]\ttrain-error:0.35245\n",
      "[2224]\ttrain-error:0.35245\n",
      "[2225]\ttrain-error:0.35245\n",
      "[2226]\ttrain-error:0.35245\n",
      "[2227]\ttrain-error:0.35245\n",
      "[2228]\ttrain-error:0.35245\n",
      "[2229]\ttrain-error:0.35245\n",
      "[2230]\ttrain-error:0.35245\n",
      "[2231]\ttrain-error:0.35245\n",
      "[2232]\ttrain-error:0.35245\n",
      "[2233]\ttrain-error:0.35245\n",
      "[2234]\ttrain-error:0.35245\n",
      "[2235]\ttrain-error:0.35245\n",
      "[2236]\ttrain-error:0.35245\n",
      "[2237]\ttrain-error:0.35245\n",
      "[2238]\ttrain-error:0.35245\n",
      "[2239]\ttrain-error:0.35245\n",
      "[2240]\ttrain-error:0.35245\n",
      "[2241]\ttrain-error:0.35245\n",
      "[2242]\ttrain-error:0.35245\n",
      "[2243]\ttrain-error:0.35245\n",
      "[2244]\ttrain-error:0.35245\n",
      "[2245]\ttrain-error:0.35245\n",
      "[2246]\ttrain-error:0.35245\n",
      "[2247]\ttrain-error:0.35245\n",
      "[2248]\ttrain-error:0.35245\n",
      "[2249]\ttrain-error:0.35245\n",
      "[2250]\ttrain-error:0.35245\n",
      "[2251]\ttrain-error:0.35245\n",
      "[2252]\ttrain-error:0.35245\n",
      "[2253]\ttrain-error:0.35245\n",
      "[2254]\ttrain-error:0.35245\n",
      "[2255]\ttrain-error:0.35245\n",
      "[2256]\ttrain-error:0.35245\n",
      "[2257]\ttrain-error:0.35245\n",
      "[2258]\ttrain-error:0.35245\n",
      "[2259]\ttrain-error:0.35245\n",
      "[2260]\ttrain-error:0.35245\n",
      "[2261]\ttrain-error:0.35245\n",
      "[2262]\ttrain-error:0.35245\n",
      "[2263]\ttrain-error:0.35245\n",
      "[2264]\ttrain-error:0.35245\n",
      "[2265]\ttrain-error:0.35245\n",
      "[2266]\ttrain-error:0.35245\n",
      "[2267]\ttrain-error:0.35245\n",
      "[2268]\ttrain-error:0.35245\n",
      "[2269]\ttrain-error:0.35245\n",
      "[2270]\ttrain-error:0.35245\n",
      "[2271]\ttrain-error:0.35245\n",
      "[2272]\ttrain-error:0.35245\n",
      "[2273]\ttrain-error:0.35245\n",
      "[2274]\ttrain-error:0.35245\n",
      "[2275]\ttrain-error:0.35245\n",
      "[2276]\ttrain-error:0.35245\n",
      "[2277]\ttrain-error:0.35245\n",
      "[2278]\ttrain-error:0.35245\n",
      "[2279]\ttrain-error:0.35245\n",
      "[2280]\ttrain-error:0.35245\n",
      "[2281]\ttrain-error:0.35245\n",
      "[2282]\ttrain-error:0.35245\n",
      "[2283]\ttrain-error:0.35245\n",
      "[2284]\ttrain-error:0.35245\n",
      "[2285]\ttrain-error:0.35245\n",
      "[2286]\ttrain-error:0.35245\n",
      "[2287]\ttrain-error:0.35245\n",
      "[2288]\ttrain-error:0.35245\n",
      "[2289]\ttrain-error:0.35245\n",
      "[2290]\ttrain-error:0.35245\n",
      "[2291]\ttrain-error:0.35245\n",
      "[2292]\ttrain-error:0.35245\n",
      "[2293]\ttrain-error:0.35245\n",
      "[2294]\ttrain-error:0.35245\n",
      "[2295]\ttrain-error:0.35245\n",
      "[2296]\ttrain-error:0.35245\n",
      "[2297]\ttrain-error:0.35245\n",
      "[2298]\ttrain-error:0.35245\n",
      "[2299]\ttrain-error:0.35245\n",
      "[2300]\ttrain-error:0.35245\n",
      "[2301]\ttrain-error:0.35245\n",
      "[2302]\ttrain-error:0.35245\n",
      "[2303]\ttrain-error:0.35245\n",
      "[2304]\ttrain-error:0.35245\n",
      "[2305]\ttrain-error:0.35245\n",
      "[2306]\ttrain-error:0.35245\n",
      "[2307]\ttrain-error:0.35245\n",
      "[2308]\ttrain-error:0.35245\n",
      "[2309]\ttrain-error:0.35245\n",
      "[2310]\ttrain-error:0.35245\n",
      "[2311]\ttrain-error:0.35245\n",
      "[2312]\ttrain-error:0.35245\n",
      "[2313]\ttrain-error:0.35245\n",
      "[2314]\ttrain-error:0.35245\n",
      "[2315]\ttrain-error:0.35245\n",
      "[2316]\ttrain-error:0.35245\n",
      "[2317]\ttrain-error:0.35245\n",
      "[2318]\ttrain-error:0.35245\n",
      "[2319]\ttrain-error:0.35245\n",
      "[2320]\ttrain-error:0.35245\n",
      "[2321]\ttrain-error:0.35245\n",
      "[2322]\ttrain-error:0.35245\n",
      "[2323]\ttrain-error:0.35245\n",
      "[2324]\ttrain-error:0.35245\n",
      "[2325]\ttrain-error:0.35245\n",
      "[2326]\ttrain-error:0.35245\n",
      "[2327]\ttrain-error:0.35245\n",
      "[2328]\ttrain-error:0.35245\n",
      "[2329]\ttrain-error:0.35245\n",
      "[2330]\ttrain-error:0.35245\n",
      "[2331]\ttrain-error:0.35245\n",
      "[2332]\ttrain-error:0.35245\n",
      "[2333]\ttrain-error:0.35245\n",
      "[2334]\ttrain-error:0.35245\n",
      "[2335]\ttrain-error:0.35245\n",
      "[2336]\ttrain-error:0.35245\n",
      "[2337]\ttrain-error:0.35245\n",
      "[2338]\ttrain-error:0.35245\n",
      "[2339]\ttrain-error:0.35245\n",
      "[2340]\ttrain-error:0.35245\n",
      "[2341]\ttrain-error:0.35245\n",
      "[2342]\ttrain-error:0.35245\n",
      "[2343]\ttrain-error:0.35245\n",
      "[2344]\ttrain-error:0.35245\n",
      "[2345]\ttrain-error:0.35245\n",
      "[2346]\ttrain-error:0.35245\n",
      "[2347]\ttrain-error:0.35245\n",
      "[2348]\ttrain-error:0.35245\n",
      "[2349]\ttrain-error:0.35245\n",
      "[2350]\ttrain-error:0.35245\n",
      "[2351]\ttrain-error:0.35245\n",
      "[2352]\ttrain-error:0.35245\n",
      "[2353]\ttrain-error:0.35245\n",
      "[2354]\ttrain-error:0.35245\n",
      "[2355]\ttrain-error:0.35245\n",
      "[2356]\ttrain-error:0.35245\n",
      "[2357]\ttrain-error:0.35245\n",
      "[2358]\ttrain-error:0.35245\n",
      "[2359]\ttrain-error:0.35245\n",
      "[2360]\ttrain-error:0.35245\n",
      "[2361]\ttrain-error:0.35245\n",
      "[2362]\ttrain-error:0.35245\n",
      "[2363]\ttrain-error:0.35245\n",
      "[2364]\ttrain-error:0.35245\n",
      "[2365]\ttrain-error:0.35245\n",
      "[2366]\ttrain-error:0.35245\n",
      "[2367]\ttrain-error:0.35245\n",
      "[2368]\ttrain-error:0.35245\n",
      "[2369]\ttrain-error:0.35245\n",
      "[2370]\ttrain-error:0.35245\n",
      "[2371]\ttrain-error:0.35245\n",
      "[2372]\ttrain-error:0.35245\n",
      "[2373]\ttrain-error:0.35245\n",
      "[2374]\ttrain-error:0.35245\n",
      "[2375]\ttrain-error:0.35245\n",
      "[2376]\ttrain-error:0.35245\n",
      "[2377]\ttrain-error:0.35245\n",
      "[2378]\ttrain-error:0.35245\n",
      "[2379]\ttrain-error:0.35245\n",
      "[2380]\ttrain-error:0.35245\n",
      "[2381]\ttrain-error:0.35245\n",
      "[2382]\ttrain-error:0.35245\n",
      "[2383]\ttrain-error:0.35245\n",
      "[2384]\ttrain-error:0.35245\n",
      "[2385]\ttrain-error:0.35245\n",
      "[2386]\ttrain-error:0.35245\n",
      "[2387]\ttrain-error:0.35245\n",
      "[2388]\ttrain-error:0.35245\n",
      "[2389]\ttrain-error:0.35245\n",
      "[2390]\ttrain-error:0.35245\n",
      "[2391]\ttrain-error:0.35245\n",
      "[2392]\ttrain-error:0.35245\n",
      "[2393]\ttrain-error:0.35245\n",
      "[2394]\ttrain-error:0.35245\n",
      "[2395]\ttrain-error:0.35245\n",
      "[2396]\ttrain-error:0.35245\n",
      "[2397]\ttrain-error:0.35245\n",
      "[2398]\ttrain-error:0.35245\n",
      "[2399]\ttrain-error:0.35245\n",
      "[2400]\ttrain-error:0.35245\n",
      "[2401]\ttrain-error:0.35245\n",
      "[2402]\ttrain-error:0.35245\n",
      "[2403]\ttrain-error:0.35245\n",
      "[2404]\ttrain-error:0.35245\n",
      "[2405]\ttrain-error:0.35245\n",
      "[2406]\ttrain-error:0.35245\n",
      "[2407]\ttrain-error:0.35245\n",
      "[2408]\ttrain-error:0.35245\n",
      "[2409]\ttrain-error:0.35245\n",
      "[2410]\ttrain-error:0.35245\n",
      "[2411]\ttrain-error:0.35245\n",
      "[2412]\ttrain-error:0.35245\n",
      "[2413]\ttrain-error:0.35245\n",
      "[2414]\ttrain-error:0.35245\n",
      "[2415]\ttrain-error:0.35245\n",
      "[2416]\ttrain-error:0.35245\n",
      "[2417]\ttrain-error:0.35245\n",
      "[2418]\ttrain-error:0.35245\n",
      "[2419]\ttrain-error:0.35245\n",
      "[2420]\ttrain-error:0.35245\n",
      "[2421]\ttrain-error:0.35245\n",
      "[2422]\ttrain-error:0.35245\n",
      "[2423]\ttrain-error:0.35245\n",
      "[2424]\ttrain-error:0.35245\n",
      "[2425]\ttrain-error:0.35245\n",
      "[2426]\ttrain-error:0.35245\n",
      "[2427]\ttrain-error:0.35245\n",
      "[2428]\ttrain-error:0.35245\n",
      "[2429]\ttrain-error:0.35245\n",
      "[2430]\ttrain-error:0.35245\n",
      "[2431]\ttrain-error:0.35245\n",
      "[2432]\ttrain-error:0.35245\n",
      "[2433]\ttrain-error:0.35245\n",
      "[2434]\ttrain-error:0.35245\n",
      "[2435]\ttrain-error:0.35245\n",
      "[2436]\ttrain-error:0.35245\n",
      "[2437]\ttrain-error:0.35245\n",
      "[2438]\ttrain-error:0.35245\n",
      "[2439]\ttrain-error:0.35245\n",
      "[2440]\ttrain-error:0.35245\n",
      "[2441]\ttrain-error:0.35245\n",
      "[2442]\ttrain-error:0.35245\n",
      "[2443]\ttrain-error:0.35245\n",
      "[2444]\ttrain-error:0.35245\n",
      "[2445]\ttrain-error:0.35245\n",
      "[2446]\ttrain-error:0.35245\n",
      "[2447]\ttrain-error:0.35245\n",
      "[2448]\ttrain-error:0.35245\n",
      "[2449]\ttrain-error:0.35245\n",
      "[2450]\ttrain-error:0.35245\n",
      "[2451]\ttrain-error:0.35245\n",
      "[2452]\ttrain-error:0.35245\n",
      "[2453]\ttrain-error:0.35245\n",
      "[2454]\ttrain-error:0.35245\n",
      "[2455]\ttrain-error:0.35245\n",
      "[2456]\ttrain-error:0.35245\n",
      "[2457]\ttrain-error:0.35245\n",
      "[2458]\ttrain-error:0.35245\n",
      "[2459]\ttrain-error:0.35245\n",
      "[2460]\ttrain-error:0.35245\n",
      "[2461]\ttrain-error:0.35245\n",
      "[2462]\ttrain-error:0.35245\n",
      "[2463]\ttrain-error:0.35245\n",
      "[2464]\ttrain-error:0.35245\n",
      "[2465]\ttrain-error:0.35245\n",
      "[2466]\ttrain-error:0.35245\n",
      "[2467]\ttrain-error:0.35245\n",
      "[2468]\ttrain-error:0.35245\n",
      "[2469]\ttrain-error:0.35245\n",
      "[2470]\ttrain-error:0.35245\n",
      "[2471]\ttrain-error:0.35245\n",
      "[2472]\ttrain-error:0.35245\n",
      "[2473]\ttrain-error:0.35245\n",
      "[2474]\ttrain-error:0.35245\n",
      "[2475]\ttrain-error:0.35245\n",
      "[2476]\ttrain-error:0.35245\n",
      "[2477]\ttrain-error:0.35245\n",
      "[2478]\ttrain-error:0.35245\n",
      "[2479]\ttrain-error:0.35245\n",
      "[2480]\ttrain-error:0.35245\n",
      "[2481]\ttrain-error:0.35245\n",
      "[2482]\ttrain-error:0.35245\n",
      "[2483]\ttrain-error:0.35245\n",
      "[2484]\ttrain-error:0.35245\n",
      "[2485]\ttrain-error:0.35245\n",
      "[2486]\ttrain-error:0.35245\n",
      "[2487]\ttrain-error:0.35245\n",
      "[2488]\ttrain-error:0.35245\n",
      "[2489]\ttrain-error:0.35245\n",
      "[2490]\ttrain-error:0.35245\n",
      "[2491]\ttrain-error:0.35245\n",
      "[2492]\ttrain-error:0.35245\n",
      "[2493]\ttrain-error:0.35245\n",
      "[2494]\ttrain-error:0.35245\n",
      "[2495]\ttrain-error:0.35245\n",
      "[2496]\ttrain-error:0.35245\n",
      "[2497]\ttrain-error:0.35245\n",
      "[2498]\ttrain-error:0.35245\n",
      "[2499]\ttrain-error:0.35245\n",
      "[2500]\ttrain-error:0.35245\n",
      "[2501]\ttrain-error:0.35245\n",
      "[2502]\ttrain-error:0.35245\n",
      "[2503]\ttrain-error:0.35245\n",
      "[2504]\ttrain-error:0.35245\n",
      "[2505]\ttrain-error:0.35245\n",
      "[2506]\ttrain-error:0.35245\n",
      "[2507]\ttrain-error:0.35245\n",
      "[2508]\ttrain-error:0.35245\n",
      "[2509]\ttrain-error:0.35245\n",
      "[2510]\ttrain-error:0.35245\n",
      "[2511]\ttrain-error:0.35245\n",
      "[2512]\ttrain-error:0.35245\n",
      "[2513]\ttrain-error:0.35245\n",
      "[2514]\ttrain-error:0.35245\n",
      "[2515]\ttrain-error:0.35245\n",
      "[2516]\ttrain-error:0.35245\n",
      "[2517]\ttrain-error:0.35245\n",
      "[2518]\ttrain-error:0.35245\n",
      "[2519]\ttrain-error:0.35245\n",
      "[2520]\ttrain-error:0.35245\n",
      "[2521]\ttrain-error:0.35245\n",
      "[2522]\ttrain-error:0.35245\n",
      "[2523]\ttrain-error:0.35245\n",
      "[2524]\ttrain-error:0.35245\n",
      "[2525]\ttrain-error:0.35245\n",
      "[2526]\ttrain-error:0.35245\n",
      "[2527]\ttrain-error:0.35245\n",
      "[2528]\ttrain-error:0.35245\n",
      "[2529]\ttrain-error:0.35245\n",
      "[2530]\ttrain-error:0.35245\n",
      "[2531]\ttrain-error:0.35245\n",
      "[2532]\ttrain-error:0.35245\n",
      "[2533]\ttrain-error:0.35245\n",
      "[2534]\ttrain-error:0.35245\n",
      "[2535]\ttrain-error:0.35245\n",
      "[2536]\ttrain-error:0.35245\n",
      "[2537]\ttrain-error:0.35245\n",
      "[2538]\ttrain-error:0.35245\n",
      "[2539]\ttrain-error:0.35245\n",
      "[2540]\ttrain-error:0.35245\n",
      "[2541]\ttrain-error:0.35245\n",
      "[2542]\ttrain-error:0.35245\n",
      "[2543]\ttrain-error:0.35245\n",
      "[2544]\ttrain-error:0.35245\n",
      "[2545]\ttrain-error:0.35245\n",
      "[2546]\ttrain-error:0.35245\n",
      "[2547]\ttrain-error:0.35245\n",
      "[2548]\ttrain-error:0.35245\n",
      "[2549]\ttrain-error:0.35245\n",
      "[2550]\ttrain-error:0.35245\n",
      "[2551]\ttrain-error:0.35245\n",
      "[2552]\ttrain-error:0.35245\n",
      "[2553]\ttrain-error:0.35245\n",
      "[2554]\ttrain-error:0.35245\n",
      "[2555]\ttrain-error:0.35245\n",
      "[2556]\ttrain-error:0.35245\n",
      "[2557]\ttrain-error:0.35245\n",
      "[2558]\ttrain-error:0.35245\n",
      "[2559]\ttrain-error:0.35245\n",
      "[2560]\ttrain-error:0.35245\n",
      "[2561]\ttrain-error:0.35245\n",
      "[2562]\ttrain-error:0.35245\n",
      "[2563]\ttrain-error:0.35245\n",
      "[2564]\ttrain-error:0.35245\n",
      "[2565]\ttrain-error:0.35245\n",
      "[2566]\ttrain-error:0.35245\n",
      "[2567]\ttrain-error:0.35245\n",
      "[2568]\ttrain-error:0.35245\n",
      "[2569]\ttrain-error:0.35245\n",
      "[2570]\ttrain-error:0.35245\n",
      "[2571]\ttrain-error:0.35245\n",
      "[2572]\ttrain-error:0.35245\n",
      "[2573]\ttrain-error:0.35245\n",
      "[2574]\ttrain-error:0.35245\n",
      "[2575]\ttrain-error:0.35245\n",
      "[2576]\ttrain-error:0.35245\n",
      "[2577]\ttrain-error:0.35245\n",
      "[2578]\ttrain-error:0.35245\n",
      "[2579]\ttrain-error:0.35245\n",
      "[2580]\ttrain-error:0.35245\n",
      "[2581]\ttrain-error:0.35245\n",
      "[2582]\ttrain-error:0.35245\n",
      "[2583]\ttrain-error:0.35245\n",
      "[2584]\ttrain-error:0.35245\n",
      "[2585]\ttrain-error:0.35245\n",
      "[2586]\ttrain-error:0.35245\n",
      "[2587]\ttrain-error:0.35245\n",
      "[2588]\ttrain-error:0.35245\n",
      "[2589]\ttrain-error:0.35245\n",
      "[2590]\ttrain-error:0.35245\n",
      "[2591]\ttrain-error:0.35245\n",
      "[2592]\ttrain-error:0.35245\n",
      "[2593]\ttrain-error:0.35245\n",
      "[2594]\ttrain-error:0.35245\n",
      "[2595]\ttrain-error:0.35245\n",
      "[2596]\ttrain-error:0.35245\n",
      "[2597]\ttrain-error:0.35245\n",
      "[2598]\ttrain-error:0.35245\n",
      "[2599]\ttrain-error:0.35245\n",
      "[2600]\ttrain-error:0.35245\n",
      "[2601]\ttrain-error:0.35245\n",
      "[2602]\ttrain-error:0.35245\n",
      "[2603]\ttrain-error:0.35245\n",
      "[2604]\ttrain-error:0.35245\n",
      "[2605]\ttrain-error:0.35245\n",
      "[2606]\ttrain-error:0.35245\n",
      "[2607]\ttrain-error:0.35245\n",
      "[2608]\ttrain-error:0.35245\n",
      "[2609]\ttrain-error:0.35245\n",
      "[2610]\ttrain-error:0.35245\n",
      "[2611]\ttrain-error:0.35245\n",
      "[2612]\ttrain-error:0.35245\n",
      "[2613]\ttrain-error:0.35245\n",
      "[2614]\ttrain-error:0.35245\n",
      "[2615]\ttrain-error:0.35245\n",
      "[2616]\ttrain-error:0.35245\n",
      "[2617]\ttrain-error:0.35245\n",
      "[2618]\ttrain-error:0.35245\n",
      "[2619]\ttrain-error:0.35245\n",
      "[2620]\ttrain-error:0.35245\n",
      "[2621]\ttrain-error:0.35245\n",
      "[2622]\ttrain-error:0.35245\n",
      "[2623]\ttrain-error:0.35245\n",
      "[2624]\ttrain-error:0.35245\n",
      "[2625]\ttrain-error:0.35245\n",
      "[2626]\ttrain-error:0.35245\n",
      "[2627]\ttrain-error:0.35245\n",
      "[2628]\ttrain-error:0.35245\n",
      "[2629]\ttrain-error:0.35245\n",
      "[2630]\ttrain-error:0.35245\n",
      "[2631]\ttrain-error:0.35245\n",
      "[2632]\ttrain-error:0.35245\n",
      "[2633]\ttrain-error:0.35245\n",
      "[2634]\ttrain-error:0.35245\n",
      "[2635]\ttrain-error:0.35245\n",
      "[2636]\ttrain-error:0.35245\n",
      "[2637]\ttrain-error:0.35245\n",
      "[2638]\ttrain-error:0.35245\n",
      "[2639]\ttrain-error:0.35245\n",
      "[2640]\ttrain-error:0.35245\n",
      "[2641]\ttrain-error:0.35245\n",
      "[2642]\ttrain-error:0.35245\n",
      "[2643]\ttrain-error:0.35245\n",
      "[2644]\ttrain-error:0.35245\n",
      "[2645]\ttrain-error:0.35245\n",
      "[2646]\ttrain-error:0.35245\n",
      "[2647]\ttrain-error:0.35245\n",
      "[2648]\ttrain-error:0.35245\n",
      "[2649]\ttrain-error:0.35245\n",
      "[2650]\ttrain-error:0.35245\n",
      "[2651]\ttrain-error:0.35245\n",
      "[2652]\ttrain-error:0.35245\n",
      "[2653]\ttrain-error:0.35245\n",
      "[2654]\ttrain-error:0.35245\n",
      "[2655]\ttrain-error:0.35245\n",
      "[2656]\ttrain-error:0.35245\n",
      "[2657]\ttrain-error:0.35245\n",
      "[2658]\ttrain-error:0.35245\n",
      "[2659]\ttrain-error:0.35245\n",
      "[2660]\ttrain-error:0.35245\n",
      "[2661]\ttrain-error:0.35245\n",
      "[2662]\ttrain-error:0.35245\n",
      "[2663]\ttrain-error:0.35245\n",
      "[2664]\ttrain-error:0.35245\n",
      "[2665]\ttrain-error:0.35245\n",
      "[2666]\ttrain-error:0.35245\n",
      "[2667]\ttrain-error:0.35245\n",
      "[2668]\ttrain-error:0.35245\n",
      "[2669]\ttrain-error:0.35245\n",
      "[2670]\ttrain-error:0.35245\n",
      "[2671]\ttrain-error:0.35245\n",
      "[2672]\ttrain-error:0.35245\n",
      "[2673]\ttrain-error:0.35245\n",
      "[2674]\ttrain-error:0.35245\n",
      "[2675]\ttrain-error:0.35245\n",
      "[2676]\ttrain-error:0.35245\n",
      "[2677]\ttrain-error:0.35245\n",
      "[2678]\ttrain-error:0.35245\n",
      "[2679]\ttrain-error:0.35245\n",
      "[2680]\ttrain-error:0.35245\n",
      "[2681]\ttrain-error:0.35245\n",
      "[2682]\ttrain-error:0.35245\n",
      "[2683]\ttrain-error:0.35245\n",
      "[2684]\ttrain-error:0.35245\n",
      "[2685]\ttrain-error:0.35245\n",
      "[2686]\ttrain-error:0.35245\n",
      "[2687]\ttrain-error:0.35245\n",
      "[2688]\ttrain-error:0.35245\n",
      "[2689]\ttrain-error:0.35245\n",
      "[2690]\ttrain-error:0.35245\n",
      "[2691]\ttrain-error:0.35245\n",
      "[2692]\ttrain-error:0.35245\n",
      "[2693]\ttrain-error:0.35245\n",
      "[2694]\ttrain-error:0.35245\n",
      "[2695]\ttrain-error:0.35245\n",
      "[2696]\ttrain-error:0.35245\n",
      "[2697]\ttrain-error:0.35245\n",
      "[2698]\ttrain-error:0.35245\n",
      "[2699]\ttrain-error:0.35245\n",
      "[2700]\ttrain-error:0.35245\n",
      "[2701]\ttrain-error:0.35245\n",
      "[2702]\ttrain-error:0.35245\n",
      "[2703]\ttrain-error:0.35245\n",
      "[2704]\ttrain-error:0.35245\n",
      "[2705]\ttrain-error:0.35245\n",
      "[2706]\ttrain-error:0.35245\n",
      "[2707]\ttrain-error:0.35245\n",
      "[2708]\ttrain-error:0.35245\n",
      "[2709]\ttrain-error:0.35245\n",
      "[2710]\ttrain-error:0.35245\n",
      "[2711]\ttrain-error:0.35245\n",
      "[2712]\ttrain-error:0.35245\n",
      "[2713]\ttrain-error:0.35245\n",
      "[2714]\ttrain-error:0.35245\n",
      "[2715]\ttrain-error:0.35245\n",
      "[2716]\ttrain-error:0.35245\n",
      "[2717]\ttrain-error:0.35245\n",
      "[2718]\ttrain-error:0.35245\n",
      "[2719]\ttrain-error:0.35245\n",
      "[2720]\ttrain-error:0.35245\n",
      "[2721]\ttrain-error:0.35245\n",
      "[2722]\ttrain-error:0.35245\n",
      "[2723]\ttrain-error:0.35245\n",
      "[2724]\ttrain-error:0.35245\n",
      "[2725]\ttrain-error:0.35245\n",
      "[2726]\ttrain-error:0.35245\n",
      "[2727]\ttrain-error:0.35245\n",
      "[2728]\ttrain-error:0.35245\n",
      "[2729]\ttrain-error:0.35245\n",
      "[2730]\ttrain-error:0.35245\n",
      "[2731]\ttrain-error:0.35245\n",
      "[2732]\ttrain-error:0.35245\n",
      "[2733]\ttrain-error:0.35245\n",
      "[2734]\ttrain-error:0.35245\n",
      "[2735]\ttrain-error:0.35245\n",
      "[2736]\ttrain-error:0.35245\n",
      "[2737]\ttrain-error:0.35245\n",
      "[2738]\ttrain-error:0.35245\n",
      "[2739]\ttrain-error:0.35245\n",
      "[2740]\ttrain-error:0.35245\n",
      "[2741]\ttrain-error:0.35245\n",
      "[2742]\ttrain-error:0.35245\n",
      "[2743]\ttrain-error:0.35245\n",
      "[2744]\ttrain-error:0.35245\n",
      "[2745]\ttrain-error:0.35245\n",
      "[2746]\ttrain-error:0.35245\n",
      "[2747]\ttrain-error:0.35245\n",
      "[2748]\ttrain-error:0.35245\n",
      "[2749]\ttrain-error:0.35245\n",
      "[2750]\ttrain-error:0.35245\n",
      "[2751]\ttrain-error:0.35245\n",
      "[2752]\ttrain-error:0.35245\n",
      "[2753]\ttrain-error:0.35245\n",
      "[2754]\ttrain-error:0.35245\n",
      "[2755]\ttrain-error:0.35245\n",
      "[2756]\ttrain-error:0.35245\n",
      "[2757]\ttrain-error:0.35245\n",
      "[2758]\ttrain-error:0.35245\n",
      "[2759]\ttrain-error:0.35245\n",
      "[2760]\ttrain-error:0.35245\n",
      "[2761]\ttrain-error:0.35245\n",
      "[2762]\ttrain-error:0.35245\n",
      "[2763]\ttrain-error:0.35245\n",
      "[2764]\ttrain-error:0.35245\n",
      "[2765]\ttrain-error:0.35245\n",
      "[2766]\ttrain-error:0.35245\n",
      "[2767]\ttrain-error:0.35245\n",
      "[2768]\ttrain-error:0.35245\n",
      "[2769]\ttrain-error:0.35245\n",
      "[2770]\ttrain-error:0.35245\n",
      "[2771]\ttrain-error:0.35245\n",
      "[2772]\ttrain-error:0.35245\n",
      "[2773]\ttrain-error:0.35245\n",
      "[2774]\ttrain-error:0.35245\n",
      "[2775]\ttrain-error:0.35245\n",
      "[2776]\ttrain-error:0.35245\n",
      "[2777]\ttrain-error:0.35245\n",
      "[2778]\ttrain-error:0.35245\n",
      "[2779]\ttrain-error:0.35245\n",
      "[2780]\ttrain-error:0.35245\n",
      "[2781]\ttrain-error:0.35245\n",
      "[2782]\ttrain-error:0.35245\n",
      "[2783]\ttrain-error:0.35245\n",
      "[2784]\ttrain-error:0.35245\n",
      "[2785]\ttrain-error:0.35245\n",
      "[2786]\ttrain-error:0.35245\n",
      "[2787]\ttrain-error:0.35245\n",
      "[2788]\ttrain-error:0.35245\n",
      "[2789]\ttrain-error:0.35245\n",
      "[2790]\ttrain-error:0.35245\n",
      "[2791]\ttrain-error:0.35245\n",
      "[2792]\ttrain-error:0.35245\n",
      "[2793]\ttrain-error:0.35245\n",
      "[2794]\ttrain-error:0.35245\n",
      "[2795]\ttrain-error:0.35245\n",
      "[2796]\ttrain-error:0.35245\n",
      "[2797]\ttrain-error:0.35245\n",
      "[2798]\ttrain-error:0.35245\n",
      "[2799]\ttrain-error:0.35245\n",
      "[2800]\ttrain-error:0.35245\n",
      "[2801]\ttrain-error:0.35245\n",
      "[2802]\ttrain-error:0.35245\n",
      "[2803]\ttrain-error:0.35245\n",
      "[2804]\ttrain-error:0.35245\n",
      "[2805]\ttrain-error:0.35245\n",
      "[2806]\ttrain-error:0.35245\n",
      "[2807]\ttrain-error:0.35245\n",
      "[2808]\ttrain-error:0.35245\n",
      "[2809]\ttrain-error:0.35245\n",
      "[2810]\ttrain-error:0.35245\n",
      "[2811]\ttrain-error:0.35245\n",
      "[2812]\ttrain-error:0.35245\n",
      "[2813]\ttrain-error:0.35245\n",
      "[2814]\ttrain-error:0.35245\n",
      "[2815]\ttrain-error:0.35245\n",
      "[2816]\ttrain-error:0.35245\n",
      "[2817]\ttrain-error:0.35245\n",
      "[2818]\ttrain-error:0.35245\n",
      "[2819]\ttrain-error:0.35245\n",
      "[2820]\ttrain-error:0.35245\n",
      "[2821]\ttrain-error:0.35245\n",
      "[2822]\ttrain-error:0.35245\n",
      "[2823]\ttrain-error:0.35245\n",
      "[2824]\ttrain-error:0.35245\n",
      "[2825]\ttrain-error:0.35245\n",
      "[2826]\ttrain-error:0.35245\n",
      "[2827]\ttrain-error:0.35245\n",
      "[2828]\ttrain-error:0.35245\n",
      "[2829]\ttrain-error:0.35245\n",
      "[2830]\ttrain-error:0.35245\n",
      "[2831]\ttrain-error:0.35245\n",
      "[2832]\ttrain-error:0.35245\n",
      "[2833]\ttrain-error:0.35245\n",
      "[2834]\ttrain-error:0.35245\n",
      "[2835]\ttrain-error:0.35245\n",
      "[2836]\ttrain-error:0.35245\n",
      "[2837]\ttrain-error:0.35245\n",
      "[2838]\ttrain-error:0.35245\n",
      "[2839]\ttrain-error:0.35245\n",
      "[2840]\ttrain-error:0.35245\n",
      "[2841]\ttrain-error:0.35245\n",
      "[2842]\ttrain-error:0.35245\n",
      "[2843]\ttrain-error:0.35245\n",
      "[2844]\ttrain-error:0.35245\n",
      "[2845]\ttrain-error:0.35245\n",
      "[2846]\ttrain-error:0.35245\n",
      "[2847]\ttrain-error:0.35245\n",
      "[2848]\ttrain-error:0.35245\n",
      "[2849]\ttrain-error:0.35245\n",
      "[2850]\ttrain-error:0.35245\n",
      "[2851]\ttrain-error:0.35245\n",
      "[2852]\ttrain-error:0.35245\n",
      "[2853]\ttrain-error:0.35245\n",
      "[2854]\ttrain-error:0.35245\n",
      "[2855]\ttrain-error:0.35245\n",
      "[2856]\ttrain-error:0.35245\n",
      "[2857]\ttrain-error:0.35245\n",
      "[2858]\ttrain-error:0.35245\n",
      "[2859]\ttrain-error:0.35245\n",
      "[2860]\ttrain-error:0.35245\n",
      "[2861]\ttrain-error:0.35245\n",
      "[2862]\ttrain-error:0.35245\n",
      "[2863]\ttrain-error:0.35245\n",
      "[2864]\ttrain-error:0.35245\n",
      "[2865]\ttrain-error:0.35245\n",
      "[2866]\ttrain-error:0.35245\n",
      "[2867]\ttrain-error:0.35245\n",
      "[2868]\ttrain-error:0.35245\n",
      "[2869]\ttrain-error:0.35245\n",
      "[2870]\ttrain-error:0.35245\n",
      "[2871]\ttrain-error:0.35245\n",
      "[2872]\ttrain-error:0.35245\n",
      "[2873]\ttrain-error:0.35245\n",
      "[2874]\ttrain-error:0.35245\n",
      "[2875]\ttrain-error:0.35245\n",
      "[2876]\ttrain-error:0.35245\n",
      "[2877]\ttrain-error:0.35245\n",
      "[2878]\ttrain-error:0.35245\n",
      "[2879]\ttrain-error:0.35245\n",
      "[2880]\ttrain-error:0.35245\n",
      "[2881]\ttrain-error:0.35245\n",
      "[2882]\ttrain-error:0.35245\n",
      "[2883]\ttrain-error:0.35245\n",
      "[2884]\ttrain-error:0.35245\n",
      "[2885]\ttrain-error:0.35245\n",
      "[2886]\ttrain-error:0.35245\n",
      "[2887]\ttrain-error:0.35245\n",
      "[2888]\ttrain-error:0.35245\n",
      "[2889]\ttrain-error:0.35245\n",
      "[2890]\ttrain-error:0.35245\n",
      "[2891]\ttrain-error:0.35245\n",
      "[2892]\ttrain-error:0.35245\n",
      "[2893]\ttrain-error:0.35245\n",
      "[2894]\ttrain-error:0.35245\n",
      "[2895]\ttrain-error:0.35245\n",
      "[2896]\ttrain-error:0.35245\n",
      "[2897]\ttrain-error:0.35245\n",
      "[2898]\ttrain-error:0.35245\n",
      "[2899]\ttrain-error:0.35245\n",
      "[2900]\ttrain-error:0.35245\n",
      "[2901]\ttrain-error:0.35245\n",
      "[2902]\ttrain-error:0.35245\n",
      "[2903]\ttrain-error:0.35245\n",
      "[2904]\ttrain-error:0.35245\n",
      "[2905]\ttrain-error:0.35245\n",
      "[2906]\ttrain-error:0.35245\n",
      "[2907]\ttrain-error:0.35245\n",
      "[2908]\ttrain-error:0.35245\n",
      "[2909]\ttrain-error:0.35245\n",
      "[2910]\ttrain-error:0.35245\n",
      "[2911]\ttrain-error:0.35245\n",
      "[2912]\ttrain-error:0.35245\n",
      "[2913]\ttrain-error:0.35245\n",
      "[2914]\ttrain-error:0.35245\n",
      "[2915]\ttrain-error:0.35245\n",
      "[2916]\ttrain-error:0.35245\n",
      "[2917]\ttrain-error:0.35245\n",
      "[2918]\ttrain-error:0.35245\n",
      "[2919]\ttrain-error:0.35245\n",
      "[2920]\ttrain-error:0.35245\n",
      "[2921]\ttrain-error:0.35245\n",
      "[2922]\ttrain-error:0.35245\n",
      "[2923]\ttrain-error:0.35245\n",
      "[2924]\ttrain-error:0.35245\n",
      "[2925]\ttrain-error:0.35245\n",
      "[2926]\ttrain-error:0.35245\n",
      "[2927]\ttrain-error:0.35245\n",
      "[2928]\ttrain-error:0.35245\n",
      "[2929]\ttrain-error:0.35245\n",
      "[2930]\ttrain-error:0.35245\n",
      "[2931]\ttrain-error:0.35245\n",
      "[2932]\ttrain-error:0.35245\n",
      "[2933]\ttrain-error:0.35245\n",
      "[2934]\ttrain-error:0.35245\n",
      "[2935]\ttrain-error:0.35245\n",
      "[2936]\ttrain-error:0.35245\n",
      "[2937]\ttrain-error:0.35245\n",
      "[2938]\ttrain-error:0.35245\n",
      "[2939]\ttrain-error:0.35245\n",
      "[2940]\ttrain-error:0.35245\n",
      "[2941]\ttrain-error:0.35245\n",
      "[2942]\ttrain-error:0.35245\n",
      "[2943]\ttrain-error:0.35245\n",
      "[2944]\ttrain-error:0.35245\n",
      "[2945]\ttrain-error:0.35245\n",
      "[2946]\ttrain-error:0.35245\n",
      "[2947]\ttrain-error:0.35245\n",
      "[2948]\ttrain-error:0.35245\n",
      "[2949]\ttrain-error:0.35245\n",
      "[2950]\ttrain-error:0.35245\n",
      "[2951]\ttrain-error:0.35245\n",
      "[2952]\ttrain-error:0.35245\n",
      "[2953]\ttrain-error:0.35245\n",
      "[2954]\ttrain-error:0.35245\n",
      "[2955]\ttrain-error:0.35245\n",
      "[2956]\ttrain-error:0.35245\n",
      "[2957]\ttrain-error:0.35245\n",
      "[2958]\ttrain-error:0.35245\n",
      "[2959]\ttrain-error:0.35245\n",
      "[2960]\ttrain-error:0.35245\n",
      "[2961]\ttrain-error:0.35245\n",
      "[2962]\ttrain-error:0.35245\n",
      "[2963]\ttrain-error:0.35245\n",
      "[2964]\ttrain-error:0.35245\n",
      "[2965]\ttrain-error:0.35245\n",
      "[2966]\ttrain-error:0.35245\n",
      "[2967]\ttrain-error:0.35245\n",
      "[2968]\ttrain-error:0.35245\n",
      "[2969]\ttrain-error:0.35245\n",
      "[2970]\ttrain-error:0.35245\n",
      "[2971]\ttrain-error:0.35245\n",
      "[2972]\ttrain-error:0.35245\n",
      "[2973]\ttrain-error:0.35245\n",
      "[2974]\ttrain-error:0.35245\n",
      "[2975]\ttrain-error:0.35245\n",
      "[2976]\ttrain-error:0.35245\n",
      "[2977]\ttrain-error:0.35245\n",
      "[2978]\ttrain-error:0.35245\n",
      "[2979]\ttrain-error:0.35245\n",
      "[2980]\ttrain-error:0.35245\n",
      "[2981]\ttrain-error:0.35245\n",
      "[2982]\ttrain-error:0.35245\n",
      "[2983]\ttrain-error:0.35245\n",
      "[2984]\ttrain-error:0.35245\n",
      "[2985]\ttrain-error:0.35245\n",
      "[2986]\ttrain-error:0.35245\n",
      "[2987]\ttrain-error:0.35245\n",
      "[2988]\ttrain-error:0.35245\n",
      "[2989]\ttrain-error:0.35245\n",
      "[2990]\ttrain-error:0.35245\n",
      "[2991]\ttrain-error:0.35245\n",
      "[2992]\ttrain-error:0.35245\n",
      "[2993]\ttrain-error:0.35245\n",
      "[2994]\ttrain-error:0.35245\n",
      "[2995]\ttrain-error:0.35245\n",
      "[2996]\ttrain-error:0.35245\n",
      "[2997]\ttrain-error:0.35245\n",
      "[2998]\ttrain-error:0.35245\n",
      "[2999]\ttrain-error:0.35245\n",
      "\n",
      "train error: 0.352445\n",
      "train accuracy: 0.647555\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "y_train_true, labels_dict = convert_class_labels(y_train_first_decisions, labels_dict=labels_dict)\n",
    "\n",
    "dtrain = xgb.DMatrix(x_train_first_decisions, label=y_train_true)\n",
    "dvalidation = [(xgb.DMatrix(x_train_first_decisions, label=y_train_true),'train')]\n",
    "\n",
    "if min_profit_percent==profit_noise_percent:\n",
    "    # binrary classification problem (buy or sell)\n",
    "    error_metric_name = 'error'\n",
    "    xgb_params = {'max_depth':max_depth, 'learning_rate':learning_rate, 'objective':'binary:logistic', 'eval_metric': error_metric_name, 'gamma':gamma,\n",
    "                  'colsample_bytree':colsample_bytree, 'subsample':subsample}\n",
    "else:\n",
    "    # multi-class classification problem (buy, sell, or wiat)\n",
    "    error_metric_name = 'merror'\n",
    "    xgb_params = {'max_depth':max_depth, 'learning_rate':learning_rate, 'objective':'multi:softmax', 'num_class': num_class,\n",
    "                  'eval_metric': error_metric_name, 'gamma':gamma, 'colsample_bytree':colsample_bytree, 'subsample':subsample}\n",
    "evals_result = {}\n",
    "xgb_first_decision_predictor = xgb.train(xgb_params, dtrain, num_boost_round=n_estimators, evals=dvalidation, evals_result=evals_result)\n",
    "\n",
    "# print train error\n",
    "train_error = evals_result['train']['error'][-1]\n",
    "print(f'\\ntrain error: {train_error}')\n",
    "print(f'train accuracy: {1 - train_error}')\n",
    "\n",
    "# save model\n",
    "xgb_first_decision_predictor.save_model(f'../my_stuff/{cur_pair}-{timeframe}_{min_profit_percent}-min_profit_{lots_per_trade}-lots_{currency_side}-cur_side'\n",
    "                                        f'_{tenkan_period}-{kijun_period}-{senkou_b_period}-{sigs_for_filename}-ichi_xgb_classifier.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 1320 rows of tick data from C:\\GitHub Repos\\ForexMachine\\Data\\.cache\\mt5_EURUSD_h1_ticks_2020-10-02T00;00UTC_to_2020-12-18T00;00UTC.csv\n",
      "saved 1320 rows of EURUSD h1 tick data to C:\\GitHub Repos\\ForexMachine\\Data\\RawData\\mt5_EURUSD_h1_ticks_2020-10-02T00;00UTC_to_2020-12-18T00;00UTC.csv, done.\n",
      "\n",
      "test error: 0.32158590308370044\n",
      "test accuracy: 0.6784140969162995\n",
      "potential profits from test data: 81816.39999999983\n",
      "buy/sell counts:\n",
      "buy     193\n",
      "sell     34\n",
      "Name: first_decision, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# test model on test data\n",
    "tick_data_filepath = gi.download_mt5_data(\"EURUSD\", 'H1', '2020-10-02', '2020-12-18')\n",
    "data_with_indicators = gi.add_indicators_to_raw(filepath=tick_data_filepath, \n",
    "                                                save_to_disk=True, \n",
    "                                                config=model_config, \n",
    "                                                has_headers=True,\n",
    "                                                datetime_col='datetime')\n",
    "test_data = add_features(data_with_indicators)\n",
    "\n",
    "test_data_labels = generate_ichimoku_labels(test_data, label_non_signals=label_non_signals, min_profit_percent=min_profit_percent, \n",
    "                                             profit_noise_percent=profit_noise_percent, signals_to_consider=signals_to_consider, \n",
    "                                             contract_size=contract_size, lots_per_trade=lots_per_trade,\n",
    "                                             in_quote_currency=in_quote_currency,pip_resolution=pip_resolution)\n",
    "\n",
    "test_data = apply_perc_change(test_data, cols=pc_cols, limit=1)\n",
    "start_idx, end_idx = no_missing_data_idx_range(test_data, early_ending_cols=['chikou_span_visual'])\n",
    "test_data = test_data.iloc[start_idx:end_idx+1]\n",
    "test_data_labels = test_data_labels.iloc[start_idx:end_idx+1]\n",
    "\n",
    "x_test_first_decisions, y_test_first_decisions = missing_labels_preprocess(test_data, test_data_labels, 'first_decision')\n",
    "x_test_first_decisions_profits, y_test_first_decisions_profits = missing_labels_preprocess(test_data, test_data_labels, 'best_profit_first_decision')\n",
    "\n",
    "y_test_true, labels_dict = convert_class_labels(y_test_first_decisions, to_numpy=True, labels_dict=labels_dict)\n",
    "\n",
    "dtest = xgb.DMatrix(x_test_first_decisions)\n",
    "y_test_probs = xgb_first_decision_predictor.predict(dtest)\n",
    "\n",
    "y_test_preds = np.around(y_test_probs)\n",
    "y_test_preds = pd.DataFrame(y_test_preds, columns=y_test_first_decisions.columns)\n",
    "y_test_preds = convert_class_labels(y_test_preds, to_ints=False, labels_dict=labels_dict)[0]\n",
    "\n",
    "# print results\n",
    "test_error, test_wrong_indices = error_rate(y_test_first_decisions, y_test_preds)\n",
    "p_profits_first_decision = potention_profits(y_test_first_decisions, y_test_preds, y_test_first_decisions_profits)\n",
    "\n",
    "print(f'\\ntest error: {test_error}')\n",
    "print(f'test accuracy: {1 - test_error}')\n",
    "print(f'potential profits from test data: {p_profits_first_decision}')\n",
    "print(f'buy/sell counts:\\n{y_test_preds[\"first_decision\"].value_counts()}')\n",
    "\n",
    "x = x_test_first_decisions.to_numpy()\n",
    "ytp = y_test_preds[\"first_decision\"].to_numpy()\n",
    "\n",
    "sell_inputs = []\n",
    "for i in range(len(ytp)):\n",
    "    if ytp[i] == 'sell':\n",
    "        sell_inputs.append(x[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### analyze binary probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'buy', 0: 'sell'} \n",
      "\n",
      "CORRECT: true label=1, prob=0.5895295143127441,0.41047048568725586\n",
      "CORRECT: true label=1, prob=0.5297814607620239,0.4702185392379761\n",
      "CORRECT: true label=1, prob=0.545541524887085,0.45445847511291504\n",
      "CORRECT: true label=1, prob=0.5556892156600952,0.4443107843399048\n",
      "CORRECT: true label=1, prob=0.5123186707496643,0.4876813292503357\n",
      "WRONG: true label=0, prob=0.5582850575447083,0.44171494245529175\n",
      "WRONG: true label=0, prob=0.5660315752029419,0.4339684247970581\n",
      "WRONG: true label=0, prob=0.5056567788124084,0.49434322118759155\n",
      "WRONG: true label=0, prob=0.5227710008621216,0.4772289991378784\n",
      "WRONG: true label=0, prob=0.5327770709991455,0.4672229290008545\n",
      "WRONG: true label=0, prob=0.5528644919395447,0.4471355080604553\n",
      "WRONG: true label=0, prob=0.6500375866889954,0.34996241331100464\n",
      "WRONG: true label=0, prob=0.5231617093086243,0.47683829069137573\n",
      "CORRECT: true label=1, prob=0.5124702453613281,0.4875297546386719\n",
      "WRONG: true label=1, prob=0.4195222854614258,0.4195222854614258\n",
      "CORRECT: true label=1, prob=0.5706438422203064,0.4293561577796936\n",
      "CORRECT: true label=1, prob=0.5406209230422974,0.45937907695770264\n",
      "WRONG: true label=1, prob=0.4979456663131714,0.4979456663131714\n",
      "CORRECT: true label=1, prob=0.5688799619674683,0.43112003803253174\n",
      "WRONG: true label=1, prob=0.43966972827911377,0.43966972827911377\n",
      "CORRECT: true label=1, prob=0.520990252494812,0.479009747505188\n",
      "CORRECT: true label=1, prob=0.5010769963264465,0.49892300367355347\n",
      "CORRECT: true label=1, prob=0.6185513734817505,0.3814486265182495\n",
      "CORRECT: true label=1, prob=0.5808225870132446,0.41917741298675537\n",
      "CORRECT: true label=1, prob=0.6010205745697021,0.39897942543029785\n",
      "CORRECT: true label=1, prob=0.5473210215568542,0.45267897844314575\n",
      "CORRECT: true label=1, prob=0.6631102561950684,0.33688974380493164\n",
      "CORRECT: true label=1, prob=0.5281584858894348,0.4718415141105652\n",
      "CORRECT: true label=1, prob=0.5995698571205139,0.4004301428794861\n",
      "CORRECT: true label=1, prob=0.5592742562294006,0.44072574377059937\n",
      "CORRECT: true label=1, prob=0.5449569821357727,0.4550430178642273\n",
      "CORRECT: true label=1, prob=0.5747401118278503,0.42525988817214966\n",
      "CORRECT: true label=1, prob=0.6426474452018738,0.3573525547981262\n",
      "WRONG: true label=1, prob=0.45803505182266235,0.45803505182266235\n",
      "WRONG: true label=1, prob=0.4736989736557007,0.4736989736557007\n",
      "WRONG: true label=0, prob=0.5700356960296631,0.4299643039703369\n",
      "WRONG: true label=0, prob=0.5647643804550171,0.4352356195449829\n",
      "WRONG: true label=0, prob=0.5927563905715942,0.40724360942840576\n",
      "WRONG: true label=0, prob=0.6473605036735535,0.35263949632644653\n",
      "WRONG: true label=0, prob=0.6686452627182007,0.3313547372817993\n",
      "WRONG: true label=0, prob=0.5112097263336182,0.48879027366638184\n",
      "WRONG: true label=0, prob=0.6022337675094604,0.39776623249053955\n",
      "WRONG: true label=0, prob=0.5251675844192505,0.4748324155807495\n",
      "WRONG: true label=0, prob=0.5741732716560364,0.4258267283439636\n",
      "WRONG: true label=0, prob=0.5670830607414246,0.43291693925857544\n",
      "WRONG: true label=0, prob=0.5979282855987549,0.4020717144012451\n",
      "WRONG: true label=0, prob=0.5621824264526367,0.4378175735473633\n",
      "WRONG: true label=0, prob=0.5766111016273499,0.42338889837265015\n",
      "WRONG: true label=0, prob=0.507339358329773,0.49266064167022705\n",
      "WRONG: true label=0, prob=0.5777232050895691,0.4222767949104309\n",
      "WRONG: true label=0, prob=0.5432785153388977,0.4567214846611023\n",
      "WRONG: true label=0, prob=0.5879079699516296,0.41209203004837036\n",
      "WRONG: true label=0, prob=0.591177225112915,0.40882277488708496\n",
      "WRONG: true label=0, prob=0.5204887986183167,0.47951120138168335\n",
      "WRONG: true label=0, prob=0.5420020222663879,0.45799797773361206\n",
      "WRONG: true label=0, prob=0.5031957030296326,0.49680429697036743\n",
      "WRONG: true label=0, prob=0.5652128458023071,0.43478715419769287\n",
      "WRONG: true label=0, prob=0.5664938688278198,0.4335061311721802\n",
      "WRONG: true label=0, prob=0.6217448711395264,0.37825512886047363\n",
      "WRONG: true label=0, prob=0.6187580227851868,0.38124197721481323\n",
      "WRONG: true label=0, prob=0.598871648311615,0.401128351688385\n",
      "WRONG: true label=0, prob=0.6009926795959473,0.39900732040405273\n",
      "WRONG: true label=0, prob=0.5261088013648987,0.4738911986351013\n",
      "WRONG: true label=0, prob=0.5660077333450317,0.43399226665496826\n",
      "WRONG: true label=0, prob=0.5105741024017334,0.4894258975982666\n",
      "WRONG: true label=0, prob=0.5773093104362488,0.4226906895637512\n",
      "WRONG: true label=0, prob=0.561493456363678,0.438506543636322\n",
      "CORRECT: true label=0, prob=0.4797033965587616,0.4797033965587616\n",
      "CORRECT: true label=1, prob=0.5816168189048767,0.4183831810951233\n",
      "CORRECT: true label=1, prob=0.5643792748451233,0.4356207251548767\n",
      "CORRECT: true label=1, prob=0.6164213418960571,0.38357865810394287\n",
      "WRONG: true label=1, prob=0.49137789011001587,0.49137789011001587\n",
      "CORRECT: true label=1, prob=0.5193009376525879,0.4806990623474121\n",
      "WRONG: true label=1, prob=0.49113526940345764,0.49113526940345764\n",
      "CORRECT: true label=1, prob=0.6360850930213928,0.3639149069786072\n",
      "CORRECT: true label=1, prob=0.5647526979446411,0.4352473020553589\n",
      "CORRECT: true label=1, prob=0.7250692844390869,0.2749307155609131\n",
      "CORRECT: true label=1, prob=0.6063408255577087,0.39365917444229126\n",
      "CORRECT: true label=1, prob=0.5219616293907166,0.47803837060928345\n",
      "CORRECT: true label=1, prob=0.5027216076850891,0.4972783923149109\n",
      "CORRECT: true label=1, prob=0.5216562747955322,0.4783437252044678\n",
      "CORRECT: true label=1, prob=0.6099354028701782,0.3900645971298218\n",
      "WRONG: true label=1, prob=0.47787922620773315,0.47787922620773315\n",
      "CORRECT: true label=0, prob=0.3289063274860382,0.3289063274860382\n",
      "CORRECT: true label=1, prob=0.6745615601539612,0.3254384398460388\n",
      "CORRECT: true label=1, prob=0.7030783891677856,0.29692161083221436\n",
      "WRONG: true label=1, prob=0.3754454553127289,0.3754454553127289\n",
      "WRONG: true label=1, prob=0.2905483841896057,0.2905483841896057\n",
      "CORRECT: true label=1, prob=0.6336533427238464,0.36634665727615356\n",
      "CORRECT: true label=1, prob=0.5418124794960022,0.4581875205039978\n",
      "CORRECT: true label=1, prob=0.5382311344146729,0.46176886558532715\n",
      "CORRECT: true label=1, prob=0.6099249124526978,0.39007508754730225\n",
      "CORRECT: true label=1, prob=0.5613350868225098,0.43866491317749023\n",
      "CORRECT: true label=1, prob=0.5371860861778259,0.4628139138221741\n",
      "CORRECT: true label=1, prob=0.5206968188285828,0.47930318117141724\n",
      "CORRECT: true label=1, prob=0.5876682996749878,0.4123317003250122\n",
      "WRONG: true label=1, prob=0.4183788597583771,0.4183788597583771\n",
      "WRONG: true label=1, prob=0.4969276487827301,0.4969276487827301\n",
      "CORRECT: true label=1, prob=0.5501113533973694,0.4498886466026306\n",
      "CORRECT: true label=1, prob=0.567900538444519,0.43209946155548096\n",
      "CORRECT: true label=1, prob=0.6128256916999817,0.3871743083000183\n",
      "CORRECT: true label=1, prob=0.5276408791542053,0.4723591208457947\n",
      "CORRECT: true label=1, prob=0.5534044504165649,0.44659554958343506\n",
      "CORRECT: true label=1, prob=0.5380335450172424,0.46196645498275757\n",
      "WRONG: true label=1, prob=0.4873075783252716,0.4873075783252716\n",
      "CORRECT: true label=1, prob=0.5396273732185364,0.4603726267814636\n",
      "WRONG: true label=1, prob=0.4833562970161438,0.4833562970161438\n",
      "WRONG: true label=1, prob=0.40731728076934814,0.40731728076934814\n",
      "WRONG: true label=1, prob=0.4996383488178253,0.4996383488178253\n",
      "CORRECT: true label=1, prob=0.5881766080856323,0.4118233919143677\n",
      "CORRECT: true label=1, prob=0.589648962020874,0.410351037979126\n",
      "CORRECT: true label=1, prob=0.551570475101471,0.44842952489852905\n",
      "CORRECT: true label=1, prob=0.5135141611099243,0.4864858388900757\n",
      "CORRECT: true label=1, prob=0.5309388041496277,0.4690611958503723\n",
      "CORRECT: true label=1, prob=0.5249996185302734,0.47500038146972656\n",
      "CORRECT: true label=1, prob=0.5646517276763916,0.4353482723236084\n",
      "CORRECT: true label=1, prob=0.5865254402160645,0.41347455978393555\n",
      "CORRECT: true label=1, prob=0.5665123462677002,0.4334876537322998\n",
      "WRONG: true label=1, prob=0.49343010783195496,0.49343010783195496\n",
      "CORRECT: true label=1, prob=0.5792703628540039,0.4207296371459961\n",
      "CORRECT: true label=1, prob=0.5508081912994385,0.4491918087005615\n",
      "CORRECT: true label=1, prob=0.5349986553192139,0.46500134468078613\n",
      "CORRECT: true label=1, prob=0.5612311363220215,0.4387688636779785\n",
      "CORRECT: true label=1, prob=0.5422376394271851,0.45776236057281494\n",
      "CORRECT: true label=1, prob=0.6010702252388,0.39892977476119995\n",
      "WRONG: true label=1, prob=0.4790061116218567,0.4790061116218567\n",
      "CORRECT: true label=1, prob=0.5813272595405579,0.41867274045944214\n",
      "CORRECT: true label=1, prob=0.5310149192810059,0.46898508071899414\n",
      "WRONG: true label=1, prob=0.4586901366710663,0.4586901366710663\n",
      "WRONG: true label=1, prob=0.4920947849750519,0.4920947849750519\n",
      "WRONG: true label=1, prob=0.4501139521598816,0.4501139521598816\n",
      "CORRECT: true label=1, prob=0.5367591977119446,0.4632408022880554\n",
      "CORRECT: true label=1, prob=0.5368509888648987,0.4631490111351013\n",
      "CORRECT: true label=1, prob=0.5024515986442566,0.4975484013557434\n",
      "CORRECT: true label=1, prob=0.5727136135101318,0.42728638648986816\n",
      "CORRECT: true label=1, prob=0.5537800192832947,0.4462199807167053\n",
      "CORRECT: true label=1, prob=0.5810815095901489,0.4189184904098511\n",
      "CORRECT: true label=1, prob=0.5661988854408264,0.4338011145591736\n",
      "CORRECT: true label=1, prob=0.5534300208091736,0.4465699791908264\n",
      "CORRECT: true label=1, prob=0.5523048639297485,0.44769513607025146\n",
      "WRONG: true label=1, prob=0.4087566137313843,0.4087566137313843\n",
      "CORRECT: true label=1, prob=0.6138252019882202,0.3861747980117798\n",
      "CORRECT: true label=1, prob=0.5549855828285217,0.44501441717147827\n",
      "CORRECT: true label=1, prob=0.6027216911315918,0.3972783088684082\n",
      "CORRECT: true label=1, prob=0.5572082996368408,0.4427917003631592\n",
      "CORRECT: true label=1, prob=0.5526605844497681,0.44733941555023193\n",
      "CORRECT: true label=1, prob=0.5475782752037048,0.45242172479629517\n",
      "WRONG: true label=1, prob=0.46797090768814087,0.46797090768814087\n",
      "CORRECT: true label=1, prob=0.5163910388946533,0.4836089611053467\n",
      "CORRECT: true label=1, prob=0.5200047492980957,0.4799952507019043\n",
      "CORRECT: true label=1, prob=0.5101982355117798,0.4898017644882202\n",
      "CORRECT: true label=1, prob=0.5274796485900879,0.4725203514099121\n",
      "WRONG: true label=1, prob=0.49793219566345215,0.49793219566345215\n",
      "CORRECT: true label=1, prob=0.5600163340568542,0.43998366594314575\n",
      "CORRECT: true label=1, prob=0.6164052486419678,0.3835947513580322\n",
      "CORRECT: true label=1, prob=0.5748417377471924,0.4251582622528076\n",
      "CORRECT: true label=1, prob=0.5761957764625549,0.42380422353744507\n",
      "CORRECT: true label=1, prob=0.5894020199775696,0.4105979800224304\n",
      "CORRECT: true label=1, prob=0.6760154366493225,0.3239845633506775\n",
      "CORRECT: true label=1, prob=0.5547233819961548,0.4452766180038452\n",
      "CORRECT: true label=1, prob=0.5676907896995544,0.43230921030044556\n",
      "CORRECT: true label=1, prob=0.5048219561576843,0.4951780438423157\n",
      "CORRECT: true label=1, prob=0.5749371647834778,0.4250628352165222\n",
      "CORRECT: true label=1, prob=0.532913327217102,0.46708667278289795\n",
      "CORRECT: true label=1, prob=0.5496321320533752,0.45036786794662476\n",
      "CORRECT: true label=1, prob=0.5436400175094604,0.45635998249053955\n",
      "CORRECT: true label=1, prob=0.5903477072715759,0.4096522927284241\n",
      "WRONG: true label=1, prob=0.44942012429237366,0.44942012429237366\n",
      "CORRECT: true label=1, prob=0.554402232170105,0.445597767829895\n",
      "CORRECT: true label=1, prob=0.5914955139160156,0.4085044860839844\n",
      "CORRECT: true label=1, prob=0.539625883102417,0.460374116897583\n",
      "CORRECT: true label=1, prob=0.5350063443183899,0.4649936556816101\n",
      "CORRECT: true label=1, prob=0.5429665446281433,0.4570334553718567\n",
      "WRONG: true label=0, prob=0.5313245058059692,0.46867549419403076\n",
      "CORRECT: true label=1, prob=0.5877946019172668,0.41220539808273315\n",
      "WRONG: true label=1, prob=0.45379638671875,0.45379638671875\n",
      "CORRECT: true label=1, prob=0.57973712682724,0.42026287317276\n",
      "CORRECT: true label=1, prob=0.5873993635177612,0.41260063648223877\n",
      "CORRECT: true label=1, prob=0.547541618347168,0.45245838165283203\n",
      "CORRECT: true label=1, prob=0.6245737075805664,0.3754262924194336\n",
      "CORRECT: true label=1, prob=0.6322317719459534,0.36776822805404663\n",
      "CORRECT: true label=1, prob=0.5891631245613098,0.4108368754386902\n",
      "CORRECT: true label=1, prob=0.5422240495681763,0.45777595043182373\n",
      "CORRECT: true label=1, prob=0.5950039625167847,0.40499603748321533\n",
      "CORRECT: true label=1, prob=0.592377245426178,0.407622754573822\n",
      "CORRECT: true label=1, prob=0.6073281168937683,0.3926718831062317\n",
      "CORRECT: true label=1, prob=0.555899977684021,0.444100022315979\n",
      "CORRECT: true label=1, prob=0.5541937947273254,0.44580620527267456\n",
      "WRONG: true label=1, prob=0.48503339290618896,0.48503339290618896\n",
      "CORRECT: true label=1, prob=0.5491431355476379,0.45085686445236206\n",
      "CORRECT: true label=1, prob=0.5448874831199646,0.4551125168800354\n",
      "CORRECT: true label=1, prob=0.5886242389678955,0.4113757610321045\n",
      "CORRECT: true label=1, prob=0.5040175914764404,0.49598240852355957\n",
      "CORRECT: true label=1, prob=0.5670002102851868,0.43299978971481323\n",
      "CORRECT: true label=1, prob=0.5415312051773071,0.45846879482269287\n",
      "CORRECT: true label=1, prob=0.5084475874900818,0.4915524125099182\n",
      "CORRECT: true label=1, prob=0.511529266834259,0.48847073316574097\n",
      "CORRECT: true label=1, prob=0.5498210191726685,0.45017898082733154\n",
      "CORRECT: true label=1, prob=0.6256294250488281,0.3743705749511719\n",
      "CORRECT: true label=1, prob=0.5607505440711975,0.4392494559288025\n",
      "CORRECT: true label=1, prob=0.5848297476768494,0.41517025232315063\n",
      "CORRECT: true label=1, prob=0.5093878507614136,0.4906121492385864\n",
      "CORRECT: true label=1, prob=0.5963107347488403,0.40368926525115967\n",
      "CORRECT: true label=1, prob=0.5374189615249634,0.4625810384750366\n",
      "WRONG: true label=1, prob=0.45239147543907166,0.45239147543907166\n",
      "CORRECT: true label=1, prob=0.5558121800422668,0.44418781995773315\n",
      "CORRECT: true label=1, prob=0.5438162088394165,0.4561837911605835\n",
      "CORRECT: true label=1, prob=0.5214002132415771,0.47859978675842285\n",
      "CORRECT: true label=1, prob=0.5327957272529602,0.4672042727470398\n",
      "CORRECT: true label=1, prob=0.5411856770515442,0.4588143229484558\n",
      "WRONG: true label=1, prob=0.4302701950073242,0.4302701950073242\n",
      "CORRECT: true label=1, prob=0.5314070582389832,0.46859294176101685\n",
      "CORRECT: true label=1, prob=0.5852285623550415,0.4147714376449585\n",
      "WRONG: true label=1, prob=0.48424553871154785,0.48424553871154785\n",
      "CORRECT: true label=1, prob=0.5039964318275452,0.49600356817245483\n",
      "CORRECT: true label=1, prob=0.535659670829773,0.46434032917022705\n",
      "CORRECT: true label=1, prob=0.5549498796463013,0.44505012035369873\n",
      "CORRECT: true label=1, prob=0.5510980486869812,0.4489019513130188\n",
      "CORRECT: true label=1, prob=0.664612352848053,0.335387647151947\n",
      "CORRECT: true label=1, prob=0.5524235963821411,0.4475764036178589\n",
      "CORRECT: true label=1, prob=0.5286218523979187,0.4713781476020813\n",
      "CORRECT: true label=1, prob=0.558866024017334,0.441133975982666\n",
      "CORRECT: true label=1, prob=0.5766129493713379,0.4233870506286621\n",
      "CORRECT: true label=1, prob=0.5461110472679138,0.4538889527320862\n",
      "WRONG: true label=1, prob=0.4961918890476227,0.4961918890476227\n",
      "CORRECT: true label=1, prob=0.5160596966743469,0.4839403033256531\n",
      "WRONG: true label=1, prob=0.3142797350883484,0.3142797350883484\n"
     ]
    }
   ],
   "source": [
    "print(labels_dict,'\\n')\n",
    "test_wrong_indices = set(test_wrong_indices)\n",
    "y_test_preds_np = np.around(y_test_probs)\n",
    "wrong_probs_diffs = []\n",
    "correct_probs_diffs = []\n",
    "for i in range(len(y_test_probs)):\n",
    "    if i in test_wrong_indices:\n",
    "        wrong_probs_diffs.append(abs(y_test_preds_np[i]-y_test_probs[i]))\n",
    "        print(f'WRONG: true label={y_test_true[i]}, prob={y_test_probs[i]},{wrong_probs_diffs[-1]}')\n",
    "    else:\n",
    "        correct_probs_diffs.append(abs(y_test_true[i]-y_test_probs[i]))\n",
    "        print(f'CORRECT: true label={y_test_true[i]}, prob={y_test_probs[i]},{correct_probs_diffs[-1]}')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(wrong_probs_diffs, color='red', density=True)\n",
    "ax.set_title(\"histogram of differences between wrong labels and XGB model probs\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(correct_probs_diffs, color='green', density=True)\n",
    "ax.set_title(\"histogram of differences between correct labels and XGB model probs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL w/ gym-anytrading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_anytrading\n",
    "from gym_anytrading.envs.forex_env import ForexEnv\n",
    "\n",
    "from gym_anytrading.datasets import FOREX_EURUSD_1H_ASK\n",
    "\n",
    "from stable_baselines import A2C\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "\n",
    "import quantstats as qs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### trying out sample code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gym_anytrading.datasets.STOCKS_GOOGL.copy()\n",
    "df = df.drop(['Adj Close'], axis=1)\n",
    "\n",
    "window_size = 10\n",
    "start_index = window_size\n",
    "end_index = len(df)\n",
    "\n",
    "env_maker = lambda: gym.make(\n",
    "    'stocks-v0',\n",
    "    df = df,\n",
    "    window_size = window_size,\n",
    "    frame_bound = (start_index, end_index)\n",
    ")\n",
    "\n",
    "env = DummyVecEnv([env_maker])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i, end = no_missing_data_idx_range(data_with_ichi_2, early_ending_cols=['chikou_span_visual'])\n",
    "train_df = data_with_ichi_2.iloc[i:]\n",
    "train_df.set_index('datetime', inplace=True, verify_integrity=True)\n",
    "categories_dict = {\n",
    "    'quarter': [1,2,3,4],\n",
    "    'day_of_week': [0,1,2,3,4]\n",
    "}\n",
    "train_df = dummy_and_remove_data(train_df, categories_dict=categories_dict, cols_to_remove=['momentum_rsi','month','day','minute','hour','year','spread'],\n",
    "                                 include_defaults=False)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomForexEnv(ForexEnv):\n",
    "    def _process_data(self):\n",
    "        prices = self.df.loc[:, 'Close'].to_numpy()\n",
    "\n",
    "        prices[self.frame_bound[0] - self.window_size]  # validate index (TODO: Improve validation)\n",
    "        prices = prices[self.frame_bound[0]-self.window_size:self.frame_bound[1]]\n",
    "\n",
    "        diff = np.insert(np.diff(prices), 0, 0)\n",
    "        signal_features = np.column_stack((prices, diff))\n",
    "        \n",
    "        my_features = self.df.iloc[:,4:].to_numpy()\n",
    "        signal_features = np.column_stack((signal_features, my_features))\n",
    "#         print(list(signal_features[0]))\n",
    "\n",
    "        return prices, signal_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_df\n",
    "\n",
    "window_size = 10\n",
    "start_index = window_size\n",
    "end_index = len(df)\n",
    "\n",
    "env_maker = lambda: CustomForexEnv(\n",
    "    df = train_df,\n",
    "    window_size = window_size,\n",
    "    frame_bound = (start_index, end_index),\n",
    "    unit_side = 'right'\n",
    ")\n",
    "\n",
    "env = DummyVecEnv([env_maker])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "policy_kwargs = dict(net_arch=[64, 'lstm', dict(vf=[128, 128, 128], pi=[64, 64])])\n",
    "model = A2C('MlpLstmPolicy', env, verbose=1, policy_kwargs=policy_kwargs)\n",
    "model.learn(total_timesteps=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = env_maker()\n",
    "observation = env.reset()\n",
    "actions = []\n",
    "while True:\n",
    "    observation = observation[np.newaxis, ...]\n",
    "\n",
    "    # action = env.action_space.sample()\n",
    "    action, _states = model.predict(observation)\n",
    "    actions.append(action)\n",
    "    observation, reward, done, info = env.step(action)\n",
    "\n",
    "    # env.render()\n",
    "    if done:\n",
    "        print(\"info:\", info)\n",
    "        break\n",
    "print(observation)\n",
    "# for action in actions:\n",
    "#     print(action)\n",
    "# print(len(actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "plt.figure(figsize=(16, 6))\n",
    "env.render_all()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "qs.extend_pandas()\n",
    "\n",
    "net_worth = pd.Series(env.history['total_profit'], index=df.index[start_index+1:end_index])\n",
    "returns = net_worth.pct_change().iloc[1:]\n",
    "\n",
    "qs.reports.full(returns)\n",
    "qs.reports.html(returns, output='a2c_quantstats.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-12-28ceb4c7b649>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "is GPU available for TF: True\n",
      "\n",
      "GPU devices: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "\n",
      "all devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(f'is GPU available for TF: {tf.test.is_gpu_available()}\\n')\n",
    "\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "print(f'GPU devices: {gpu_devices}\\n')\n",
    "\n",
    "all_devices = tf.config.list_physical_devices()\n",
    "print(f'all devices: {all_devices}')\n",
    "\n",
    "if len(gpu_devices) > 0:\n",
    "    for device in gpu_devices: \n",
    "        tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 128\n",
    "num_epochs = 400\n",
    "fast_ma_window = 3\n",
    "slow_ma_window = 7\n",
    "tenkan_period = 9\n",
    "kijun_period = 30\n",
    "senkou_b_period = 60\n",
    "cur_pair = 'EURUSD'\n",
    "timeframe = 'H1'\n",
    "model_config = {\n",
    "    'current_model':'ichi_cloud',\n",
    "    'ichi_cloud':{\n",
    "        'indicators': {\n",
    "            'ichimoku': {\n",
    "                'tenkan_period': tenkan_period,\n",
    "                'kijun_period': kijun_period,\n",
    "                'chikou_period': kijun_period,\n",
    "                'senkou_b_period': senkou_b_period\n",
    "            },\n",
    "            'rsi': {\n",
    "                'periods': 14\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "ma_cols = ['Open','High','Low','Close','Volume']\n",
    "pc_cols = ['Open','High','Low','Close','Volume',]\n",
    "#            'trend_ichimoku_base','trend_ichimoku_conv',\n",
    "#            'trend_ichimoku_a', 'trend_ichimoku_b']\n",
    "normalization_groups = [['Open','High','Low','Close'],  # prices\n",
    "#                         ['trend_ichimoku_base','trend_ichimoku_conv'],  # ichi conv & base lines\n",
    "#                         ['trend_ichimoku_a', 'trend_ichimoku_b'], # ichi cloud lines\n",
    "                        ['tk_cross_bull_strength','tk_cross_bear_strength',   # tk cross strength\n",
    "                        'tk_price_cross_bull_strength','tk_price_cross_bear_strength',   # tk price cross strength\n",
    "                        'senkou_cross_bull_strength','senkou_cross_bear_strength',   # semkou cross strength\n",
    "                        'chikou_cross_bull_strength','chikou_cross_bear_strength']]   # chikou cross strength\n",
    "\n",
    "train_perc = 0.8\n",
    "val_perc = (1-train_perc)/2\n",
    "test_perc = val_perc\n",
    "split_percents = (val_perc, test_perc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get data and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tensor(data):\n",
    "    tensor = tf.convert_to_tensor(data, dtype=tf.float32)\n",
    "    return tensor\n",
    "\n",
    "def get_best_batch_size(data_len, min_bs, max_bs):\n",
    "    best_bs = 0\n",
    "    best_size = 0\n",
    "    for i in range(data_len):\n",
    "        bs = min_bs\n",
    "        cur_len = data_len - i\n",
    "        while cur_len % bs != 0 and bs < max_bs:\n",
    "            bs+=1\n",
    "        if cur_len % bs == 0:\n",
    "            best_size = cur_len\n",
    "            best_bs = bs\n",
    "            break\n",
    "    return best_bs, best_size\n",
    "\n",
    "def get_split_data_ma(preprocessed_data_df, ma_window, seq_len, split_percents=None, ma_cols=None, pc_cols=None, normalization_groups=None, \n",
    "                      min_batch_size=1000, max_batch_size=3500, just_train=False, print_info=True, fully_divisible_batch_sizes=False, batch_size=1024,\n",
    "                      buy_sell_labels_df=None, apply_pct_change=True):\n",
    "    feature_names = preprocessed_data_df.columns\n",
    "    if buy_sell_labels_df is not None:\n",
    "        if buy_sell_labels_df.shape[0] != preprocessed_data_df.shape[0]:\n",
    "            print(f'buy_sell_labels_df (shape={buy_sell_labels_df.shape}) does not have the same '\n",
    "                  f'number of rows as preprocessed_data_df (shape={preprocessed_data_df.shape})')\n",
    "            return\n",
    "        buy_sell_label_name = buy_sell_labels_df.name\n",
    "        preprocessed_data_df = pd.concat((preprocessed_data_df, buy_sell_labels_df), axis=1)  \n",
    "    \n",
    "    if not ma_cols:\n",
    "        ma_cols = ['Open','High','Low','Close','Volume']\n",
    "    \n",
    "    if not pc_cols:\n",
    "        pc_cols = ['Open','High','Low','Close','Volume',\n",
    "                   'trend_ichimoku_base','trend_ichimoku_conv',\n",
    "                   'trend_ichimoku_a', 'trend_ichimoku_b']\n",
    "        \n",
    "    if not normalization_groups:\n",
    "        normalization_groups = [['Open','High','Low','Close'],  # prices\n",
    "                                ['trend_ichimoku_base','trend_ichimoku_conv'],  # ichi conv & base lines\n",
    "                                ['trend_ichimoku_a', 'trend_ichimoku_b'], # ichi cloud lines\n",
    "                                ['tk_cross_bull_strength','tk_cross_bear_strength',   # tk cross strength\n",
    "                                'tk_price_cross_bull_strength','tk_price_cross_bear_strength',   # tk price cross strength\n",
    "                                'senkou_cross_bull_strength','senkou_cross_bear_strength',   # semkou cross strength\n",
    "                                'chikou_cross_bull_strength','chikou_cross_bear_strength']]   # chikou cross strength\n",
    "        \n",
    "    col_to_idx = {col_name: preprocessed_data_df.columns.get_loc(col_name) for col_name in preprocessed_data_df.columns}\n",
    "\n",
    "    # apply moving average to data to reduce bias (just learns centre of data) due to rugged raw price data that might look like a random walk to model\n",
    "    # (but since we are now predicting a mov avg of the price it is less representative of the outliers which are still important)\n",
    "    if ma_window is not None:\n",
    "        preprocessed_data_df = apply_moving_avg(preprocessed_data_df, cols=ma_cols, window=ma_window)\n",
    "        preprocessed_data_df.dropna(how='any', axis=0, inplace=True, subset=feature_names) # drop any NA rows due to applying moving average\n",
    "    \n",
    "    # apply percentage change to data to make it so data is more stationary (past data more related to future data) since \n",
    "    # the price data is typically strictly increasing or decreasing over the whole distribution\n",
    "    if apply_pct_change:\n",
    "        preprocessed_data_df = apply_perc_change(preprocessed_data_df, cols=pc_cols)\n",
    "        preprocessed_data_df.dropna(how='any', axis=0, inplace=True, subset=feature_names) # drop any NA rows due to applying percentage change\n",
    "    \n",
    "    if buy_sell_labels_df is not None:\n",
    "        buy_sell_labels_df = preprocessed_data_df[buy_sell_label_name]\n",
    "        preprocessed_data_df = preprocessed_data_df[feature_names]\n",
    "    \n",
    "    # normalize data for improved model training performance\n",
    "    all_train_df, all_train_normalization_terms = normalize_data(preprocessed_data_df, train_data=True, groups=normalization_groups)\n",
    "    \n",
    "    # all training data\n",
    "    all_train = all_train_df.to_numpy()\n",
    "    \n",
    "    if buy_sell_labels_df is not None:\n",
    "        decision_to_int = {'buy': 1, 'sell': 0}\n",
    "        buy_sell_labels = buy_sell_labels_df.to_numpy()\n",
    "    \n",
    "    all_x_train, all_y_train = [], []\n",
    "    for i in range(seq_len, len(all_train)):\n",
    "        if buy_sell_labels_df is None: \n",
    "            all_x_train.append(all_train[i-seq_len:i])\n",
    "            all_y_train.append(all_train[i][col_to_idx['Close']])\n",
    "        else:\n",
    "            decision = buy_sell_labels[i]\n",
    "            if decision is not None:\n",
    "                all_x_train.append(all_train[i-seq_len:i])\n",
    "                all_y_train.append(decision_to_int[decision])    \n",
    "    all_x_train, all_y_train = np.array(all_x_train), np.array(all_y_train)\n",
    "    \n",
    "    if fully_divisible_batch_sizes:\n",
    "        final_batch_size, final_train_data_size = get_best_batch_size(len(all_x_train), min_batch_size, max_batch_size)\n",
    "    else:\n",
    "        final_batch_size, final_train_data_size = batch_size, len(all_x_train)\n",
    "        \n",
    "    all_x_train_len_orig = len(all_x_train)\n",
    "    all_x_train, all_y_train = all_x_train[-final_train_data_size:], all_y_train[-final_train_data_size:]\n",
    "    \n",
    "    # split data\n",
    "    if not just_train:\n",
    "        if sum(split_percents) > 1:\n",
    "            print(f'sum of split_percents {split_percents} should not exceed 1')\n",
    "            return\n",
    "\n",
    "        len_data = preprocessed_data_df.shape[0]\n",
    "        # only need to pass validation and test split percentages and the rest will be used as training data\n",
    "        val_p, test_p = split_percents\n",
    "        # only need to define num rows for validation and test data split so that they remain constant w/ respect to the size of preprocessed_data_df\n",
    "        # so that when plotting data of diffrernt moving averages they line up consistatnly\n",
    "        val_len, test_len = int(len_data*val_p), int(len_data*test_p)\n",
    "\n",
    "        test_data_df = preprocessed_data_df.iloc[-test_len:] \n",
    "        val_data_df = preprocessed_data_df.iloc[-(val_len+test_len):-test_len]\n",
    "        train_data_df = preprocessed_data_df.iloc[:-(val_len+test_len)]\n",
    "        \n",
    "        if buy_sell_labels_df is not None:\n",
    "            test_buy_sell_labels = buy_sell_labels_df.iloc[-test_len:].to_numpy() \n",
    "            val_buy_sell_labels = buy_sell_labels_df.iloc[-(val_len+test_len):-test_len].to_numpy()\n",
    "            train_buy_sell_labels = buy_sell_labels_df.iloc[:-(val_len+test_len)].to_numpy()\n",
    "        \n",
    "        train_data_df, normalization_terms = normalize_data(train_data_df, train_data=True, groups=normalization_groups)   \n",
    "        val_data_df = normalize_data(val_data_df, train_data=False, normalization_terms=normalization_terms)[0]\n",
    "        test_data_df = normalize_data(test_data_df, train_data=False, normalization_terms=normalization_terms)[0]\n",
    "        \n",
    "        train_data = train_data_df.to_numpy()\n",
    "        val_data = val_data_df.to_numpy()\n",
    "        test_data = test_data_df.to_numpy()\n",
    "        \n",
    "        # training data\n",
    "        x_train, y_train = [], []  \n",
    "        for i in range(seq_len, len(train_data)):\n",
    "            if buy_sell_labels_df is None: \n",
    "                x_train.append(train_data[i-seq_len:i])\n",
    "                y_train.append(train_data[i][col_to_idx['Close']])\n",
    "            else:\n",
    "                decision = train_buy_sell_labels[i]\n",
    "                if decision is not None:\n",
    "                    x_train.append(train_data[i-seq_len:i])\n",
    "                    y_train.append(decision_to_int[decision])\n",
    "        x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "\n",
    "        # validation data\n",
    "        x_val, y_val = [], []\n",
    "        for i in range(seq_len, len(val_data)):\n",
    "            if buy_sell_labels_df is None: \n",
    "                x_val.append(val_data[i-seq_len:i])\n",
    "                y_val.append(val_data[i][col_to_idx['Close']])\n",
    "            else:\n",
    "                decision = val_buy_sell_labels[i]\n",
    "                if decision is not None:\n",
    "                    x_val.append(val_data[i-seq_len:i])\n",
    "                    y_val.append(decision_to_int[decision])\n",
    "        x_val, y_val = np.array(x_val), np.array(y_val)\n",
    "\n",
    "        # test data\n",
    "        x_test, y_test = [], []\n",
    "        for i in range(seq_len, len(test_data)):\n",
    "            if buy_sell_labels_df is None: \n",
    "                x_test.append(test_data[i-seq_len:i])\n",
    "                y_test.append(test_data[i][col_to_idx['Close']])\n",
    "            else:\n",
    "                decision = test_buy_sell_labels[i]\n",
    "                if decision is not None:\n",
    "                    x_test.append(test_data[i-seq_len:i])\n",
    "                    y_test.append(decision_to_int[decision])\n",
    "        x_test, y_test = np.array(x_test), np.array(y_test)\n",
    "        \n",
    "        if fully_divisible_batch_sizes:\n",
    "            eval_batch_size, eval_train_data_size = get_best_batch_size(len(x_train), min_batch_size, max_batch_size)\n",
    "        else:\n",
    "            eval_batch_size, eval_train_data_size = batch_size, len(x_train)\n",
    "\n",
    "        x_train_len_orig = len(x_train)\n",
    "        x_train, y_train = x_train[-eval_train_data_size:], y_train[-eval_train_data_size:]\n",
    "        \n",
    "        if print_info:\n",
    "            print('------------------------------------------------------')\n",
    "            print(f'data w/ moving average window of {ma_window} info:\\n')\n",
    "            print(f'batch size for evaluation: {eval_batch_size}')\n",
    "            print(f'training data size reduction for evaulation: {x_train_len_orig} -> {eval_train_data_size}')\n",
    "            print(f'batch size for final training: {final_batch_size}')\n",
    "            print(f'training data size reduction for final training: {all_x_train_len_orig} -> {final_train_data_size}\\n')\n",
    "            print(f'training data shape: x={x_train.shape}, y={y_train.shape}')\n",
    "            print(f'validation data shape: x={x_val.shape}, y={y_val.shape}')\n",
    "            print(f'test data shape: x={x_test.shape}, y={y_test.shape}')\n",
    "            print(f'all train data shape: x={all_x_train.shape}, y={all_y_train.shape}')\n",
    "            print('------------------------------------------------------')\n",
    "\n",
    "        data_dict = {\n",
    "            'ma_window': ma_window,\n",
    "            'eval_batch_size': eval_batch_size,\n",
    "            'final_batch_size': final_batch_size,\n",
    "            'train_data_df': train_data_df,\n",
    "            'val_data_df': val_data_df,\n",
    "            'test_data_df': test_data_df,\n",
    "            'all_train_df': all_train_df,\n",
    "            'train_data_np': (x_train, y_train),\n",
    "            'val_data_np': (x_val, y_val),\n",
    "            'test_data_np': (x_test, y_test),\n",
    "            'all_train_data_np': (all_x_train, all_y_train),\n",
    "            'all_train_normalization_terms': all_train_normalization_terms\n",
    "        }\n",
    "    else:\n",
    "        if print_info:\n",
    "            print('------------------------------------------------------')\n",
    "            print(f'data w/ moving average window of {ma_window} info:\\n')\n",
    "            print(f'batch size for final training: {final_batch_size}')\n",
    "            print(f'training data size reduction for final training: {all_x_train_len_orig} -> {final_train_data_size}\\n')\n",
    "            print(f'all train data shape: x={all_x_train.shape}, y={all_y_train.shape}')\n",
    "            print('------------------------------------------------------')\n",
    "\n",
    "        data_dict = {\n",
    "            'ma_window': ma_window,\n",
    "            'final_batch_size': final_batch_size,\n",
    "            'all_train_df': all_train_df,\n",
    "            'all_train_data_np': (all_x_train, all_y_train),\n",
    "            'all_train_normalization_terms': all_train_normalization_terms\n",
    "        }\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tick_data_filepath = gi.download_mt5_data(cur_pair, timeframe, '2011-01-01', '2020-10-01')\n",
    "data_with_indicators = gi.add_indicators_to_raw(filepath=tick_data_filepath,\n",
    "                                                save_to_disk=True, \n",
    "                                                config=model_config, \n",
    "                                                has_headers=True,\n",
    "                                                datetime_col='datetime')\n",
    "data_with_ichi_sigs = add_features(data_with_indicators)\n",
    "\n",
    "all_data = missing_labels_preprocess(data_with_ichi_sigs,None,None)[0]\n",
    "all_data_orig = all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_ma_data = get_split_data_ma(all_data, ma_window=fast_ma_window, seq_len=seq_len, split_percents=split_percents, fully_divisible_batch_sizes=True,\n",
    "                                 normalization_groups=normalization_groups, pc_cols=pc_cols, ma_cols=ma_cols, min_batch_size=1000, max_batch_size=2000)\n",
    "slow_ma_data = get_split_data_ma(all_data, ma_window=slow_ma_window, seq_len=seq_len, split_percents=split_percents, fully_divisible_batch_sizes=True,\n",
    "                                 normalization_groups=normalization_groups, pc_cols=pc_cols, ma_cols=ma_cols, min_batch_size=1000, max_batch_size=2000)\n",
    "\n",
    "x_train_fast_ma, y_train_fast_ma = fast_ma_data['train_data_np']\n",
    "x_val_fast_ma, y_val_fast_ma = fast_ma_data['val_data_np']\n",
    "x_test_fast_ma, y_test_fast_ma = fast_ma_data['test_data_np']\n",
    "\n",
    "x_train_slow_ma, y_train_slow_ma = slow_ma_data['train_data_np']\n",
    "x_val_slow_ma, y_val_slow_ma = slow_ma_data['val_data_np']\n",
    "x_test_slow_ma, y_test_slow_ma = slow_ma_data['test_data_np']\n",
    "\n",
    "# process orignal price data for plotting comparison\n",
    "\n",
    "all_data_orig = apply_perc_change(all_data_orig, cols=pc_cols)\n",
    "all_data_orig.dropna(how='any', axis=0, inplace=True) # drop any NA rows due to applying percentage change\n",
    "\n",
    "train_data_df_orig = all_data_orig.iloc[:fast_ma_data['train_data_df'].index[-1]+1]\n",
    "val_data_df_orig = all_data_orig.iloc[fast_ma_data['train_data_df'].index[-1]+1:fast_ma_data['val_data_df'].index[-1]+1]\n",
    "test_data_df_orig = all_data_orig.iloc[fast_ma_data['val_data_df'].index[-1]+1:]\n",
    "\n",
    "train_data_df_orig, normalization_terms_2 = normalize_data(train_data_df_orig, train_data=True, groups=normalization_groups)   \n",
    "val_data_df_orig, normalization_terms_2 = normalize_data(val_data_df_orig, train_data=False, normalization_terms=normalization_terms_2)\n",
    "test_data_df_orig = normalize_data(test_data_df_orig, train_data=False, normalization_terms=normalization_terms_2)[0]\n",
    "\n",
    "train_data_orig = train_data_df_orig.to_numpy()\n",
    "val_data_orig = val_data_df_orig.to_numpy()\n",
    "test_data_orig = test_data_df_orig.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tup in ((fast_ma_data, 'Fast MA'), (slow_ma_data, 'Slow MA')):\n",
    "    data, marker = tup\n",
    "    \n",
    "    train_data_df = data['train_data_df']\n",
    "    val_data_df = data['val_data_df']\n",
    "    test_data_df = data['test_data_df']\n",
    "\n",
    "    train_data = train_data_df.to_numpy()\n",
    "    val_data = val_data_df.to_numpy()\n",
    "    test_data = test_data_df.to_numpy()\n",
    "\n",
    "    fig = plt.figure(figsize=(15,10))\n",
    "    st = fig.suptitle(f'{marker} Data Separation', fontsize=20)\n",
    "    st.set_y(0.92)\n",
    "\n",
    "    ###############################################################################\n",
    "\n",
    "    ax1 = fig.add_subplot(211)\n",
    "    ax1.plot(np.arange(train_data.shape[0]), train_data_df['Close'], label='Training data')\n",
    "\n",
    "    ax1.plot(np.arange(train_data.shape[0], \n",
    "                       train_data.shape[0]+val_data.shape[0]), val_data_df['Close'], label='Validation data')\n",
    "\n",
    "    ax1.plot(np.arange(train_data.shape[0]+val_data.shape[0], \n",
    "                       train_data.shape[0]+val_data.shape[0]+test_data.shape[0]), test_data_df['Close'], label='Test data')\n",
    "    ax1.set_xlabel('Date')\n",
    "    ax1.set_ylabel(f'{marker} Normalized Closing Returns')\n",
    "\n",
    "    ###############################################################################\n",
    "\n",
    "    ax2 = fig.add_subplot(212)\n",
    "    ax2.plot(np.arange(train_data.shape[0]), train_data_df['Volume'], label='Training data')\n",
    "\n",
    "    ax2.plot(np.arange(train_data.shape[0], \n",
    "                       train_data.shape[0]+val_data.shape[0]), val_data_df['Volume'], label='Validation data')\n",
    "\n",
    "    ax2.plot(np.arange(train_data.shape[0]+val_data.shape[0], \n",
    "                       train_data.shape[0]+val_data.shape[0]+test_data.shape[0]), test_data_df['Volume'], label='Test data')\n",
    "    ax2.set_xlabel('Date')\n",
    "    ax2.set_ylabel(f'{marker} Normalized Volume Changes')\n",
    "\n",
    "    plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Inception_A(layer_in, c7):\n",
    "    branch1x1_1 = layers.Conv1D(c7, kernel_size=1, padding=\"same\", use_bias=False)(layer_in)\n",
    "    branch1x1 = layers.BatchNormalization()(branch1x1_1)\n",
    "    branch1x1 = layers.ReLU()(branch1x1)\n",
    "\n",
    "    branch5x5_1 = layers.Conv1D(c7, kernel_size=1, padding='same', use_bias=False)(layer_in)\n",
    "    branch5x5 = layers.BatchNormalization()(branch5x5_1)\n",
    "    branch5x5 = layers.ReLU()(branch5x5)\n",
    "    branch5x5 = layers.Conv1D(c7, kernel_size=5, padding='same', use_bias=False)(branch5x5)\n",
    "    branch5x5 = layers.BatchNormalization()(branch5x5)\n",
    "    branch5x5 = layers.ReLU()(branch5x5)  \n",
    "\n",
    "    branch3x3_1 = layers.Conv1D(c7, kernel_size=1, padding='same', use_bias=False)(layer_in)\n",
    "    branch3x3 = layers.BatchNormalization()(branch3x3_1)\n",
    "    branch3x3 = layers.ReLU()(branch3x3)\n",
    "    branch3x3 = layers.Conv1D(c7, kernel_size=3, padding='same', use_bias=False)(branch3x3)\n",
    "    branch3x3 = layers.BatchNormalization()(branch3x3)\n",
    "    branch3x3 = layers.ReLU()(branch3x3)\n",
    "    branch3x3 = layers.Conv1D(c7, kernel_size=3, padding='same', use_bias=False)(branch3x3)\n",
    "    branch3x3 = layers.BatchNormalization()(branch3x3)\n",
    "    branch3x3 = layers.ReLU()(branch3x3) \n",
    "\n",
    "    branch_pool = layers.AveragePooling1D(pool_size=(3), strides=1, padding='same')(layer_in)\n",
    "    branch_pool = layers.Conv1D(c7, kernel_size=1, padding='same', use_bias=False)(branch_pool)\n",
    "    branch_pool = layers.BatchNormalization()(branch_pool)\n",
    "    branch_pool = layers.ReLU()(branch_pool)\n",
    "    outputs = layers.Concatenate(axis=-1)([branch1x1, branch5x5, branch3x3, branch_pool])\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def Inception_B(layer_in, c7):\n",
    "    branch3x3 = layers.Conv1D(c7, kernel_size=3, padding=\"same\", strides=2, use_bias=False)(layer_in)\n",
    "    branch3x3 = layers.BatchNormalization()(branch3x3)\n",
    "    branch3x3 = layers.ReLU()(branch3x3)  \n",
    "\n",
    "    branch3x3dbl = layers.Conv1D(c7, kernel_size=1, padding=\"same\", use_bias=False)(layer_in)\n",
    "    branch3x3dbl = layers.BatchNormalization()(branch3x3dbl)\n",
    "    branch3x3dbl = layers.ReLU()(branch3x3dbl)  \n",
    "    branch3x3dbl = layers.Conv1D(c7, kernel_size=3, padding=\"same\", use_bias=False)(branch3x3dbl)  \n",
    "    branch3x3dbl = layers.BatchNormalization()(branch3x3dbl)\n",
    "    branch3x3dbl = layers.ReLU()(branch3x3dbl)  \n",
    "    branch3x3dbl = layers.Conv1D(c7, kernel_size=3, padding=\"same\", strides=2, use_bias=False)(branch3x3dbl)    \n",
    "    branch3x3dbl = layers.BatchNormalization()(branch3x3dbl)\n",
    "    branch3x3dbl = layers.ReLU()(branch3x3dbl)   \n",
    "\n",
    "    branch_pool = layers.MaxPooling1D(pool_size=3, strides=2, padding=\"same\")(layer_in)\n",
    "\n",
    "    outputs = layers.Concatenate(axis=-1)([branch3x3, branch3x3dbl, branch_pool])\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def Inception_C(layer_in, c7):\n",
    "    branch1x1_1 = layers.Conv1D(c7, kernel_size=1, padding=\"same\", use_bias=False)(layer_in)\n",
    "    branch1x1 = layers.BatchNormalization()(branch1x1_1)\n",
    "    branch1x1 = layers.ReLU()(branch1x1)   \n",
    "\n",
    "    branch7x7_1 = layers.Conv1D(c7, kernel_size=1, padding=\"same\", use_bias=False)(layer_in)\n",
    "    branch7x7 = layers.BatchNormalization()(branch7x7_1)\n",
    "    branch7x7 = layers.ReLU()(branch7x7)   \n",
    "    branch7x7 = layers.Conv1D(c7, kernel_size=(7), padding=\"same\", use_bias=False)(branch7x7)\n",
    "    branch7x7 = layers.BatchNormalization()(branch7x7)\n",
    "    branch7x7 = layers.ReLU()(branch7x7)  \n",
    "    branch7x7 = layers.Conv1D(c7, kernel_size=(1), padding=\"same\", use_bias=False)(branch7x7)  \n",
    "    branch7x7 = layers.BatchNormalization()(branch7x7)\n",
    "    branch7x7 = layers.ReLU()(branch7x7)   \n",
    "\n",
    "    branch7x7dbl_1 = layers.Conv1D(c7, kernel_size=1, padding=\"same\", use_bias=False)(layer_in)  \n",
    "    branch7x7dbl = layers.BatchNormalization()(branch7x7dbl_1)\n",
    "    branch7x7dbl = layers.ReLU()(branch7x7dbl)  \n",
    "    branch7x7dbl = layers.Conv1D(c7, kernel_size=(7), padding=\"same\", use_bias=False)(branch7x7dbl)  \n",
    "    branch7x7dbl = layers.BatchNormalization()(branch7x7dbl)\n",
    "    branch7x7dbl = layers.ReLU()(branch7x7dbl) \n",
    "    branch7x7dbl = layers.Conv1D(c7, kernel_size=(1), padding=\"same\", use_bias=False)(branch7x7dbl)  \n",
    "    branch7x7dbl = layers.BatchNormalization()(branch7x7dbl)\n",
    "    branch7x7dbl = layers.ReLU()(branch7x7dbl)  \n",
    "    branch7x7dbl = layers.Conv1D(c7, kernel_size=(7), padding=\"same\", use_bias=False)(branch7x7dbl)  \n",
    "    branch7x7dbl = layers.BatchNormalization()(branch7x7dbl)\n",
    "    branch7x7dbl = layers.ReLU()(branch7x7dbl)  \n",
    "    branch7x7dbl = layers.Conv1D(c7, kernel_size=(1), padding=\"same\", use_bias=False)(branch7x7dbl)  \n",
    "    branch7x7dbl = layers.BatchNormalization()(branch7x7dbl)\n",
    "    branch7x7dbl = layers.ReLU()(branch7x7dbl)  \n",
    "\n",
    "    branch_pool = layers.AveragePooling1D(pool_size=3, strides=1, padding='same')(layer_in)\n",
    "    branch_pool = layers.Conv1D(c7, kernel_size=1, padding='same', use_bias=False)(branch_pool)\n",
    "    branch_pool = layers.BatchNormalization()(branch_pool)\n",
    "    branch_pool = layers.ReLU()(branch_pool)  \n",
    "\n",
    "    outputs = layers.Concatenate(axis=-1)([branch1x1, branch7x7, branch7x7dbl, branch_pool])\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def create_model(seq_len, num_features):\n",
    "    in_seq = layers.Input(shape=(seq_len, num_features))\n",
    "\n",
    "    x = Inception_A(in_seq, 32)\n",
    "    x = Inception_A(x, 32)\n",
    "    x = Inception_B(x, 32)\n",
    "    x = Inception_B(x, 32)\n",
    "    x = Inception_C(x, 32)\n",
    "    x = Inception_C(x, 32)    \n",
    "\n",
    "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x) \n",
    "\n",
    "    avg_pool = layers.GlobalAveragePooling1D()(x)\n",
    "    max_pool = layers.GlobalMaxPooling1D()(x)\n",
    "    conc = layers.concatenate([avg_pool, max_pool])\n",
    "    conc = layers.Dense(64, activation=\"relu\")(conc)\n",
    "    out = layers.Dense(1, activation=\"sigmoid\")(conc)      \n",
    "\n",
    "    model = keras.Model(inputs=in_seq, outputs=out)\n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\", metrics=['mae', 'mape'])     \n",
    "    return model\n",
    "\n",
    "# def create_model(seq_len, num_features):\n",
    "#     in_seq = layers.Input(shape = (seq_len, num_features))\n",
    "\n",
    "#     x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(in_seq)\n",
    "#     x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(x)\n",
    "#     x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x) \n",
    "\n",
    "#     avg_pool = layers.GlobalAveragePooling1D()(x)\n",
    "#     max_pool = layers.GlobalMaxPooling1D()(x)\n",
    "#     conc = layers.concatenate([avg_pool, max_pool])\n",
    "#     conc = layers.Dense(64, activation=\"relu\")(conc)\n",
    "#     out = layers.Dense(1, activation=\"linear\")(conc)      \n",
    "\n",
    "#     model = keras.Model(inputs=in_seq, outputs=out)\n",
    "#     model.compile(loss=\"mse\", optimizer=\"adam\", metrics=['mae', 'mape'])    \n",
    "#     return model\n",
    "\n",
    "def create_model_binary(seq_len, num_features):\n",
    "#     in_seq = layers.Input(shape=(seq_len, num_features))\n",
    "\n",
    "#     x = Inception_A(in_seq, 32)\n",
    "#     x = Inception_A(x, 32)\n",
    "#     x = Inception_B(x, 32)\n",
    "#     x = Inception_B(x, 32)\n",
    "#     x = Inception_C(x, 32)\n",
    "#     x = Inception_C(x, 32)    \n",
    "\n",
    "#     x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(x)\n",
    "#     x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(x)\n",
    "#     x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x) \n",
    "\n",
    "#     avg_pool = layers.GlobalAveragePooling1D()(x)\n",
    "#     max_pool = layers.GlobalMaxPooling1D()(x)\n",
    "#     conc = layers.concatenate([avg_pool, max_pool])\n",
    "#     conc = layers.Dense(64, activation=\"relu\")(conc)\n",
    "#     out = layers.Dense(1, activation=\"sigmoid\")(conc)      \n",
    "\n",
    "#     model = keras.Model(inputs=in_seq, outputs=out)\n",
    "#     model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy', 'AUC']) \n",
    "\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(seq_len, num_features)))\n",
    "    model.add(layers.Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "    model.add(layers.MaxPooling1D(pool_size=2))\n",
    "    model.add(layers.LSTM(100))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### try using model as buy/sell classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "label_non_signals = False\n",
    "min_profit_percent, profit_noise_percent = 0.01, 0.01\n",
    "contract_size = 100_000   # size of 1 lot is typically 100,000 (100 for gold, becuase 1 lot = 100 oz of gold)\n",
    "lots_per_trade = 0.2  \n",
    "in_quote_currency = True\n",
    "pip_resolution = 0.0001\n",
    "\n",
    "labels_dict = {1: 'buy', 0: 'sell'}\n",
    "n_estimators = 3000\n",
    "max_depth = 2\n",
    "learning_rate = 0.1\n",
    "subsample = 1\n",
    "colsample_bytree = 1\n",
    "gamma = 0\n",
    "num_class = 3 # buy, sell, wait\n",
    "tenkan_period = 8\n",
    "kijun_period = 22\n",
    "senkou_b_period = 44\n",
    "model_config = {\n",
    "    'current_model':'ichi_cloud',\n",
    "    'ichi_cloud':{\n",
    "        'indicators': {\n",
    "            'ichimoku': {\n",
    "                'tenkan_period': tenkan_period,\n",
    "                'kijun_period': kijun_period,\n",
    "                'chikou_period': kijun_period,\n",
    "                'senkou_b_period': senkou_b_period\n",
    "            },\n",
    "            'rsi': {\n",
    "                'periods': 14\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "signals_to_consider = ['cloud_breakout_bull','cloud_breakout_bear',                       # cloud breakout\n",
    "                       'tk_cross_bull_strength', 'tk_cross_bear_strength',                # Tenkan Sen / Kijun Sen Cross\n",
    "                       'tk_price_cross_bull_strength', 'tk_price_cross_bear_strength',    # price crossing both the Tenkan Sen / Kijun Sen\n",
    "                       'senkou_cross_bull_strength', 'senkou_cross_bear_strength',        # Senkou Span Cross\n",
    "                       'chikou_cross_bull_strength', 'chikou_cross_bear_strength']        # Chikou Span Cross\n",
    "sigs_for_filename = ['cb-tk-tkp-sen-chi']\n",
    "\n",
    "# get data\n",
    "\n",
    "cur_pair = 'EURUSD'\n",
    "timeframe = 'H1'\n",
    "tick_data_filepath = gi.download_mt5_data(cur_pair, timeframe, '2011-01-01', '2020-10-01')\n",
    "data_with_indicators = gi.add_indicators_to_raw(filepath=tick_data_filepath, \n",
    "                                                save_to_disk=True, \n",
    "                                                config=model_config, \n",
    "                                                has_headers=True,\n",
    "                                                datetime_col='datetime')\n",
    "train_data = add_features(data_with_indicators)\n",
    "\n",
    "train_data_labels = generate_ichimoku_labels(train_data, label_non_signals=label_non_signals, min_profit_percent=min_profit_percent, \n",
    "                                             profit_noise_percent=profit_noise_percent, signals_to_consider=signals_to_consider, \n",
    "                                             contract_size=contract_size, lots_per_trade=lots_per_trade,\n",
    "                                             in_quote_currency=in_quote_currency,pip_resolution=pip_resolution)\n",
    "\n",
    "start_idx, end_idx = no_missing_data_idx_range(train_data, early_ending_cols=['chikou_span_visual'])\n",
    "train_data = train_data.iloc[start_idx:end_idx+1]\n",
    "train_data = dummy_and_remove_features(train_data)\n",
    "train_data_labels = train_data_labels.iloc[start_idx:end_idx+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_dict = get_split_data_ma(train_data, ma_window=None, seq_len=128, split_percents=split_percents, fully_divisible_batch_sizes=True,\n",
    "                              normalization_groups=normalization_groups, pc_cols=pc_cols, ma_cols=ma_cols, min_batch_size=1000, max_batch_size=2000,\n",
    "                              buy_sell_labels_df=train_data_labels['first_decision'], apply_pct_change=False)\n",
    "\n",
    "x_train, y_train = data_dict['train_data_np']\n",
    "x_val, y_val = data_dict['val_data_np']\n",
    "x_test, y_test = data_dict['test_data_np']\n",
    "\n",
    "binary_model = create_model_binary(seq_len=x_train.shape[1], num_features=x_train.shape[2])\n",
    "    \n",
    "filepath = f'../my_stuff/test_model.hdf5'\n",
    "callback = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "                                    \n",
    "binary_model.fit(convert_to_tensor(x_train), convert_to_tensor(y_train),\n",
    "                  batch_size=data_dict['eval_batch_size'],\n",
    "                  callbacks=[callback],\n",
    "                  epochs=num_epochs,\n",
    "                  validation_data=(convert_to_tensor(x_val), convert_to_tensor(y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### try using model for close price forcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fast_ma_window = fast_ma_data['ma_window']\n",
    "filepath = f'../my_stuff/{cur_pair}-{timeframe}_Bi-LSTM_{fast_ma_window}-ma_{tenkan_period}-{kijun_period}-{senkou_b_period}-ichi.hdf5'\n",
    "callback = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "\n",
    "fast_ma_model = create_model(seq_len=x_train_fast_ma.shape[1], num_features=x_train_fast_ma.shape[2])\n",
    "# print(fast_ma_model.summary())\n",
    "\n",
    "start_t = time.time()\n",
    "\n",
    "fast_ma_model.fit(convert_to_tensor(x_train_fast_ma), convert_to_tensor(y_train_fast_ma),\n",
    "                  batch_size=fast_ma_data['eval_batch_size'],\n",
    "                  callbacks=[callback],\n",
    "                  epochs=num_epochs,\n",
    "                  validation_data=(convert_to_tensor(x_val_fast_ma), convert_to_tensor(y_val_fast_ma)))\n",
    "\n",
    "print(f'training time = {(time.time()-start_t)/60} min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slow_ma_window = slow_ma_data['ma_window']\n",
    "filepath = f'../my_stuff/{cur_pair}-{timeframe}_Bi-LSTM_{slow_ma_window}-ma_{tenkan_period}-{kijun_period}-{senkou_b_period}-ichi.hdf5'\n",
    "callback = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "\n",
    "slow_ma_model = create_model(seq_len=x_train_slow_ma.shape[1], num_features=x_train_slow_ma.shape[2])\n",
    "\n",
    "start_t = time.time()\n",
    "\n",
    "slow_ma_model.fit(convert_to_tensor(x_train_slow_ma), convert_to_tensor(y_train_slow_ma),\n",
    "                  batch_size=slow_ma_data['eval_batch_size'],\n",
    "                  callbacks=[callback],\n",
    "                  epochs=num_epochs,\n",
    "                  #shuffle=True,\n",
    "                  validation_data=(convert_to_tensor(x_val_slow_ma), convert_to_tensor(y_val_slow_ma)))\n",
    "\n",
    "print(f'training time = {(time.time()-start_t)/60} min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_ma_model = tf.keras.models.load_model('../my_stuff/final_Bi-LSTM_fast_5_ma.hdf5')#('../my_stuff/Bi-LSTM_mov_avg_5.hdf5')\n",
    "\n",
    "print('done loading fast ma model')\n",
    "\n",
    "# #Calculate predication for training, validation and test data\n",
    "# train_pred_fast_ma = fast_ma_model.predict(convert_to_tensor(x_train_fast_ma))\n",
    "# val_pred_fast_ma = fast_ma_model.predict(convert_to_tensor(x_val_fast_ma))\n",
    "# test_pred_fast_ma = fast_ma_model.predict(convert_to_tensor(x_test_fast_ma))\n",
    "\n",
    "#Print evaluation metrics for all datasets\n",
    "train_eval_fast_ma = fast_ma_model.evaluate(convert_to_tensor(x_train_fast_ma), convert_to_tensor(y_train_fast_ma), verbose=0)\n",
    "val_eval_fast_ma = fast_ma_model.evaluate(convert_to_tensor(x_val_fast_ma), convert_to_tensor(y_val_fast_ma), verbose=0)\n",
    "test_eval_fast_ma = fast_ma_model.evaluate(convert_to_tensor(x_test_fast_ma), convert_to_tensor(y_test_fast_ma), verbose=0)\n",
    "\n",
    "print('Evaluation metrics')\n",
    "print('Training Data - Loss: {:.4f}, MAE: {:.4f}, MAPE: {:.4f}'.format(train_eval_fast_ma[0], train_eval_fast_ma[1], train_eval_fast_ma[2]))\n",
    "print('Validation Data - Loss: {:.4f}, MAE: {:.4f}, MAPE: {:.4f}'.format(val_eval_fast_ma[0], val_eval_fast_ma[1], val_eval_fast_ma[2]))\n",
    "print('Test Data - Loss: {:.4f}, MAE: {:.4f}, MAPE: {:.4f}'.format(test_eval_fast_ma[0], test_eval_fast_ma[1], test_eval_fast_ma[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slow_ma_model = tf.keras.models.load_model('../my_stuff/final_Bi-LSTM_slow_13_ma.hdf5')#('../my_stuff/Bi-LSTM_mov_avg_13.hdf5')\n",
    "\n",
    "print('done loading slow ma model')\n",
    "\n",
    "#Calculate predication for training, validation and test data\n",
    "train_pred_slow_ma = slow_ma_model.predict(convert_to_tensor(x_train_slow_ma))\n",
    "val_pred_slow_ma = slow_ma_model.predict(convert_to_tensor(x_val_slow_ma))\n",
    "test_pred_slow_ma = slow_ma_model.predict(convert_to_tensor(x_test_slow_ma))\n",
    "\n",
    "#Print evaluation metrics for all datasets\n",
    "train_eval_slow_ma = slow_ma_model.evaluate(convert_to_tensor(x_train_slow_ma), convert_to_tensor(y_train_slow_ma), verbose=0)\n",
    "val_eval_slow_ma = slow_ma_model.evaluate(convert_to_tensor(x_val_slow_ma), convert_to_tensor(y_val_slow_ma), verbose=0)\n",
    "test_eval_slow_ma = slow_ma_model.evaluate(convert_to_tensor(x_test_slow_ma), convert_to_tensor(y_test_slow_ma), verbose=0)\n",
    "\n",
    "print('Evaluation metrics')\n",
    "print('Training Data - Loss: {:.4f}, MAE: {:.4f}, MAPE: {:.4f}'.format(train_eval_slow_ma[0], train_eval_slow_ma[1], train_eval_slow_ma[2]))\n",
    "print('Validation Data - Loss: {:.4f}, MAE: {:.4f}, MAPE: {:.4f}'.format(val_eval_slow_ma[0], val_eval_slow_ma[1], val_eval_slow_ma[2]))\n",
    "print('Test Data - Loss: {:.4f}, MAE: {:.4f}, MAPE: {:.4f}'.format(test_eval_slow_ma[0], test_eval_slow_ma[1], test_eval_slow_ma[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_fast_ma = fast_ma_data['test_data_df'].to_numpy()\n",
    "\n",
    "test_data_slow_ma = slow_ma_data['test_data_df'].to_numpy()\n",
    "\n",
    "fig = plt.figure(figsize=(15,15))\n",
    "st = fig.suptitle(\"CNN + Bi-LSTM Model\", fontsize=22)\n",
    "st.set_y(1.02)\n",
    "\n",
    "# #Plot training data results\n",
    "# ax11 = fig.add_subplot(311)\n",
    "# ax11.plot(train_data[seq_len:, 3], label='EURUSD Closing Returns')\n",
    "# ax11.plot(train_pred, color='yellow', linewidth=3, label='Predicted EURUSD Closing Returns')\n",
    "# ax11.set_title(\"Training Data\", fontsize=18)\n",
    "# ax11.set_xlabel('Date')\n",
    "# ax11.set_ylabel('EURUSD Closing Returns')\n",
    "\n",
    "# #Plot validation data results\n",
    "# ax21 = fig.add_subplot(312)\n",
    "# ax21.plot(val_data[seq_len:, 3], label='EURUSD Closing Returns')\n",
    "# ax21.plot(val_pred, color='yellow', linewidth=3, label='Predicted EURUSD Closing Returns')\n",
    "# ax21.set_title(\"Validation Data\", fontsize=18)\n",
    "# ax21.set_xlabel('Date')\n",
    "# ax21.set_ylabel('EURUSD Closing Returns')\n",
    "\n",
    "#Plot test data results\n",
    "ax31 = fig.add_subplot(111)\n",
    "ax31.plot(test_data_fast_ma[seq_len:, 3], label='EURUSD closing mov avg 5')\n",
    "ax31.plot(test_pred_fast_ma, linewidth=3, label='Predicted EURUSD closing mov avg 5')\n",
    "ax31.plot(test_data_slow_ma[seq_len:, 3], label='EURUSD closing mov avg 13')\n",
    "ax31.plot(test_pred_slow_ma, linewidth=3, label='Predicted EURUSD closing mov avg 13')\n",
    "ax31.plot(test_data_orig[:, 3], label='Original EURUSD Closing Returns')\n",
    "ax31.set_title(\"Test Data\", fontsize=18)\n",
    "ax31.set_xlabel('Date')\n",
    "ax31.set_ylabel('EURUSD Closing Returns')\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "plt.tight_layout()\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train models for backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fast_ma_window = fast_ma_data['ma_window']\n",
    "filepath = f'../my_stuff/final_{cur_pair}-{timeframe}_Bi-LSTM_{fast_ma_window}-ma_{tenkan_period}-{kijun_period}-{senkou_b_period}-ichi.hdf5'\n",
    "callback = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='loss', save_best_only=True, verbose=1)\n",
    "\n",
    "all_x_train_fast_ma, all_y_train_fast_ma = fast_ma_data['all_train_data_np']\n",
    "fast_ma_model = create_model(seq_len=all_x_train_fast_ma.shape[1], num_features=all_x_train_fast_ma.shape[2])\n",
    "\n",
    "start_t = time.time()\n",
    "\n",
    "fast_ma_model.fit(conc8vert_to_tensor(all_x_train_fast_ma), convert_to_tensor(all_y_train_fast_ma),\n",
    "                  batch_size=fast_ma_data['final_batch_size'],\n",
    "                  callbacks=[callback],\n",
    "                  epochs=num_epochs)\n",
    "\n",
    "print(f'training time = {(time.time()-start_t)/60} min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "slow_ma_window = slow_ma_data['ma_window']\n",
    "filepath = f'../my_stuff/final_{cur_pair}-{timeframe}_Bi-LSTM_{slow_ma_window}-ma_{tenkan_period}-{kijun_period}-{senkou_b_period}-ichi.hdf5'\n",
    "callback = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='loss', save_best_only=True, verbose=1)\n",
    "\n",
    "all_x_train_slow_ma, all_y_train_slow_ma = slow_ma_data['all_train_data_np']\n",
    "slow_ma_model = create_model(seq_len=all_x_train_slow_ma.shape[1], num_features=all_x_train_slow_ma.shape[2])\n",
    "\n",
    "start_t = time.time()\n",
    "\n",
    "slow_ma_model.fit(convert_to_tensor(all_x_train_slow_ma), convert_to_tensor(all_y_train_slow_ma),\n",
    "                  batch_size=slow_ma_data['final_batch_size'],\n",
    "                  callbacks=[callback],\n",
    "                  epochs=num_epochs)\n",
    "\n",
    "print(f'training time = {(time.time()-start_t)/60} min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# backtest models (xgboost for opening and CNN+Bi-LSTM for closing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prepare CNN+Bi-LSTM models and preprocessing vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fast MA window: 7\n",
      "sequence length for LSTMs: 128\n"
     ]
    }
   ],
   "source": [
    "fast_ma_model = tf.keras.models.load_model('../my_stuff/final_EURUSD-H1_Bi-LSTM_7-ma_9-30-60-ichi.hdf5')\n",
    "# slow_ma_model = tf.keras.models.load_model('../my_stuff/final_EURUSD-H1_Bi-LSTM_7-ma_9-30-60-ichi.hdf5')\n",
    "\n",
    "fast_ma_window = 7\n",
    "# slow_ma_window = 7\n",
    "lstm_seq_len = 128\n",
    "\n",
    "print(f'fast MA window: {fast_ma_window}')\n",
    "# print(f'slow MA window: {slow_ma_window}')\n",
    "print(f'sequence length for LSTMs: {lstm_seq_len}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prepare xgboost models and preprocessing vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels dict for XGB classifier:\n",
      "\t{1: 'buy', 0: 'sell'}\n",
      "signals to consider for opening trades:\n",
      "\tcloud_breakout_bull\n",
      "\tcloud_breakout_bear\n",
      "\ttk_cross_bull_strength\n",
      "\ttk_cross_bear_strength\n",
      "\ttk_price_cross_bull_strength\n",
      "\ttk_price_cross_bear_strength\n",
      "\tsenkou_cross_bull_strength\n",
      "\tsenkou_cross_bear_strength\n",
      "\tchikou_cross_bull_strength\n",
      "\tchikou_cross_bear_strength\n"
     ]
    }
   ],
   "source": [
    "xgb_decision_predictor = xgb.Booster()\n",
    "xgb_decision_predictor.load_model('../my_stuff/EURUSD-H1_0.01-min_profit_0.2-lots_right-cur_side_9-30-60-cb-tk-tkp-sen-chi-ichi_xgb_classifier.json')\n",
    "xgb_labels_dict = {1: 'buy', 0: 'sell'}\n",
    "open_trade_sigs = ['cloud_breakout_bull','cloud_breakout_bear',                       # cloud breakout\n",
    "                   'tk_cross_bull_strength', 'tk_cross_bear_strength',                # Tenkan Sen / Kijun Sen Cross\n",
    "                   'tk_price_cross_bull_strength', 'tk_price_cross_bear_strength',    # price crossing both the Tenkan Sen / Kijun Sen\n",
    "                   'senkou_cross_bull_strength', 'senkou_cross_bear_strength',        # Senkou Span Cross\n",
    "                   'chikou_cross_bull_strength', 'chikou_cross_bear_strength']        # Chikou Span Cross\n",
    "\n",
    "print(f'labels dict for XGB classifier:\\n\\t{xgb_labels_dict}')\n",
    "print('signals to consider for opening trades:')\n",
    "for sig in open_trade_sigs:\n",
    "    print(f'\\t{sig}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### global hyperparameters for backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all parameters SHOULD match what the models were trained on for best results (so far this assumption is consistant)\n",
    "\n",
    "# independant params\n",
    "min_profit_percent, profit_noise_percent = 0.002, 0.002\n",
    "contract_size = 100_000   # size of 1 lot is typically 100,000 (100 for gold, becuase 1 lot = 100 oz of gold)\n",
    "lots_per_trade = 0.2  \n",
    "starting_balance = 1000\n",
    "leverage = 500    # 1:leverage\n",
    "max_concurrent_trades = np.inf\n",
    "currency_side = 'right'\n",
    "in_quote_currency = True if currency_side == 'right' else False\n",
    "pip_resolution = 0.0001\n",
    "stop_out_pct = 0.2  # explaination: https://www.tradersway.com/new_to_the_market/forex_and_cfd_basics#margin\n",
    "fast_ma_diff_thresh = 0.039  #0.02\n",
    "# slow_ma_diff_thresh = 0.05   #0.02\n",
    "# fast_ma_diff_thresh_profit = 0.03  #0.02\n",
    "# slow_ma_diff_thresh_profit = 0.03   #0.02\n",
    "# fast_ma_diff_thresh_loss = 0.03 \n",
    "# slow_ma_diff_thresh_loss = 0.03   \n",
    "decision_prob_diff_thresh = 0.46   # 0.5 accepts all probabilities\n",
    "tenkan_period = 9\n",
    "kijun_period = 30\n",
    "senkou_b_period = 60\n",
    "label_non_signals=False\n",
    "hedged_margin = 50_000\n",
    "tradersway_commodity = False\n",
    "\n",
    "# dependant params (don't edit)\n",
    "pip_value = contract_size * lots_per_trade * pip_resolution   # in quote currency (right side currency of currency pair)\n",
    "min_profit = min_profit_percent * lots_per_trade * contract_size   # in base currecy because thats what models were traied on\n",
    "profit_noise = profit_noise_percent * lots_per_trade * contract_size   # in base currecy because thats what models were traied on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prepare data for backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 1053 rows of tick data from C:\\GitHub Repos\\ForexMachine\\Data\\.cache\\mt5_EURUSD_h1_ticks_2020-11-02T00;00UTC_to_2021-01-05T00;00UTC.csv\n",
      "saved 1053 rows of EURUSD h1 tick data to C:\\GitHub Repos\\ForexMachine\\Data\\RawData\\mt5_EURUSD_h1_ticks_2020-11-02T00;00UTC_to_2021-01-05T00;00UTC.csv, done.\n"
     ]
    }
   ],
   "source": [
    "ma_cols = ['Open','High','Low','Close','Volume']\n",
    "pc_cols = ['Open','High','Low','Close','Volume',\n",
    "           'trend_ichimoku_base','trend_ichimoku_conv',\n",
    "           'trend_ichimoku_a', 'trend_ichimoku_b']\n",
    "normalization_groups = [['Open','High','Low','Close'],  # prices\n",
    "                        ['trend_ichimoku_base','trend_ichimoku_conv'],  # ichi conv & base lines\n",
    "                        ['trend_ichimoku_a', 'trend_ichimoku_b'], # ichi cloud lines\n",
    "                        ['tk_cross_bull_strength','tk_cross_bear_strength',   # tk cross strength\n",
    "                        'tk_price_cross_bull_strength','tk_price_cross_bear_strength',   # tk price cross strength\n",
    "                        'senkou_cross_bull_strength','senkou_cross_bear_strength',   # semkou cross strength\n",
    "                        'chikou_cross_bull_strength','chikou_cross_bear_strength']]   # chikou cross strength\n",
    "\n",
    "model_config = {\n",
    "    'current_model':'ichi_cloud',\n",
    "    'ichi_cloud':{\n",
    "        'indicators': {\n",
    "            'ichimoku': {\n",
    "                'tenkan_period': tenkan_period,\n",
    "                'kijun_period': kijun_period,\n",
    "                'chikou_period': kijun_period,\n",
    "                'senkou_b_period': senkou_b_period\n",
    "            },\n",
    "            'rsi': {\n",
    "                'periods': 14\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# tick_data_filepath = gi.download_mt5_data(\"EURUSD\", 'H1', '2020-10-02', '2021-01-05')\n",
    "tick_data_filepath = gi.download_mt5_data(\"EURUSD\", 'H1', '2020-11-02', '2021-01-05')\n",
    "# tick_data_filepath = gi.download_mt5_data(\"EURUSD\", 'H1', '2020-10-02', '2020-12-18')\n",
    "data_with_indicators = gi.add_indicators_to_raw(filepath=tick_data_filepath, \n",
    "                                                save_to_disk=True, \n",
    "                                                config=model_config, \n",
    "                                                has_headers=True,\n",
    "                                                datetime_col='datetime')\n",
    "\n",
    "test_data_with_ichi_sigs = add_features(data_with_indicators)\n",
    "model_data = dummy_and_remove_features(test_data_with_ichi_sigs)\n",
    "\n",
    "start, stop = no_missing_data_idx_range(model_data)\n",
    "\n",
    "model_data = model_data.iloc[start:stop+1]\n",
    "model_data_np = model_data.to_numpy()\n",
    "\n",
    "test_data_with_ichi_sigs = test_data_with_ichi_sigs.iloc[start:stop+1]\n",
    "test_data_np = test_data_with_ichi_sigs.to_numpy()\n",
    "\n",
    "ma_cols_set = set([model_data.columns.get_loc(col_name) for col_name in ma_cols])\n",
    "pc_cols_set = set([model_data.columns.get_loc(col_name) for col_name in pc_cols])\n",
    "\n",
    "feature_indices = {test_data_with_ichi_sigs.columns[i]: i for i in range(len(test_data_with_ichi_sigs.columns))}\n",
    "\n",
    "fast_ma_data = get_split_data_ma(model_data, ma_window=fast_ma_window, seq_len=lstm_seq_len, split_percents=(0,0), \n",
    "                                  normalization_groups=normalization_groups, pc_cols=pc_cols, ma_cols=ma_cols, min_batch_size=1000, \n",
    "                                  max_batch_size=2000, just_train=True, print_info=False)\n",
    "# slow_ma_data = get_split_data_ma(model_data, ma_window=slow_ma_window, seq_len=lstm_seq_len, split_percents=(0,0), \n",
    "#                                   normalization_groups=normalization_groups, pc_cols=pc_cols, ma_cols=ma_cols, min_batch_size=1000, \n",
    "#                                   max_batch_size=2000, just_train=True, print_info=False)\n",
    "\n",
    "fast_ma_norm_terms = fast_ma_data['all_train_normalization_terms']\n",
    "# slow_ma_norm_terms = slow_ma_data['all_train_normalization_terms']\n",
    "\n",
    "test_data_labels = generate_ichimoku_labels(test_data_with_ichi_sigs, label_non_signals=label_non_signals, min_profit_percent=min_profit_percent, \n",
    "                                             profit_noise_percent=profit_noise_percent, signals_to_consider=open_trade_sigs, \n",
    "                                             contract_size=contract_size, lots_per_trade=lots_per_trade,\n",
    "                                             in_quote_currency=in_quote_currency, pip_resolution=pip_resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### analyze test data to develop trading strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot move avg cnn+lstm preds vs price data\n",
    "\n",
    "fast_ma_preds = fast_ma_model.predict(convert_to_tensor(fast_ma_data['all_train_data_np'][0]))\n",
    "fast_ma_preds = np.reshape(fast_ma_preds,(fast_ma_preds.shape[0],))\n",
    "fast_ma_preds = fast_ma_preds.tolist()\n",
    "fill = [None]*(len(test_data_with_ichi_sigs) - len(fast_ma_preds))\n",
    "fill.extend(fast_ma_preds)\n",
    "fast_ma_preds = fill\n",
    "\n",
    "# slow_ma_preds = slow_ma_model.predict(convert_to_tensor(slow_ma_data['all_train_data_np'][0]))\n",
    "# slow_ma_preds = np.reshape(slow_ma_preds,(slow_ma_preds.shape[0],))\n",
    "# slow_ma_preds = slow_ma_preds.tolist()\n",
    "# fill = [None]*(len(test_data_with_ichi_sigs) - len(slow_ma_preds))\n",
    "# fill.extend(slow_ma_preds)\n",
    "# slow_ma_preds = fill\n",
    "\n",
    "lstm_preds = pd.DataFrame({\n",
    "    'fast_ma':fast_ma_preds, \n",
    "#     'slow_ma':slow_ma_preds\n",
    "})\n",
    "\n",
    "# import random\n",
    "# test_data = [random.random() for i in range(len(test_data_with_ichi_sigs))]\n",
    "# test_data2 = [random.random() for i in range(len(test_data_with_ichi_sigs))]\n",
    "# test_data = {'testing1': test_data,\n",
    "#              'testing2': test_data2}\n",
    "# test_data=pd.DataFrame(test_data)\n",
    "# show_data_from_range(test_data_with_ichi_sigs, '2020-10-12', '2020-10-16', \n",
    "#                      main_indicator='ichimoku', sub_indicators=[test_data,'rsi'], visualize_crosses=True,\n",
    "#                      visualize_labels=True, labels_df=test_data_labels)\n",
    "\n",
    "labels = ['first_decision','ticks_till_best_profit_first_decision', 'best_profit_first_decision', 'profit_peak_first_decision',\n",
    "          'second_decision','ticks_till_best_profit_second_decision', 'best_profit_second_decision', 'profit_peak_second_decision']\n",
    "show_data_from_range(test_data_with_ichi_sigs, '2020-11-12', '2020-12-17', \n",
    "                     main_indicator='ichimoku', sub_indicators=[lstm_preds], visualize_crosses=True,\n",
    "                     visualize_labels=True, labels_df=test_data_labels, labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### backtest strat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model buffers full, beginning trade sim...\n",
      "backtest percentage done: 10%\n",
      "backtest percentage done: 20%\n",
      "backtest percentage done: 30%\n",
      "backtest percentage done: 40%\n",
      "backtest percentage done: 50%\n",
      "backtest percentage done: 60%\n",
      "backtest percentage done: 70%\n",
      "backtest percentage done: 80%\n",
      "backtest percentage done: 90%\n",
      "backtest percentage done: 100%\n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "BACKTEST RESULTS:\n",
      "ticks data duration: 47 days\n",
      "starting balance: 1000\n",
      "ending balance: 2820.9999999999986\n",
      "number of trades won: 53\n",
      "number of trades lost: 37\n",
      "number of buys: 83 (49 won, 34 lost)\n",
      "number of sells: 7 (4 won, 3 lost)\n",
      "balance range: [720.0000000000088, 3076.4000000000106]\n",
      "equity range: [720.0000000000092, 3102.000000000007]\n",
      "free margin range: [625.5754200000116, 3051.43120000001]\n",
      "margins range: [24.2822, 242.32760000000002]\n",
      "margin levels range: [476.79686947559895, 12196.312876210966]\n",
      "concurrently open trades range: [0, 5]\n",
      "concurrently losing trades range: [0, 3]\n",
      "backtest runtime: 0.4109503666559855 min\n",
      "\n",
      "WON TRADES RESULTS:\n",
      "fast_ma_diff_at_sig that aggreed: count=24, min=0.002360105514526367, max=0.2476765215396881, mean=0.05477861687541008, median=0.03575606644153595\n",
      "fast_ma_diff_at_sig that opposed: count=29, min=0.00038874149322509766, max=0.2312207818031311, mean=0.061209242790937424, median=0.04058310389518738\n",
      "fast_ma_diff_at_close: count=53, min=0.03935086727142334, max=0.20111024379730225, mean=0.07174347341060638, median=0.05651730298995972\n",
      "slow_ma_diff_at_close: count=53, min=0, max=0, mean=0.0, median=0.0\n",
      "fast_ma_diff_at_best_sign_to_close: count=48, min=0.0030101537704467773, max=0.10992085933685303, mean=0.01846056990325451, median=0.014145970344543457\n",
      "\n",
      "LOST TRADES RESULTS:\n",
      "fast_ma_diff_at_sig that aggreed: count=20, min=0.009531021118164062, max=0.08080616593360901, mean=0.04194578528404236, median=0.03756469488143921\n",
      "fast_ma_diff_at_sig that opposed: count=17, min=0.000674515962600708, max=0.09592795372009277, mean=0.04940159618854523, median=0.04841583967208862\n",
      "fast_ma_diff_at_close: count=37, min=0.03915613889694214, max=0.2312207818031311, mean=0.08705807477235794, median=0.08387595415115356\n",
      "slow_ma_diff_at_close: count=37, min=0, max=0, mean=0.0, median=0.0\n",
      "fast_ma_diff_at_best_sign_to_close: count=28, min=0.0020485520362854004, max=0.2312207818031311, mean=0.06826365739107132, median=0.06551942229270935\n",
      "\n",
      "MODELS STATS:\n",
      "average pred time of fast & slow MA CNN+LSTM models: 32.10414597328673 ms\n",
      "average pred time of XGB model: 2.258212335648075 ms\n"
     ]
    }
   ],
   "source": [
    "trades = {}\n",
    "backtest_trades = {}   # closed trades results\n",
    "pending_order = None\n",
    "pending_close = None\n",
    "decisions_so_far = []\n",
    "fast_ma_seq_buf = deque()\n",
    "slow_ma_seq_buf = deque()\n",
    "fast_ma_window_buf = deque()\n",
    "slow_ma_window_buf = deque()\n",
    "fast_ma_avgs = []\n",
    "slow_ma_avgs = []\n",
    "fast_ma_perc_chngs = []\n",
    "slow_ma_perc_chngs = []\n",
    "xgb_model_perc_chngs = []\n",
    "fast_ma_preds = []\n",
    "slow_ma_preds = []\n",
    "cnn_lstm_pred_times = []\n",
    "xgb_pred_times = []\n",
    "free_margins = []\n",
    "margins = []\n",
    "margin_levels = []\n",
    "equities = []\n",
    "balances = []\n",
    "open_trades_counts = []\n",
    "losing_trades_counts = []\n",
    "pct_done = 0\n",
    "buffers_rdy_idx = None\n",
    "balance = starting_balance\n",
    "equity = starting_balance\n",
    "free_margin = starting_balance\n",
    "losing_trades = 0\n",
    "margin_level = None\n",
    "margin = None\n",
    "final_dt = None\n",
    "stop = False\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(len(test_data_np)):\n",
    "    \"\"\"\n",
    "    fill data buffers for models\n",
    "    \"\"\"\n",
    "    \n",
    "    # for xgb model\n",
    "    \n",
    "    if i > 0:\n",
    "        row = apply_perc_change_list(model_data_np[i-1], model_data_np[i], cols_set=pc_cols_set)\n",
    "        xgb_model_perc_chngs.append(row)\n",
    "    \n",
    "    # for fast MA model\n",
    "    \n",
    "    fast_ma_window_buf.append(model_data_np[i])\n",
    "    if len(fast_ma_window_buf) > fast_ma_window:\n",
    "        fast_ma_window_buf.popleft()\n",
    "    \n",
    "    if len(fast_ma_window_buf) == fast_ma_window:\n",
    "        row = apply_moving_avg_q(fast_ma_window_buf, ma_cols_set)\n",
    "        fast_ma_avgs.append(row)\n",
    "    \n",
    "    if len(fast_ma_avgs) >= 2:\n",
    "        row = apply_perc_change_list(fast_ma_avgs[-2], fast_ma_avgs[-1], pc_cols_set)\n",
    "        row = normalize_data_list(row, fast_ma_norm_terms)\n",
    "        fast_ma_perc_chngs.append(row) \n",
    "    \n",
    "    if len(fast_ma_perc_chngs) > 0:\n",
    "        fast_ma_seq_buf.append(fast_ma_perc_chngs[-1])\n",
    "    \n",
    "    if len(fast_ma_seq_buf) > lstm_seq_len:\n",
    "        fast_ma_seq_buf.popleft()\n",
    "    \n",
    "    # for slow MA model\n",
    "    \n",
    "#     slow_ma_window_buf.append(model_data_np[i])\n",
    "#     if len(slow_ma_window_buf) > slow_ma_window:\n",
    "#         slow_ma_window_buf.popleft()\n",
    "    \n",
    "#     if len(slow_ma_window_buf) == slow_ma_window:\n",
    "#         row = apply_moving_avg_q(slow_ma_window_buf, ma_cols_set)\n",
    "#         slow_ma_avgs.append(row)\n",
    "    \n",
    "#     if len(slow_ma_avgs) >= 2:\n",
    "#         row = apply_perc_change_list(slow_ma_avgs[-2], slow_ma_avgs[-1], pc_cols_set)\n",
    "#         row = normalize_data_list(row, slow_ma_norm_terms)\n",
    "#         slow_ma_perc_chngs.append(row)  \n",
    "        \n",
    "#     if len(slow_ma_perc_chngs) > 0:\n",
    "#         slow_ma_seq_buf.append(slow_ma_perc_chngs[-1])\n",
    "    \n",
    "#     if len(slow_ma_seq_buf) > lstm_seq_len:\n",
    "#         slow_ma_seq_buf.popleft()\n",
    "    \n",
    "    # now check if LSTMs have enough data to being trade simulation\n",
    "    \n",
    "#     if len(fast_ma_seq_buf) == lstm_seq_len and len(slow_ma_seq_buf) == lstm_seq_len:\n",
    "    if len(fast_ma_seq_buf) == lstm_seq_len:\n",
    "        \"\"\"\n",
    "        simulate trading\n",
    "        \"\"\"\n",
    "        \n",
    "        if buffers_rdy_idx is None:\n",
    "            buffers_rdy_idx = i\n",
    "            print('model buffers full, beginning trade sim...')\n",
    "\n",
    "        # look for ichiomku signals\n",
    "        causes = []\n",
    "        for sig in open_trade_sigs:\n",
    "            sig_i = feature_indices[sig]\n",
    "            if test_data_np[i][sig_i] != 0:\n",
    "                causes.append(sig)\n",
    "\n",
    "        start = time.time()\n",
    "        fast_ma_pred = fast_ma_model.predict(np.array([fast_ma_seq_buf]))\n",
    "        slow_ma_pred = [[0]] #slow_ma_model.predict(np.array([slow_ma_seq_buf]))\n",
    "        duration = time.time() - start\n",
    "        cnn_lstm_pred_times.append(duration)\n",
    "\n",
    "        fast_ma_preds.append(fast_ma_pred[0][0])\n",
    "        slow_ma_preds.append(slow_ma_pred[0][0])\n",
    "    \n",
    "        if len(fast_ma_preds) > 1:\n",
    "            fast_ma_diff = fast_ma_preds[-1] - fast_ma_preds[-2]    # remember this is the diff in the pct_change of the mov avg\n",
    "            slow_ma_diff = slow_ma_preds[-1] - slow_ma_preds[-2]\n",
    "        else:\n",
    "            fast_ma_diff = 0\n",
    "            slow_ma_diff = 0\n",
    "\n",
    "        if pending_order is not None:\n",
    "            pending_order_i, decision_label, decision_prob, order_causes, sig_fast_ma_diff, sig_slow_ma_diff  = pending_order\n",
    "            open_price = test_data_np[i][feature_indices['Open']]\n",
    "            decision_prob_diff = abs(decision_label-decision_prob)\n",
    "            \n",
    "            trades[pending_order_i] = {\n",
    "                'decision_label': decision_label,\n",
    "                'decision_prob': decision_prob,\n",
    "                'causes': order_causes,\n",
    "                'open_price': open_price,\n",
    "                'trade_open_tick_i': i,\n",
    "                'profit': None,\n",
    "                'best_profit': None,\n",
    "                'ticks_till_close': None,\n",
    "                'close_idx': None,\n",
    "                'lots': lots_per_trade,\n",
    "                'look_to_close': False,\n",
    "                'forced_close': False,\n",
    "                'fast_ma_diff_at_sig': sig_fast_ma_diff,\n",
    "                'slow_ma_diff_at_sig': sig_slow_ma_diff,\n",
    "                'fast_ma_diff_at_close': None,\n",
    "                'slow_ma_diff_at_close': None,\n",
    "                'fast_ma_diff_at_best_sign_to_close': None,\n",
    "                'slow_ma_diff_at_best_sign_to_close': None\n",
    "            }\n",
    "            \n",
    "            required_margin = get_margin(trades, buy_label=1, sell_label=0, contract_size=contract_size, leverage=leverage, \n",
    "                                         tradersway_commodity=tradersway_commodity, in_quote_currency=in_quote_currency, hedged_margin=hedged_margin)\n",
    "            \n",
    "            # reference on opening trades and margin level https://www.luckscout.com/leverage-margin-balance-equity-free-margin-and-margin-level-in-forex-trading/\n",
    "            if required_margin > free_margin or (margin_level is not None and margin_level <= 100) \\\n",
    "                    or len(trades) > max_concurrent_trades or decision_prob_diff > decision_prob_diff_thresh:\n",
    "                del trades[pending_order_i]\n",
    "            else:\n",
    "                margin = required_margin\n",
    "            \n",
    "            pending_order = None\n",
    "        \n",
    "        # update equity and free margin based on currently opened trades\n",
    "        losing_trades = 0\n",
    "        for trade_i in trades:\n",
    "            trade = trades[trade_i]\n",
    "            close_price = test_data_np[i][feature_indices['Close']]\n",
    "            trade_decision = xgb_labels_dict[trade['decision_label']]\n",
    "\n",
    "            profit = get_profit(close_price, trade['open_price'], pip_value=pip_value, pip_resolution=pip_resolution, in_quote_currency=in_quote_currency)\n",
    "            if trade_decision == 'sell':\n",
    "                profit *= - 1\n",
    "            \n",
    "            if trade['profit'] is None:\n",
    "                profit_delta = profit\n",
    "            else:\n",
    "                profit_delta = profit - trade['profit']\n",
    "            trade['profit'] = profit\n",
    "            \n",
    "            if profit < 0:\n",
    "                losing_trades += 1\n",
    "            \n",
    "            if trade['best_profit'] is None or profit > trade['best_profit']:\n",
    "                trade['best_profit'] = profit\n",
    "                if (fast_ma_diff < 0 and trade_decision == 'buy') or (fast_ma_diff > 0 and trade_decision == 'sell'):\n",
    "                    trade['fast_ma_diff_at_best_sign_to_close'] = fast_ma_diff\n",
    "                if (slow_ma_diff < 0 and trade_decision == 'buy') or (slow_ma_diff > 0 and trade_decision == 'sell'):\n",
    "                    trade['slow_ma_diff_at_best_sign_to_close'] = slow_ma_diff\n",
    "            \n",
    "            equity += profit_delta\n",
    "            free_margin = equity - margin \n",
    "            margin_level = equity / margin * 100\n",
    "            \n",
    "            scaled_profit_noise = profit_noise if not in_quote_currency else profit_noise / close_price\n",
    "            if abs(profit) >= scaled_profit_noise:\n",
    "                trade['look_to_close'] = True\n",
    "        \n",
    "        # check if equity is <= 0, and if so end the sim\n",
    "        if equity <= 0:\n",
    "            stop = True\n",
    "            print(f'strat failed (i={i}, dt={test_data_np[i][feature_indices[\"datetime\"]]}): no more equity')\n",
    "            \n",
    "        # check if trades should be closed due to stop-out starting with biggest loss if so\n",
    "        if margin_level is not None and margin_level <= stop_out_pct:\n",
    "            sorted_keys = sorted(trades, key=lambda trade_i: trades[trade_i]['profit'])\n",
    "            for j, trade_i in enumerate(sorted_keys):\n",
    "                balance += trades[trade_i]['profit']\n",
    "\n",
    "                open_tick_i = trades[trade_i]['trade_open_tick_i']\n",
    "                trades[trade_i]['ticks_till_close'] = i - open_tick_i\n",
    "                trades[trade_i]['close_idx'] = i\n",
    "                trades[trade_i]['forced_close'] = True\n",
    "                trades[trade_i]['fast_ma_diff_at_close'] = fast_ma_diff\n",
    "                trades[trade_i]['slow_ma_diff_at_close'] = slow_ma_diff\n",
    "                backtest_trades[trade_i] = trades[trade_i]\n",
    "\n",
    "                del trades[trade_i]\n",
    "                \n",
    "                if j != len(sorted_keys) - 1:\n",
    "                    margin = get_margin(trades, buy_label=1, sell_label=0, contract_size=contract_size, leverage=leverage, tradersway_commodity=tradersway_commodity, \n",
    "                                        in_quote_currency=in_quote_currency, hedged_margin=hedged_margin)\n",
    "                    free_margin = equity - margin\n",
    "                    margin_level = equity / margin * 100                    \n",
    "                    if margin_level > stop_out_pct:\n",
    "                        break   \n",
    "        \n",
    "        # find trades to close based on CNN-LSTM preds\n",
    "        closed_trades = []\n",
    "        for trade_i in trades: \n",
    "            trade = trades[trade_i]\n",
    "            trade_decision = xgb_labels_dict[trade['decision_label']]\n",
    "        \n",
    "            if trade['look_to_close']:\n",
    "                if abs(fast_ma_diff) >= fast_ma_diff_thresh:\n",
    "                    # (MA pct_change is decreasing on a long trade) or (MA pct_change is increasing on a short trade)\n",
    "                    if (fast_ma_diff < 0 and trade_decision == 'buy') or (fast_ma_diff > 0 and trade_decision == 'sell'):  \n",
    "                        closed_trades.append(trade_i)\n",
    "\n",
    "        for trade_i in closed_trades:\n",
    "            balance += trades[trade_i]['profit']\n",
    "            \n",
    "            open_tick_i = trades[trade_i]['trade_open_tick_i']\n",
    "            trades[trade_i]['ticks_till_close'] = i - open_tick_i\n",
    "            trades[trade_i]['close_idx'] = i\n",
    "            trades[trade_i]['fast_ma_diff_at_close'] = fast_ma_diff\n",
    "            trades[trade_i]['slow_ma_diff_at_close'] = slow_ma_diff\n",
    "            backtest_trades[trade_i] = trades[trade_i]\n",
    "            \n",
    "            del trades[trade_i]\n",
    "        \n",
    "        if len(trades) == 0:\n",
    "            margin = None\n",
    "            margin_level = None\n",
    "\n",
    "        # generate decision w/ XGB classifier and create pending order\n",
    "        if len(causes) > 0 and not stop:\n",
    "            start = time.time()\n",
    "            model_input = pd.DataFrame([xgb_model_perc_chngs[-1]], columns=model_data.columns)\n",
    "            model_input = xgb.DMatrix(model_input)\n",
    "            decision_prob = xgb_decision_predictor.predict(model_input)[0]\n",
    "            duration = time.time() - start # inlucde converting input in pred time\n",
    "            xgb_pred_times.append(duration)\n",
    "            \n",
    "            decision_label = np.around(decision_prob)\n",
    "            \n",
    "#             if (decision_label == 1 and fast_ma_diff > 0) or (decision_label == 0 and fast_ma_diff < 0):\n",
    "#                 pending_order = (i, decision_label, decision_prob, causes, fast_ma_diff, slow_ma_diff)\n",
    "            pending_order = (i, decision_label, decision_prob, causes, fast_ma_diff, slow_ma_diff)\n",
    "\n",
    "        cur_pct_done = int((i-buffers_rdy_idx+1) / (len(test_data_np)-buffers_rdy_idx) * 100)\n",
    "        if cur_pct_done != pct_done and cur_pct_done % 10 == 0:\n",
    "            pct_done = cur_pct_done\n",
    "            print(f'backtest percentage done: {cur_pct_done}%')\n",
    "    \n",
    "    free_margins.append(free_margin)\n",
    "    equities.append(equity)\n",
    "    balances.append(balance)\n",
    "    margins.append(margin)\n",
    "    margin_levels.append(margin_level)\n",
    "    open_trades_counts.append(len(trades))\n",
    "    losing_trades_counts.append(losing_trades)\n",
    "    \n",
    "    final_dt = test_data_np[i][feature_indices[\"datetime\"]]\n",
    "    if stop:\n",
    "        break\n",
    "\n",
    "# print backtest results\n",
    "\n",
    "backtest_runtime = time.time() - start_time\n",
    "start_dt = test_data_np[buffers_rdy_idx][feature_indices['datetime']]\n",
    "end_dt = final_dt\n",
    "\n",
    "margin_levels_no_none = [ml for ml in margin_levels if ml is not None]\n",
    "margins_no_none = [m for m in margins if m is not None]\n",
    "\n",
    "num_won = 0\n",
    "num_lost = 0\n",
    "num_won_sells = 0\n",
    "num_won_buys = 0\n",
    "num_lost_sells = 0\n",
    "num_lost_buys = 0\n",
    "ma_diff_stat_names = ['fast_ma_diff_at_sig', 'slow_ma_diff_at_sig', 'fast_ma_diff_at_close', 'slow_ma_diff_at_close',\n",
    "                      'fast_ma_diff_at_best_sign_to_close', 'slow_ma_diff_at_best_sign_to_close']\n",
    "losses_ma_diff_stats = {name: {'list': [], 'agree_list':[], 'oppose_list':[]} for name in ma_diff_stat_names}\n",
    "wins_ma_diff_stats = {name: {'list': [], 'agree_list':[], 'oppose_list':[]} for name in ma_diff_stat_names}\n",
    "for trade_i in backtest_trades:\n",
    "    trade = backtest_trades[trade_i]\n",
    "    if trade['profit'] > 0:\n",
    "        if trade['decision_label'] == 1:\n",
    "            num_won_buys += 1\n",
    "        else:\n",
    "            num_won_sells += 1\n",
    "        num_won += 1\n",
    "        \n",
    "        if (trade['decision_label'] == 1 and trade['fast_ma_diff_at_sig'] > 0) or (trade['decision_label'] == 0 and trade['fast_ma_diff_at_sig'] < 0):\n",
    "            wins_ma_diff_stats['fast_ma_diff_at_sig']['agree_list'].append(abs(trade['fast_ma_diff_at_sig']))\n",
    "        elif (trade['decision_label'] == 1 and trade['fast_ma_diff_at_sig'] < 0) or (trade['decision_label'] == 0 and trade['fast_ma_diff_at_sig'] > 0):\n",
    "            wins_ma_diff_stats['fast_ma_diff_at_sig']['oppose_list'].append(abs(trade['fast_ma_diff_at_sig']))\n",
    "        if (trade['decision_label'] == 1 and trade['slow_ma_diff_at_sig'] > 0) or (trade['decision_label'] == 0 and trade['slow_ma_diff_at_sig'] < 0):\n",
    "            wins_ma_diff_stats['slow_ma_diff_at_sig']['agree_list'].append(abs(trade['slow_ma_diff_at_sig']))\n",
    "        elif (trade['decision_label'] == 1 and trade['slow_ma_diff_at_sig'] < 0) or (trade['decision_label'] == 0 and trade['slow_ma_diff_at_sig'] > 0):\n",
    "            wins_ma_diff_stats['slow_ma_diff_at_sig']['oppose_list'].append(abs(trade['slow_ma_diff_at_sig']))\n",
    "        \n",
    "        wins_ma_diff_stats['fast_ma_diff_at_close']['list'].append(abs(trade['fast_ma_diff_at_close']))\n",
    "        wins_ma_diff_stats['slow_ma_diff_at_close']['list'].append(abs(trade['slow_ma_diff_at_close']))\n",
    "        \n",
    "        if trade['fast_ma_diff_at_best_sign_to_close'] is not None:\n",
    "            wins_ma_diff_stats['fast_ma_diff_at_best_sign_to_close']['list'].append(abs(trade['fast_ma_diff_at_best_sign_to_close']))\n",
    "        if trade['slow_ma_diff_at_best_sign_to_close'] is not None:\n",
    "            wins_ma_diff_stats['slow_ma_diff_at_best_sign_to_close']['list'].append(abs(trade['slow_ma_diff_at_best_sign_to_close']))\n",
    "    else:\n",
    "        if trade['decision_label'] == 1:\n",
    "            num_lost_buys += 1\n",
    "        else:\n",
    "            num_lost_sells += 1\n",
    "        num_lost += 1\n",
    "        \n",
    "        if (trade['decision_label'] == 1 and trade['fast_ma_diff_at_sig'] > 0) or (trade['decision_label'] == 0 and trade['fast_ma_diff_at_sig'] < 0):\n",
    "            losses_ma_diff_stats['fast_ma_diff_at_sig']['agree_list'].append(abs(trade['fast_ma_diff_at_sig']))\n",
    "        elif (trade['decision_label'] == 1 and trade['fast_ma_diff_at_sig'] < 0) or (trade['decision_label'] == 0 and trade['fast_ma_diff_at_sig'] > 0):\n",
    "            losses_ma_diff_stats['fast_ma_diff_at_sig']['oppose_list'].append(abs(trade['fast_ma_diff_at_sig']))\n",
    "        if (trade['decision_label'] == 1 and trade['slow_ma_diff_at_sig'] > 0) or (trade['decision_label'] == 0 and trade['slow_ma_diff_at_sig'] < 0):\n",
    "            losses_ma_diff_stats['slow_ma_diff_at_sig']['agree_list'].append(abs(trade['slow_ma_diff_at_sig']))\n",
    "        elif (trade['decision_label'] == 1 and trade['slow_ma_diff_at_sig'] < 0) or (trade['decision_label'] == 0 and trade['slow_ma_diff_at_sig'] > 0):\n",
    "            losses_ma_diff_stats['slow_ma_diff_at_sig']['oppose_list'].append(abs(trade['slow_ma_diff_at_sig']))\n",
    "        \n",
    "        losses_ma_diff_stats['fast_ma_diff_at_close']['list'].append(abs(trade['fast_ma_diff_at_close']))\n",
    "        losses_ma_diff_stats['slow_ma_diff_at_close']['list'].append(abs(trade['slow_ma_diff_at_close']))\n",
    "        \n",
    "        if trade['fast_ma_diff_at_best_sign_to_close'] is not None:\n",
    "            losses_ma_diff_stats['fast_ma_diff_at_best_sign_to_close']['list'].append(abs(trade['fast_ma_diff_at_best_sign_to_close']))\n",
    "        if trade['slow_ma_diff_at_best_sign_to_close'] is not None:\n",
    "            losses_ma_diff_stats['slow_ma_diff_at_best_sign_to_close']['list'].append(abs(trade['slow_ma_diff_at_best_sign_to_close']))\n",
    "losses_ma_diff_stats = {name: {'arr': np.array(losses_ma_diff_stats[name]['list']), \n",
    "                               'agree_arr': np.array(losses_ma_diff_stats[name]['agree_list']), \n",
    "                               'oppose_arr': np.array(losses_ma_diff_stats[name]['oppose_list'])} for name in losses_ma_diff_stats}\n",
    "wins_ma_diff_stats = {name: {'arr': np.array(wins_ma_diff_stats[name]['list']), \n",
    "                             'agree_arr': np.array(wins_ma_diff_stats[name]['agree_list']), \n",
    "                             'oppose_arr': np.array(wins_ma_diff_stats[name]['oppose_list'])} for name in wins_ma_diff_stats}\n",
    "\n",
    "print('\\n--------------------------------------------------------------------\\n')\n",
    "print('BACKTEST RESULTS:')\n",
    "print(f'ticks data duration: {(end_dt-start_dt).days} days')\n",
    "print(f'starting balance: {starting_balance}')\n",
    "print(f'ending balance: {balance}')\n",
    "print(f'number of trades won: {num_won}')\n",
    "print(f'number of trades lost: {num_lost}')\n",
    "print(f'number of buys: {num_won_buys+num_lost_buys} ({num_won_buys} won, {num_lost_buys} lost)')\n",
    "print(f'number of sells: {num_won_sells+num_lost_sells} ({num_won_sells} won, {num_lost_sells} lost)')\n",
    "print(f'balance range: [{min(balances)}, {max(balances)}]')\n",
    "print(f'equity range: [{min(equities)}, {max(equities)}]')\n",
    "print(f'free margin range: [{min(free_margins)}, {max(free_margins)}]')\n",
    "print(f'margins range: [{min(margins_no_none)}, {max(margins_no_none)}]')\n",
    "print(f'margin levels range: [{min(margin_levels_no_none)}, {max(margin_levels_no_none)}]')\n",
    "print(f'concurrently open trades range: [{min(open_trades_counts)}, {max(open_trades_counts)}]')\n",
    "print(f'concurrently losing trades range: [{min(losing_trades_counts)}, {max(losing_trades_counts)}]')\n",
    "print(f'backtest runtime: {backtest_runtime/60} min')\n",
    "\n",
    "print('\\nWON TRADES RESULTS:')\n",
    "for stat in wins_ma_diff_stats:\n",
    "    stat_arr = wins_ma_diff_stats[stat]['arr']\n",
    "    stat_agree_arr = wins_ma_diff_stats[stat]['agree_arr']\n",
    "    stat_oppose_arr = wins_ma_diff_stats[stat]['oppose_arr']\n",
    "    if len(stat_arr) > 0:\n",
    "        print(f'{stat}: count={len(stat_arr)}, min={np.amin(stat_arr)}, max={np.amax(stat_arr)},'\n",
    "              f' mean={np.mean(stat_arr)}, median={np.median(stat_arr)}')\n",
    "    if len(stat_agree_arr) > 0:\n",
    "        print(f'{stat} that aggreed: count={len(stat_agree_arr)}, min={np.amin(stat_agree_arr)}, max={np.amax(stat_agree_arr)},'\n",
    "              f' mean={np.mean(stat_agree_arr)}, median={np.median(stat_agree_arr)}')\n",
    "    if len(stat_oppose_arr) > 0:\n",
    "        print(f'{stat} that opposed: count={len(stat_oppose_arr)}, min={np.amin(stat_oppose_arr)}, max={np.amax(stat_oppose_arr)},'\n",
    "              f' mean={np.mean(stat_oppose_arr)}, median={np.median(stat_oppose_arr)}')\n",
    "\n",
    "print('\\nLOST TRADES RESULTS:')\n",
    "for stat in losses_ma_diff_stats:\n",
    "    stat_arr = losses_ma_diff_stats[stat]['arr']\n",
    "    stat_agree_arr = losses_ma_diff_stats[stat]['agree_arr']\n",
    "    stat_oppose_arr = losses_ma_diff_stats[stat]['oppose_arr']\n",
    "    if len(stat_arr) > 0:\n",
    "        print(f'{stat}: count={len(stat_arr)}, min={np.amin(stat_arr)}, max={np.amax(stat_arr)},'\n",
    "              f' mean={np.mean(stat_arr)}, median={np.median(stat_arr)}')\n",
    "    if len(stat_agree_arr) > 0:\n",
    "        print(f'{stat} that aggreed: count={len(stat_agree_arr)}, min={np.amin(stat_agree_arr)}, max={np.amax(stat_agree_arr)},'\n",
    "              f' mean={np.mean(stat_agree_arr)}, median={np.median(stat_agree_arr)}')\n",
    "    if len(stat_oppose_arr) > 0:\n",
    "        print(f'{stat} that opposed: count={len(stat_oppose_arr)}, min={np.amin(stat_oppose_arr)}, max={np.amax(stat_oppose_arr)},'\n",
    "              f' mean={np.mean(stat_oppose_arr)}, median={np.median(stat_oppose_arr)}')\n",
    "\n",
    "print('\\nMODELS STATS:')\n",
    "print(f'average pred time of fast & slow MA CNN+LSTM models: {sum(cnn_lstm_pred_times)/len(cnn_lstm_pred_times)*1000} ms')\n",
    "print(f'average pred time of XGB model: {sum(xgb_pred_times)/len(xgb_pred_times)*1000} ms')\n",
    "\n",
    "# plot strategy over time vs. price data\n",
    "\n",
    "backtest_labels_col_names = ['decision_pred','ticks_till_best_profit_decision_pred', 'best_profit_decision_pred', 'profit_peak_decision_pred']\n",
    "backtest_labels = []\n",
    "for i in range(len(test_data_np)):\n",
    "    if i in backtest_trades:\n",
    "        trade = backtest_trades[i]\n",
    "        trade_decision = xgb_labels_dict[trade['decision_label']]\n",
    "        backtest_labels.append([trade_decision, trade['ticks_till_close'], trade['profit'], trade['close_idx']])\n",
    "    else:\n",
    "        backtest_labels.append([None]*len(backtest_labels_col_names))\n",
    "backtest_labels = pd.DataFrame(backtest_labels, columns=backtest_labels_col_names)\n",
    "backtest_labels = pd.concat((test_data_labels, backtest_labels.reset_index(drop=True)), axis=1)    \n",
    "\n",
    "fill = [None] * buffers_rdy_idx\n",
    "fill.extend(fast_ma_preds)\n",
    "fast_ma_preds = fill \n",
    "fast_ma_preds.extend([None]*(len(test_data_np) - len(fast_ma_preds)))\n",
    "\n",
    "fill = [None] * buffers_rdy_idx\n",
    "fill.extend(slow_ma_preds)\n",
    "slow_ma_preds = fill \n",
    "slow_ma_preds.extend([None]*(len(test_data_np) - len(slow_ma_preds)))\n",
    "\n",
    "lstm_preds = pd.DataFrame({\n",
    "    'fast_ma': fast_ma_preds, \n",
    "    'slow_ma': slow_ma_preds\n",
    "})\n",
    "\n",
    "balances.extend([None]*(len(test_data_np) - len(balances)))\n",
    "equities.extend([None]*(len(test_data_np) - len(equities)))\n",
    "free_margins.extend([None]*(len(test_data_np) - len(free_margins)))\n",
    "open_trades_counts.extend([None]*(len(test_data_np) - len(open_trades_counts)))\n",
    "losing_trades_counts.extend([None]*(len(test_data_np) - len(losing_trades_counts)))\n",
    "\n",
    "strat_data_df = pd.DataFrame({\n",
    "    'balance': balances,\n",
    "    'equity': equities,\n",
    "    'free margin': free_margins\n",
    "})\n",
    "\n",
    "open_trades_counts_df = pd.DataFrame({\n",
    "    'open trades': open_trades_counts,\n",
    "    'losing trades': losing_trades_counts\n",
    "})\n",
    "\n",
    "labels = [#'first_decision','ticks_till_best_profit_first_decision', 'best_profit_first_decision', 'profit_peak_first_decision',\n",
    "          'decision_pred','ticks_till_best_profit_decision_pred', 'best_profit_decision_pred', 'profit_peak_decision_pred']\n",
    "show_data_from_range(test_data_with_ichi_sigs, start_dt.isoformat(), end_dt.isoformat(), main_indicator='ichimoku', \n",
    "                     sub_indicators=[lstm_preds, strat_data_df, open_trades_counts_df], visualize_crosses=True, visualize_labels=True, \n",
    "                     labels_df=backtest_labels, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for debugging\n",
    "\n",
    "# fast_ma_perc_chngs = pd.DataFrame(fast_ma_perc_chngs,columns=model_data.columns)\n",
    "# print(fast_ma_perc_chngs.shape)\n",
    "\n",
    "# slow_ma_perc_chngs = pd.DataFrame(slow_ma_perc_chngs,columns=model_data.columns)\n",
    "# print(slow_ma_perc_chngs.shape)  \n",
    "\n",
    "# x = apply_moving_avg(model_data, ma_cols, fast_ma_window)\n",
    "# x.dropna(how='any', axis=0, inplace=True)\n",
    "# x = apply_perc_change(x, pc_cols)\n",
    "# x.dropna(how='any', axis=0, inplace=True)\n",
    "# x = normalize_data(x, train_data=False, normalization_terms=fast_ma_norm_terms)[0]\n",
    "# x_vals = x.to_numpy().astype(np.float32)\n",
    "\n",
    "# print(x.shape)\n",
    "# res = np.isclose(x_vals, fast_ma_perc_chngs.to_numpy().astype(np.float32))\n",
    "# print(res)\n",
    "# print(np.all(res))\n",
    "\n",
    "# print()\n",
    "\n",
    "# x = apply_moving_avg(model_data, ma_cols, slow_ma_window)\n",
    "# x.dropna(how='any', axis=0, inplace=True)\n",
    "# x = apply_perc_change(x, pc_cols)\n",
    "# x.dropna(how='any', axis=0, inplace=True)\n",
    "# x = normalize_data(x, train_data=False, normalization_terms=slow_ma_norm_terms)[0]\n",
    "# x_vals = x.to_numpy().astype(np.float32)\n",
    "\n",
    "# print(x.shape)\n",
    "# res = np.isclose(x_vals, slow_ma_perc_chngs.to_numpy().astype(np.float32))\n",
    "# print(res)\n",
    "# print(np.all(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tune strat hyperparams with grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'ma_models_settings': [\n",
    "        {\n",
    "            'fast_ma_model_path': '../my_stuff/final_EURUSD-H1_Bi-LSTM_5-ma_8-22-44-ichi.hdf5',\n",
    "            'slow_ma_model_path': '../my_stuff/final_EURUSD-H1_Bi-LSTM_13-ma_8-22-44-ichi.hdf5',\n",
    "            'fast_ma_window': 5,\n",
    "            'slow_ma_window': 13\n",
    "        },\n",
    "        {\n",
    "            'fast_ma_model_path': '../my_stuff/final_EURUSD-H1_Bi-LSTM_3-ma_8-22-44-ichi.hdf5',\n",
    "            'slow_ma_model_path': '../my_stuff/final_EURUSD-H1_Bi-LSTM_5-ma_8-22-44-ichi.hdf5',\n",
    "            'fast_ma_window': 3,\n",
    "            'slow_ma_window': 13\n",
    "        },\n",
    "        {\n",
    "            'fast_ma_model_path': '../my_stuff/final_EURUSD-H1_Bi-LSTM_7-ma_8-22-44-ichi.hdf5',\n",
    "            'slow_ma_model_path': '../my_stuff/final_EURUSD-H1_Bi-LSTM_13-ma_8-22-44-ichi.hdf5',\n",
    "            'fast_ma_window': 7,\n",
    "            'slow_ma_window': 13\n",
    "        },\n",
    "    ],\n",
    "    'xgb_model_settings': [\n",
    "        {\n",
    "            'model_filepath': '../my_stuff/EURUSD-H1_0.003-min_profit_0.2-lots_left-cur_side_8-22-24-cb-tk-tkp-sen-chi-ichi_xgb_classifier.json',\n",
    "            'lots_per_trade': 0.2,\n",
    "            'profit_noise_percent': 0.003,\n",
    "            'ichi_settings': (8, 22, 24),\n",
    "            'currency_side': 'left'\n",
    "        },\n",
    "        {\n",
    "            'model_filepath': '../my_stuff/EURUSD-H1_0.003-min_profit_0.2-lots_left-cur_side_8-22-44-cb-tk-tkp-sen-chi-ichi_xgb_classifier.json',\n",
    "            'lots_per_trade': 0.2,\n",
    "            'profit_noise_percent': 0.003,\n",
    "            'ichi_settings': (8, 22, 44),\n",
    "            'currency_side': 'left'\n",
    "        },\n",
    "        {\n",
    "            'model_filepath': '../my_stuff/EURUSD-H1_0.004-min_profit_0.2-lots_right-cur_side_8-22-24-cb-tk-tkp-sen-chi-ichi_xgb_classifier.json',\n",
    "            'lots_per_trade': 0.2,\n",
    "            'profit_noise_percent': 0.004,\n",
    "            'ichi_settings': (8, 22, 24),\n",
    "            'currency_side': 'right'\n",
    "        },\n",
    "        {\n",
    "            'model_filepath': '../my_stuff/EURUSD-H1_0.004-min_profit_0.2-lots_right-cur_side_8-22-44-cb-tk-tkp-sen-chi-ichi_xgb_classifier.json',\n",
    "            'lots_per_trade': 0.2,\n",
    "            'profit_noise_percent': 0.004,\n",
    "            'ichi_settings': (8, 22, 44),\n",
    "            'currency_side': 'right'\n",
    "        }\n",
    "    ],\n",
    "    'strat_params': [{\n",
    "        'starting_balance': [1000],\n",
    "        'leverage': [500],\n",
    "        'max_concurrent_trades': [np.inf, 10, 5],\n",
    "        'ma_diff_threshold_pairs': [(0.02, 0.02), (0.06, 0.06), (0.03, 0.03), (0.04, 0.04), (0.05, 0.05)],   # (fast ma's, slow ma's)\n",
    "        'decision_prob_diff_thresh': [0.5, 0.3, 0.4, 0.2]\n",
    "    }]\n",
    "}\n",
    "\n",
    "lstm_seq_len = 128\n",
    "xgb_labels_dict = {1: 'buy', 0: 'sell'}\n",
    "contract_size = 100_000   # size of 1 lot is typically 100,000 (100 for gold, becuase 1 lot = 100 oz of gold)\n",
    "pip_resolution = 0.0001\n",
    "stop_out_pct = 0.2  # explaination: https://www.tradersway.com/new_to_the_market/forex_and_cfd_basics#margin\n",
    "label_non_signals=False\n",
    "cur_pair = 'EURUSD'\n",
    "timeframe = 'H1'\n",
    "hedged_margin = 50_000\n",
    "tradersway_commodity = False\n",
    "\n",
    "open_trade_sigs = ['cloud_breakout_bull','cloud_breakout_bear',                       # cloud breakout\n",
    "                   'tk_cross_bull_strength', 'tk_cross_bear_strength',                # Tenkan Sen / Kijun Sen Cross\n",
    "                   'tk_price_cross_bull_strength', 'tk_price_cross_bear_strength',    # price crossing both the Tenkan Sen / Kijun Sen\n",
    "                   'senkou_cross_bull_strength', 'senkou_cross_bear_strength',        # Senkou Span Cross\n",
    "                   'chikou_cross_bull_strength', 'chikou_cross_bear_strength']        # Chikou Span Cross\n",
    "ma_cols = ['Open','High','Low','Close','Volume']\n",
    "pc_cols = ['Open','High','Low','Close','Volume',\n",
    "           'trend_ichimoku_base','trend_ichimoku_conv',\n",
    "           'trend_ichimoku_a', 'trend_ichimoku_b']\n",
    "normalization_groups = [['Open','High','Low','Close'],  # prices\n",
    "                        ['trend_ichimoku_base','trend_ichimoku_conv'],  # ichi conv & base lines\n",
    "                        ['trend_ichimoku_a', 'trend_ichimoku_b'], # ichi cloud lines\n",
    "                        ['tk_cross_bull_strength','tk_cross_bear_strength',   # tk cross strength\n",
    "                        'tk_price_cross_bull_strength','tk_price_cross_bear_strength',   # tk price cross strength\n",
    "                        'senkou_cross_bull_strength','senkou_cross_bear_strength',   # semkou cross strength\n",
    "                        'chikou_cross_bull_strength','chikou_cross_bear_strength']]   # chikou cross strength\n",
    "\n",
    "\n",
    "param_grid = ParameterGrid(param_grid)\n",
    "param_grid = random.sample(list(param_grid), len(param_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_dt_str = '2020-11-02'\n",
    "end_dt_str = '2021-01-05'\n",
    "tick_data_filepath = gi.download_mt5_data(cur_pair, timeframe, start_dt_str, end_dt_str) # (cur_pair, timeframe, '2020-10-02', '2021-01-05')\n",
    "best_strat_results = None\n",
    "best_strat_score = None\n",
    "backtest_results = []\n",
    "\n",
    "grid_search_start_time = time.time()\n",
    "for params_i, params in enumerate(param_grid):\n",
    "    s1 = time.time()\n",
    "    ma_models_settings = params['ma_models_settings']\n",
    "    xgb_model_settings = params['xgb_model_settings']\n",
    "    \n",
    "    strat_params = params['strat_params']\n",
    "    strat_params = ParameterGrid(strat_params)\n",
    "    strat_params = random.sample(list(strat_params), len(strat_params))\n",
    "    \n",
    "    fast_ma_model_path = ma_models_settings['fast_ma_model_path']\n",
    "    slow_ma_model_path = ma_models_settings['slow_ma_model_path']\n",
    "    fast_ma_window = ma_models_settings['fast_ma_window']\n",
    "    slow_ma_window = ma_models_settings['slow_ma_window']\n",
    "    \n",
    "    fast_ma_model = tf.keras.models.load_model(fast_ma_model_path)\n",
    "    slow_ma_model = tf.keras.models.load_model(slow_ma_model_path)\n",
    "    \n",
    "    xgb_model_path = xgb_model_settings['model_filepath']\n",
    "    lots_per_trade = xgb_model_settings['lots_per_trade']\n",
    "    profit_noise_percent = xgb_model_settings['profit_noise_percent']\n",
    "    tenkan_period, kijun_period, senkou_b_period = xgb_model_settings['ichi_settings']\n",
    "    currency_side = xgb_model_settings['currency_side']\n",
    "    in_quote_currency = True if currency_side == 'right' else False\n",
    "    \n",
    "    xgb_decision_predictor = xgb.Booster()\n",
    "    xgb_decision_predictor.load_model(xgb_model_path)\n",
    "    \n",
    "    model_config = {\n",
    "        'current_model':'ichi_cloud',\n",
    "        'ichi_cloud':{\n",
    "            'indicators': {\n",
    "                'ichimoku': {\n",
    "                    'tenkan_period': tenkan_period,\n",
    "                    'kijun_period': kijun_period,\n",
    "                    'chikou_period': kijun_period,\n",
    "                    'senkou_b_period': senkou_b_period\n",
    "                },\n",
    "                'rsi': {\n",
    "                    'periods': 14\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    data_with_indicators = gi.add_indicators_to_raw(filepath=tick_data_filepath, \n",
    "                                                    save_to_disk=False, \n",
    "                                                    config=model_config, \n",
    "                                                    has_headers=True,\n",
    "                                                    datetime_col='datetime')\n",
    "    test_data_with_ichi_sigs = add_features(data_with_indicators)\n",
    "    model_data = dummy_and_remove_features(test_data_with_ichi_sigs)\n",
    "    \n",
    "    start, stop = no_missing_data_idx_range(test_data_with_ichi_sigs, early_ending_cols=['chikou_span_visual'])\n",
    "    test_data_with_ichi_sigs = test_data_with_ichi_sigs.iloc[start:stop+1]\n",
    "    test_data_np = test_data_with_ichi_sigs.to_numpy()\n",
    "\n",
    "    start, stop = no_missing_data_idx_range(model_data, early_ending_cols=['chikou_span_visual'])\n",
    "    model_data = model_data.iloc[start:stop+1]\n",
    "    model_data_np = model_data.to_numpy()\n",
    "\n",
    "    ma_cols_set = set([model_data.columns.get_loc(col_name) for col_name in ma_cols])\n",
    "    pc_cols_set = set([model_data.columns.get_loc(col_name) for col_name in pc_cols])\n",
    "\n",
    "    feature_indices = {test_data_with_ichi_sigs.columns[i]: i for i in range(len(test_data_with_ichi_sigs.columns))}\n",
    "    \n",
    "    for params_i_2, params_2 in enumerate(strat_params):\n",
    "        s2 = time.time()\n",
    "        starting_balance = params_2['starting_balance']\n",
    "        leverage = params_2['leverage']    # 1:leverage\n",
    "        max_concurrent_trades = params_2['max_concurrent_trades']\n",
    "        fast_ma_diff_thresh, slow_ma_diff_thresh = params_2['ma_diff_threshold_pairs']\n",
    "        decision_prob_diff_thresh = params_2['decision_prob_diff_thresh']   # 0.5 accepts all probabilities\n",
    "        \n",
    "        pip_value = contract_size * lots_per_trade * pip_resolution   # in quote currency (right side currency of currency pair)\n",
    "        profit_noise = profit_noise_percent * lots_per_trade * contract_size   # in base currecy because thats what models were traied on\n",
    "        \n",
    "        fast_ma_data = get_split_data_ma(model_data, ma_window=fast_ma_window, seq_len=lstm_seq_len, split_percents=(0,0), \n",
    "                                          normalization_groups=normalization_groups, pc_cols=pc_cols, ma_cols=ma_cols, min_batch_size=1000, \n",
    "                                          max_batch_size=2000, just_train=True, print_info=False)\n",
    "        slow_ma_data = get_split_data_ma(model_data, ma_window=slow_ma_window, seq_len=lstm_seq_len, split_percents=(0,0), \n",
    "                                          normalization_groups=normalization_groups, pc_cols=pc_cols, ma_cols=ma_cols, min_batch_size=1000, \n",
    "                                          max_batch_size=2000, just_train=True, print_info=False)\n",
    "        \n",
    "        fast_ma_norm_terms = fast_ma_data['all_train_normalization_terms']\n",
    "        slow_ma_norm_terms = slow_ma_data['all_train_normalization_terms']\n",
    "        \n",
    "        trades = {}\n",
    "        backtest_trades = {}   # closed trades results\n",
    "        pending_order = None\n",
    "        pending_close = None\n",
    "        decisions_so_far = []\n",
    "        fast_ma_seq_buf = deque()\n",
    "        slow_ma_seq_buf = deque()\n",
    "        fast_ma_window_buf = deque()\n",
    "        slow_ma_window_buf = deque()\n",
    "        fast_ma_avgs = []\n",
    "        slow_ma_avgs = []\n",
    "        fast_ma_perc_chngs = []\n",
    "        slow_ma_perc_chngs = []\n",
    "        fast_ma_preds = []\n",
    "        slow_ma_preds = []\n",
    "        cnn_lstm_pred_times = []\n",
    "        xgb_pred_times = []\n",
    "        free_margins = []\n",
    "        margins = []\n",
    "        margin_levels = []\n",
    "        equities = []\n",
    "        balances = []\n",
    "        open_trades_counts = []\n",
    "        losing_trades_counts = []\n",
    "        pct_done = 0\n",
    "        buffers_rdy_idx = None\n",
    "        balance = starting_balance\n",
    "        equity = starting_balance\n",
    "        free_margin = starting_balance\n",
    "        losing_trades = 0\n",
    "        margin_level = None\n",
    "        margin = None\n",
    "        final_dt = None\n",
    "        stop = False\n",
    "\n",
    "        start_time = time.time()\n",
    "        for i in range(len(test_data_np)):\n",
    "            \"\"\"\n",
    "            fill data buffers for models\n",
    "            \"\"\"\n",
    "\n",
    "            # for fast MA model\n",
    "\n",
    "            fast_ma_window_buf.append(model_data_np[i])\n",
    "            if len(fast_ma_window_buf) > fast_ma_window:\n",
    "                fast_ma_window_buf.popleft()\n",
    "\n",
    "            if len(fast_ma_window_buf) == fast_ma_window:\n",
    "                row = apply_moving_avg_q(fast_ma_window_buf, ma_cols_set)\n",
    "                fast_ma_avgs.append(row)\n",
    "\n",
    "            if len(fast_ma_avgs) >= 2:\n",
    "                row = apply_perc_change_list(fast_ma_avgs[-2], fast_ma_avgs[-1], pc_cols_set)\n",
    "                row = normalize_data_list(row, fast_ma_norm_terms)\n",
    "                fast_ma_perc_chngs.append(row) \n",
    "\n",
    "            if len(fast_ma_perc_chngs) > 0:\n",
    "                fast_ma_seq_buf.append(fast_ma_perc_chngs[-1])\n",
    "\n",
    "            if len(fast_ma_seq_buf) > lstm_seq_len:\n",
    "                fast_ma_seq_buf.popleft()\n",
    "\n",
    "            # for slow MA model\n",
    "\n",
    "            slow_ma_window_buf.append(model_data_np[i])\n",
    "            if len(slow_ma_window_buf) > slow_ma_window:\n",
    "                slow_ma_window_buf.popleft()\n",
    "\n",
    "            if len(slow_ma_window_buf) == slow_ma_window:\n",
    "                row = apply_moving_avg_q(slow_ma_window_buf, ma_cols_set)\n",
    "                slow_ma_avgs.append(row)\n",
    "\n",
    "            if len(slow_ma_avgs) >= 2:\n",
    "                row = apply_perc_change_list(slow_ma_avgs[-2], slow_ma_avgs[-1], pc_cols_set)\n",
    "                row = normalize_data_list(row, slow_ma_norm_terms)\n",
    "                slow_ma_perc_chngs.append(row)  \n",
    "\n",
    "            if len(slow_ma_perc_chngs) > 0:\n",
    "                slow_ma_seq_buf.append(slow_ma_perc_chngs[-1])\n",
    "\n",
    "            if len(slow_ma_seq_buf) > lstm_seq_len:\n",
    "                slow_ma_seq_buf.popleft()\n",
    "\n",
    "            # check if LSTMs have enough data to being trade simulation\n",
    "            if len(fast_ma_seq_buf) == lstm_seq_len and len(slow_ma_seq_buf) == lstm_seq_len:\n",
    "                \"\"\"\n",
    "                simulate trading\n",
    "                \"\"\"\n",
    "\n",
    "                if buffers_rdy_idx is None:\n",
    "                    buffers_rdy_idx = i\n",
    "                    print('model buffers full, beginning trade sim...')\n",
    "\n",
    "                # look for ichiomku signals\n",
    "                causes = []\n",
    "                for sig in open_trade_sigs:\n",
    "                    sig_i = feature_indices[sig]\n",
    "                    if test_data_np[i][sig_i] != 0:\n",
    "                        causes.append(sig)\n",
    "\n",
    "                start = time.time()\n",
    "                fast_ma_pred = fast_ma_model.predict(np.array([fast_ma_seq_buf]))\n",
    "                slow_ma_pred = slow_ma_model.predict(np.array([slow_ma_seq_buf]))\n",
    "                duration = time.time() - start\n",
    "                cnn_lstm_pred_times.append(duration)\n",
    "\n",
    "                fast_ma_preds.append(fast_ma_pred[0][0])\n",
    "                slow_ma_preds.append(slow_ma_pred[0][0])\n",
    "\n",
    "                if len(fast_ma_preds) > 1:\n",
    "                    fast_ma_diff = fast_ma_preds[-1] - fast_ma_preds[-2]    # remember this is the diff in the pct_change of the mov avg\n",
    "                    slow_ma_diff = slow_ma_preds[-1] - slow_ma_preds[-2]\n",
    "                else:\n",
    "                    fast_ma_diff = 0\n",
    "                    slow_ma_diff = 0\n",
    "\n",
    "                if pending_order is not None:\n",
    "                    pending_order_i, decision_label, decision_prob, order_causes, sig_fast_ma_diff, sig_slow_ma_diff  = pending_order\n",
    "                    open_price = test_data_np[i][feature_indices['Open']]\n",
    "                    decision_prob_diff = abs(decision_label-decision_prob)\n",
    "\n",
    "                    trades[pending_order_i] = {\n",
    "                        'decision_label': decision_label,\n",
    "                        'decision_prob': decision_prob,\n",
    "                        'causes': order_causes,\n",
    "                        'open_price': open_price,\n",
    "                        'trade_open_tick_i': i,\n",
    "                        'profit': None,\n",
    "                        'best_profit': None,\n",
    "                        'ticks_till_close': None,\n",
    "                        'close_idx': None,\n",
    "                        'lots': lots_per_trade,\n",
    "                        'look_to_close': False,\n",
    "                        'forced_close': False,\n",
    "                        'fast_ma_diff_at_sig': sig_fast_ma_diff,\n",
    "                        'slow_ma_diff_at_sig': sig_slow_ma_diff,\n",
    "                        'fast_ma_diff_at_close': None,\n",
    "                        'slow_ma_diff_at_close': None,\n",
    "                        'fast_ma_diff_at_best_sign_to_close': None,\n",
    "                        'slow_ma_diff_at_best_sign_to_close': None\n",
    "                    }\n",
    "\n",
    "                    required_margin = get_margin(trades, buy_label=1, sell_label=0, contract_size=contract_size, leverage=leverage, \n",
    "                                                 tradersway_commodity=tradersway_commodity, in_quote_currency=in_quote_currency, hedged_margin=hedged_margin)\n",
    "\n",
    "                    # reference on opening trades and margin level https://www.luckscout.com/leverage-margin-balance-equity-free-margin-and-margin-level-in-forex-trading/\n",
    "                    if required_margin > free_margin or (margin_level is not None and margin_level <= 100) \\\n",
    "                            or len(trades) > max_concurrent_trades or decision_prob_diff > decision_prob_diff_thresh:\n",
    "                        del trades[pending_order_i]\n",
    "                    else:\n",
    "                        margin = required_margin\n",
    "\n",
    "                    pending_order = None\n",
    "\n",
    "                # update equity and free margin based on currently opened trades\n",
    "                losing_trades = 0\n",
    "                for trade_i in trades:\n",
    "                    trade = trades[trade_i]\n",
    "                    close_price = test_data_np[i][feature_indices['Close']]\n",
    "                    trade_decision = xgb_labels_dict[trade['decision_label']]\n",
    "\n",
    "                    profit = get_profit(close_price, trade['open_price'], pip_value=pip_value, pip_resolution=pip_resolution, in_quote_currency=in_quote_currency)\n",
    "                    if trade_decision == 'sell':\n",
    "                        profit *= - 1\n",
    "\n",
    "                    if trade['profit'] is None:\n",
    "                        profit_delta = profit\n",
    "                    else:\n",
    "                        profit_delta = profit - trade['profit']\n",
    "                    trade['profit'] = profit\n",
    "\n",
    "                    if profit < 0:\n",
    "                        losing_trades += 1\n",
    "\n",
    "                    if trade['best_profit'] is None or profit > trade['best_profit']:\n",
    "                        trade['best_profit'] = profit\n",
    "                        if (fast_ma_diff < 0 and trade_decision == 'buy') or (fast_ma_diff > 0 and trade_decision == 'sell'):\n",
    "                            trade['fast_ma_diff_at_best_sign_to_close'] = fast_ma_diff\n",
    "                        if (slow_ma_diff < 0 and trade_decision == 'buy') or (slow_ma_diff > 0 and trade_decision == 'sell'):\n",
    "                            trade['slow_ma_diff_at_best_sign_to_close'] = slow_ma_diff\n",
    "\n",
    "                    equity += profit_delta\n",
    "                    free_margin = equity - margin \n",
    "                    margin_level = equity / margin * 100\n",
    "\n",
    "                    scaled_profit_noise = profit_noise if not in_quote_currency else profit_noise / close_price\n",
    "                    if abs(profit) >= scaled_profit_noise:\n",
    "                        trade['look_to_close'] = True\n",
    "\n",
    "                # check if equity is <= 0, and if so end the sim\n",
    "                if equity <= 0:\n",
    "                    stop = True\n",
    "                    print(f'strat failed (i={i}, dt={test_data_np[i][feature_indices[\"datetime\"]]}): no more equity')\n",
    "\n",
    "                # check if trades should be closed due to stop-out starting with biggest loss if so\n",
    "                if margin_level is not None and margin_level <= stop_out_pct:\n",
    "                    sorted_keys = sorted(trades, key=lambda trade_i: trades[trade_i]['profit'])\n",
    "                    for j, trade_i in enumerate(sorted_keys):\n",
    "                        balance += trades[trade_i]['profit']\n",
    "\n",
    "                        open_tick_i = trades[trade_i]['trade_open_tick_i']\n",
    "                        trades[trade_i]['ticks_till_close'] = i - open_tick_i\n",
    "                        trades[trade_i]['close_idx'] = i\n",
    "                        trades[trade_i]['forced_close'] = True\n",
    "                        trades[trade_i]['fast_ma_diff_at_close'] = fast_ma_diff\n",
    "                        trades[trade_i]['slow_ma_diff_at_close'] = slow_ma_diff\n",
    "                        backtest_trades[trade_i] = trades[trade_i]\n",
    "\n",
    "                        del trades[trade_i]\n",
    "\n",
    "                        if j != len(sorted_keys) - 1:\n",
    "                            margin = get_margin(trades, buy_label=1, sell_label=0, contract_size=contract_size, leverage=leverage, tradersway_commodity=tradersway_commodity, \n",
    "                                                in_quote_currency=in_quote_currency, hedged_margin=hedged_margin)\n",
    "                            free_margin = equity - margin\n",
    "                            margin_level = equity / margin * 100                    \n",
    "                            if margin_level > stop_out_pct:\n",
    "                                break   \n",
    "\n",
    "                # find trades to close based on CNN-LSTM preds\n",
    "                closed_trades = []\n",
    "                for trade_i in trades: \n",
    "                    trade = trades[trade_i]\n",
    "                    trade_decision = xgb_labels_dict[trade['decision_label']]\n",
    "\n",
    "                    if trade['look_to_close']:\n",
    "                        if abs(fast_ma_diff) >= fast_ma_diff_thresh:\n",
    "                            # (MA pct_change is decreasing on a long trade) or (MA pct_change is increasing on a short trade)\n",
    "                            if (fast_ma_diff < 0 and trade_decision == 'buy') or (fast_ma_diff > 0 and trade_decision == 'sell'):  \n",
    "                                closed_trades.append(trade_i)\n",
    "\n",
    "                for trade_i in closed_trades:\n",
    "                    balance += trades[trade_i]['profit']\n",
    "\n",
    "                    open_tick_i = trades[trade_i]['trade_open_tick_i']\n",
    "                    trades[trade_i]['ticks_till_close'] = i - open_tick_i\n",
    "                    trades[trade_i]['close_idx'] = i\n",
    "                    trades[trade_i]['fast_ma_diff_at_close'] = fast_ma_diff\n",
    "                    trades[trade_i]['slow_ma_diff_at_close'] = slow_ma_diff\n",
    "                    backtest_trades[trade_i] = trades[trade_i]\n",
    "\n",
    "                    del trades[trade_i]\n",
    "\n",
    "                if len(trades) == 0:\n",
    "                    margin = None\n",
    "                    margin_level = None\n",
    "\n",
    "                # generate decision w/ XGB classifier and create pending order\n",
    "                if len(causes) > 0 and not stop:\n",
    "                    start = time.time()\n",
    "                    model_input = pd.DataFrame([model_data_np[i]], columns=model_data.columns)\n",
    "                    model_input = xgb.DMatrix(model_input)\n",
    "                    decision_prob = xgb_decision_predictor.predict(model_input)[0]\n",
    "                    duration = time.time() - start # inlucde converting input in pred time\n",
    "                    xgb_pred_times.append(duration)\n",
    "\n",
    "                    decision_label = np.around(decision_prob)\n",
    "\n",
    "#                     if (decision_label == 1 and fast_ma_diff > 0) or (decision_label == 0 and fast_ma_diff < 0):\n",
    "#                         pending_order = (i, decision_label, decision_prob, causes, fast_ma_diff, slow_ma_diff)\n",
    "                    pending_order = (i, decision_label, decision_prob, causes, fast_ma_diff, slow_ma_diff)\n",
    "\n",
    "                cur_pct_done = int((i-buffers_rdy_idx+1) / (len(test_data_np)-buffers_rdy_idx) * 100)\n",
    "                if cur_pct_done != pct_done and cur_pct_done % 10 == 0:\n",
    "                    pct_done = cur_pct_done\n",
    "                    print(f'backtest percentage done: {cur_pct_done}%')\n",
    "\n",
    "            free_margins.append(free_margin)\n",
    "            equities.append(equity)\n",
    "            balances.append(balance)\n",
    "            margins.append(margin)\n",
    "            margin_levels.append(margin_level)\n",
    "            open_trades_counts.append(len(trades))\n",
    "            losing_trades_counts.append(losing_trades)\n",
    "\n",
    "            final_dt = test_data_np[i][feature_indices[\"datetime\"]]\n",
    "            if stop:\n",
    "                break\n",
    "\n",
    "        # print backtest results\n",
    "\n",
    "        backtest_runtime = time.time() - start_time\n",
    "        start_dt = test_data_np[buffers_rdy_idx][feature_indices['datetime']]\n",
    "        end_dt = final_dt\n",
    "\n",
    "        margin_levels_no_none = [ml for ml in margin_levels if ml is not None]\n",
    "        margins_no_none = [m for m in margins if m is not None]\n",
    "\n",
    "#         ma_diff_stat_names = ['fast_ma_diff_at_sig', 'slow_ma_diff_at_sig', 'fast_ma_diff_at_close', 'slow_ma_diff_at_close',\n",
    "#                               'fast_ma_diff_at_best_sign_to_close', 'slow_ma_diff_at_best_sign_to_close']\n",
    "#         losses_ma_diff_stats = {name: {'list': [], 'agree_list':[], 'oppose_list':[]} for name in ma_diff_stat_names}\n",
    "#         wins_ma_diff_stats = {name: {'list': [], 'agree_list':[], 'oppose_list':[]} for name in ma_diff_stat_names}\n",
    "#         for trade_i in backtest_trades:\n",
    "#             trade = backtest_trades[trade_i]\n",
    "#             if trade['profit'] > 0:\n",
    "#                 if (trade['decision_label'] == 1 and trade['fast_ma_diff_at_sig'] > 0) or (trade['decision_label'] == 0 and trade['fast_ma_diff_at_sig'] < 0):\n",
    "#                     wins_ma_diff_stats['fast_ma_diff_at_sig']['agree_list'].append(abs(trade['fast_ma_diff_at_sig']))\n",
    "#                 elif (trade['decision_label'] == 1 and trade['fast_ma_diff_at_sig'] < 0) or (trade['decision_label'] == 0 and trade['fast_ma_diff_at_sig'] > 0):\n",
    "#                     wins_ma_diff_stats['fast_ma_diff_at_sig']['oppose_list'].append(abs(trade['fast_ma_diff_at_sig']))\n",
    "#                 if (trade['decision_label'] == 1 and trade['slow_ma_diff_at_sig'] > 0) or (trade['decision_label'] == 0 and trade['slow_ma_diff_at_sig'] < 0):\n",
    "#                     wins_ma_diff_stats['slow_ma_diff_at_sig']['agree_list'].append(abs(trade['slow_ma_diff_at_sig']))\n",
    "#                 elif (trade['decision_label'] == 1 and trade['slow_ma_diff_at_sig'] < 0) or (trade['decision_label'] == 0 and trade['slow_ma_diff_at_sig'] > 0):\n",
    "#                     wins_ma_diff_stats['slow_ma_diff_at_sig']['oppose_list'].append(abs(trade['slow_ma_diff_at_sig']))\n",
    "\n",
    "#                 wins_ma_diff_stats['fast_ma_diff_at_close']['list'].append(abs(trade['fast_ma_diff_at_close']))\n",
    "#                 wins_ma_diff_stats['slow_ma_diff_at_close']['list'].append(abs(trade['slow_ma_diff_at_close']))\n",
    "\n",
    "#                 if trade['fast_ma_diff_at_best_sign_to_close'] is not None:\n",
    "#                     wins_ma_diff_stats['fast_ma_diff_at_best_sign_to_close']['list'].append(abs(trade['fast_ma_diff_at_best_sign_to_close']))\n",
    "#                 if trade['slow_ma_diff_at_best_sign_to_close'] is not None:\n",
    "#                     wins_ma_diff_stats['slow_ma_diff_at_best_sign_to_close']['list'].append(abs(trade['slow_ma_diff_at_best_sign_to_close']))\n",
    "#             else:\n",
    "#                 if (trade['decision_label'] == 1 and trade['fast_ma_diff_at_sig'] > 0) or (trade['decision_label'] == 0 and trade['fast_ma_diff_at_sig'] < 0):\n",
    "#                     losses_ma_diff_stats['fast_ma_diff_at_sig']['agree_list'].append(abs(trade['fast_ma_diff_at_sig']))\n",
    "#                 elif (trade['decision_label'] == 1 and trade['fast_ma_diff_at_sig'] < 0) or (trade['decision_label'] == 0 and trade['fast_ma_diff_at_sig'] > 0):\n",
    "#                     losses_ma_diff_stats['fast_ma_diff_at_sig']['oppose_list'].append(abs(trade['fast_ma_diff_at_sig']))\n",
    "#                 if (trade['decision_label'] == 1 and trade['slow_ma_diff_at_sig'] > 0) or (trade['decision_label'] == 0 and trade['slow_ma_diff_at_sig'] < 0):\n",
    "#                     losses_ma_diff_stats['slow_ma_diff_at_sig']['agree_list'].append(abs(trade['slow_ma_diff_at_sig']))\n",
    "#                 elif (trade['decision_label'] == 1 and trade['slow_ma_diff_at_sig'] < 0) or (trade['decision_label'] == 0 and trade['slow_ma_diff_at_sig'] > 0):\n",
    "#                     losses_ma_diff_stats['slow_ma_diff_at_sig']['oppose_list'].append(abs(trade['slow_ma_diff_at_sig']))\n",
    "\n",
    "#                 losses_ma_diff_stats['fast_ma_diff_at_close']['list'].append(abs(trade['fast_ma_diff_at_close']))\n",
    "#                 losses_ma_diff_stats['slow_ma_diff_at_close']['list'].append(abs(trade['slow_ma_diff_at_close']))\n",
    "\n",
    "#                 if trade['fast_ma_diff_at_best_sign_to_close'] is not None:\n",
    "#                     losses_ma_diff_stats['fast_ma_diff_at_best_sign_to_close']['list'].append(abs(trade['fast_ma_diff_at_best_sign_to_close']))\n",
    "#                 if trade['slow_ma_diff_at_best_sign_to_close'] is not None:\n",
    "#                     losses_ma_diff_stats['slow_ma_diff_at_best_sign_to_close']['list'].append(abs(trade['slow_ma_diff_at_best_sign_to_close']))\n",
    "#         losses_ma_diff_stats = {name: {'arr': np.array(losses_ma_diff_stats[name]['list']), \n",
    "#                                        'agree_arr': np.array(losses_ma_diff_stats[name]['agree_list']), \n",
    "#                                        'oppose_arr': np.array(losses_ma_diff_stats[name]['oppose_list'])} for name in losses_ma_diff_stats}\n",
    "#         wins_ma_diff_stats = {name: {'arr': np.array(wins_ma_diff_stats[name]['list']), \n",
    "#                                      'agree_arr': np.array(wins_ma_diff_stats[name]['agree_list']), \n",
    "#                                      'oppose_arr': np.array(wins_ma_diff_stats[name]['oppose_list'])} for name in wins_ma_diff_stats}\n",
    "\n",
    "        print('\\n--------------------------------------------------------------------\\n')\n",
    "        print('BACKTEST RESULTS:')\n",
    "        print(f'ticks data duration: {(end_dt-start_dt).days} days')\n",
    "        print(f'starting balance: {starting_balance}')\n",
    "        print(f'ending balance: {balance}')\n",
    "        print(f'balance range: [{min(balances)}, {max(balances)}]')\n",
    "        print(f'equity range: [{min(equities)}, {max(equities)}]')\n",
    "        print(f'free margin range: [{min(free_margins)}, {max(free_margins)}]')\n",
    "        print(f'margins range: [{min(margins_no_none)}, {max(margins_no_none)}]')\n",
    "        print(f'margin levels range: [{min(margin_levels_no_none)}, {max(margin_levels_no_none)}]')\n",
    "        print(f'concurrently open trades range: [{min(open_trades_counts)}, {max(open_trades_counts)}]')\n",
    "        print(f'concurrently losing trades range: [{min(losing_trades_counts)}, {max(losing_trades_counts)}]')\n",
    "        print(f'backtest runtime: {backtest_runtime/60} min')\n",
    "\n",
    "#         print('\\nWON TRADES RESULTS:')\n",
    "#         for stat in wins_ma_diff_stats:\n",
    "#             stat_arr = wins_ma_diff_stats[stat]['arr']\n",
    "#             stat_agree_arr = wins_ma_diff_stats[stat]['agree_arr']\n",
    "#             stat_oppose_arr = wins_ma_diff_stats[stat]['oppose_arr']\n",
    "#             if len(stat_arr) > 0:\n",
    "#                 print(f'{stat}: count={len(stat_arr)}, min={np.amin(stat_arr)}, max={np.amax(stat_arr)},'\n",
    "#                       f' mean={np.mean(stat_arr)}, median={np.median(stat_arr)}')\n",
    "#             if len(stat_agree_arr) > 0:\n",
    "#                 print(f'{stat} that aggreed: count={len(stat_agree_arr)}, min={np.amin(stat_agree_arr)}, max={np.amax(stat_agree_arr)},'\n",
    "#                       f' mean={np.mean(stat_agree_arr)}, median={np.median(stat_agree_arr)}')\n",
    "#             if len(stat_oppose_arr) > 0:\n",
    "#                 print(f'{stat} that opposed: count={len(stat_oppose_arr)}, min={np.amin(stat_oppose_arr)}, max={np.amax(stat_oppose_arr)},'\n",
    "#                       f' mean={np.mean(stat_oppose_arr)}, median={np.median(stat_oppose_arr)}')\n",
    "\n",
    "#         print('\\nLOST TRADES RESULTS:')\n",
    "#         for stat in losses_ma_diff_stats:\n",
    "#             stat_arr = losses_ma_diff_stats[stat]['arr']\n",
    "#             stat_agree_arr = losses_ma_diff_stats[stat]['agree_arr']\n",
    "#             stat_oppose_arr = losses_ma_diff_stats[stat]['oppose_arr']\n",
    "#             if len(stat_arr) > 0:\n",
    "#                 print(f'{stat}: count={len(stat_arr)}, min={np.amin(stat_arr)}, max={np.amax(stat_arr)},'\n",
    "#                       f' mean={np.mean(stat_arr)}, median={np.median(stat_arr)}')\n",
    "#             if len(stat_agree_arr) > 0:\n",
    "#                 print(f'{stat} that aggreed: count={len(stat_agree_arr)}, min={np.amin(stat_agree_arr)}, max={np.amax(stat_agree_arr)},'\n",
    "#                       f' mean={np.mean(stat_agree_arr)}, median={np.median(stat_agree_arr)}')\n",
    "#             if len(stat_oppose_arr) > 0:\n",
    "#                 print(f'{stat} that opposed: count={len(stat_oppose_arr)}, min={np.amin(stat_oppose_arr)}, max={np.amax(stat_oppose_arr)},'\n",
    "#                       f' mean={np.mean(stat_oppose_arr)}, median={np.median(stat_oppose_arr)}')\n",
    "\n",
    "        print('\\nMODELS STATS:')\n",
    "        print(f'average pred time of fast & slow MA CNN+LSTM models: {sum(cnn_lstm_pred_times)/len(cnn_lstm_pred_times)*1000} ms')\n",
    "        print(f'average pred time of XGB model: {sum(xgb_pred_times)/len(xgb_pred_times)*1000} ms')\n",
    "        \n",
    "        results = {\n",
    "            'tenkan_period': tenkan_period,\n",
    "            'kijun_period': kijun_period,\n",
    "            'chikou_period': kijun_period,\n",
    "            'senkou_b_period': senkou_b_period,\n",
    "            'fast_ma_model_path': fast_ma_model_path,\n",
    "            'slow_ma_model_path': slow_ma_model_path,\n",
    "            'fast_ma_window': fast_ma_window,\n",
    "            'slow_ma_window': slow_ma_window,\n",
    "            'xgb_model_path': xgb_model_path,\n",
    "            'lots_per_trade': lots_per_trade,\n",
    "            'profit_noise_percent': profit_noise_percent,\n",
    "            'stop_out_pct': stop_out_pct,\n",
    "            'starting_balance': starting_balance,\n",
    "            'leverage': leverage,\n",
    "            'max_concurrent_trades': max_concurrent_trades,\n",
    "            'currency_side': currency_side,\n",
    "            'fast_ma_diff_thresh': fast_ma_diff_thresh,\n",
    "            'slow_ma_diff_thresh': slow_ma_diff_thresh,\n",
    "            'decision_prob_diff_thresh': decision_prob_diff_thresh,\n",
    "            'ending_balance': balance,\n",
    "            'max_balance': max(balances),\n",
    "            'min_balance': min(balances),\n",
    "            'max_equity': max(equities),\n",
    "            'min_equity': min(equities),\n",
    "            'max_free_margin': max(free_margins),\n",
    "            'min_free_margin': min(free_margins),\n",
    "            'max_margin': max(margins_no_none),\n",
    "            'min_margin': min(margins_no_none),\n",
    "            'max_margin_level': max(margin_levels_no_none),\n",
    "            'min_margin_level': min(margin_levels_no_none),\n",
    "            'max_concurrently_open_trades': max(open_trades_counts),\n",
    "            'min_concurrently_open_trades': min(open_trades_counts),\n",
    "        }\n",
    "        \n",
    "        strat_score = balance\n",
    "        if best_strat_results is None or best_strat_score < strat_score:\n",
    "            best_strat_results = results\n",
    "            best_strat_score = strat_score\n",
    "            \n",
    "        backtest_results.append(results)\n",
    "        \n",
    "        print('\\n--------------------------------------------------------------------------------')\n",
    "        print(f'{params_i_2+1}/{len(strat_params)} strat params tested, runtime of last params: {(time.time()-s2)/60} min')\n",
    "        print('--------------------------------------------------------------------------------\\n')\n",
    "        print(f'last backtest results:')\n",
    "        print(f'{results}\\n')\n",
    "        print(f'best backtest results:')\n",
    "        print(f'{best_strat_results}\\n')\n",
    "        \n",
    "    print('\\n--------------------------------------------------------------------------------')\n",
    "    print(f'{params_i+1}/{len(param_grid)} model combos tested, runtime of last combo: {(time.time()-s1)/60} min')\n",
    "    print('--------------------------------------------------------------------------------\\n')\n",
    "\n",
    "    backtest_results_sorted = sorted(backtest_results, key=lambda d: d['ending_balance'], reverse=True)\n",
    "    backtest_results_sorted_df = pd.DataFrame(backtest_results_sorted)\n",
    "    backtest_results_sorted_df.to_csv(f'../my_stuff/{cur_pair}-{timeframe}_{start_dt_str}-to-{end_dt_str}_backtest_grid_search_results.csv')\n",
    "\n",
    "print(f'grid search runtime: {(time.time()-grid_search_start_time)/60} min')\n",
    "\n",
    "backtest_results_sorted = sorted(backtest_results, key=lambda d: d['ending_balance'], reverse=True)\n",
    "backtest_results_sorted_df = pd.DataFrame(backtest_results_sorted)\n",
    "backtest_results_sorted_df.to_csv(f'../my_stuff/{cur_pair}-{timeframe}_{start_dt_str}-to-{end_dt_str}_backtest_grid_search_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# notes on things to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "if self.data[i][self.feature_indices['datetime']].strftime('%Y-%m-%dT%H:%M') == '2013-05-28T10:00':\n",
    "    print('yo')\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "To-do:\n",
    "\n",
    "1) tune hyperparams for backtest of xgboost for opening and CNN+Bi-LSTM for closing strat\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
