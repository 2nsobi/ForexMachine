{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# external packages\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt \n",
    "from importlib import reload\n",
    "import matplotlib\n",
    "%matplotlib qt\n",
    "# %matplotlib inline\n",
    "import numpy as np\n",
    "matplotlib.style.use('default')\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "from collections import namedtuple\n",
    "from collections import deque\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local modules and packages\n",
    "from ForexMachine.Preprocessing import research\n",
    "from ForexMachine import util\n",
    "reload(research)\n",
    "reload(util)\n",
    "\n",
    "# global variables\n",
    "global_train_data_range_start, global_train_data_range_end = research.TRAIN_DATA_START_ISO, research.TRAIN_DATA_END_ISO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trying loading and adding indicators to raw data w/ ForexMachine package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>datetime</th>\n",
       "      <th>trend_ichimoku_conv</th>\n",
       "      <th>trend_ichimoku_base</th>\n",
       "      <th>trend_ichimoku_a</th>\n",
       "      <th>trend_ichimoku_b</th>\n",
       "      <th>trend_visual_ichimoku_a</th>\n",
       "      <th>trend_visual_ichimoku_b</th>\n",
       "      <th>chikou_span</th>\n",
       "      <th>chikou_span_visual</th>\n",
       "      <th>momentum_rsi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018.11.09</td>\n",
       "      <td>02:00</td>\n",
       "      <td>14.11521</td>\n",
       "      <td>14.12511</td>\n",
       "      <td>14.10894</td>\n",
       "      <td>14.12415</td>\n",
       "      <td>8384</td>\n",
       "      <td>2018-11-09 02:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.117025</td>\n",
       "      <td>15.373863</td>\n",
       "      <td>15.372442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.30154</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018.11.09</td>\n",
       "      <td>03:00</td>\n",
       "      <td>14.12420</td>\n",
       "      <td>14.12751</td>\n",
       "      <td>14.11264</td>\n",
       "      <td>14.12071</td>\n",
       "      <td>5841</td>\n",
       "      <td>2018-11-09 03:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.118225</td>\n",
       "      <td>15.373863</td>\n",
       "      <td>15.372442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.30169</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018.11.09</td>\n",
       "      <td>04:00</td>\n",
       "      <td>14.12069</td>\n",
       "      <td>14.13135</td>\n",
       "      <td>14.11893</td>\n",
       "      <td>14.12025</td>\n",
       "      <td>2804</td>\n",
       "      <td>2018-11-09 04:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.120145</td>\n",
       "      <td>15.373863</td>\n",
       "      <td>15.372442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.32429</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018.11.09</td>\n",
       "      <td>05:00</td>\n",
       "      <td>14.12025</td>\n",
       "      <td>14.15063</td>\n",
       "      <td>14.11565</td>\n",
       "      <td>14.15018</td>\n",
       "      <td>3769</td>\n",
       "      <td>2018-11-09 05:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.129785</td>\n",
       "      <td>15.373863</td>\n",
       "      <td>15.372442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.41788</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018.11.09</td>\n",
       "      <td>06:00</td>\n",
       "      <td>14.15020</td>\n",
       "      <td>14.18167</td>\n",
       "      <td>14.14717</td>\n",
       "      <td>14.18014</td>\n",
       "      <td>2260</td>\n",
       "      <td>2018-11-09 06:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.145305</td>\n",
       "      <td>15.373863</td>\n",
       "      <td>15.372442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.40633</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date   Time      Open      High       Low     Close  Volume  \\\n",
       "0  2018.11.09  02:00  14.11521  14.12511  14.10894  14.12415    8384   \n",
       "1  2018.11.09  03:00  14.12420  14.12751  14.11264  14.12071    5841   \n",
       "2  2018.11.09  04:00  14.12069  14.13135  14.11893  14.12025    2804   \n",
       "3  2018.11.09  05:00  14.12025  14.15063  14.11565  14.15018    3769   \n",
       "4  2018.11.09  06:00  14.15020  14.18167  14.14717  14.18014    2260   \n",
       "\n",
       "             datetime  trend_ichimoku_conv  trend_ichimoku_base  \\\n",
       "0 2018-11-09 02:00:00                  NaN                  NaN   \n",
       "1 2018-11-09 03:00:00                  NaN                  NaN   \n",
       "2 2018-11-09 04:00:00                  NaN                  NaN   \n",
       "3 2018-11-09 05:00:00                  NaN                  NaN   \n",
       "4 2018-11-09 06:00:00                  NaN                  NaN   \n",
       "\n",
       "   trend_ichimoku_a  trend_ichimoku_b  trend_visual_ichimoku_a  \\\n",
       "0               NaN         14.117025                15.373863   \n",
       "1               NaN         14.118225                15.373863   \n",
       "2               NaN         14.120145                15.373863   \n",
       "3               NaN         14.129785                15.373863   \n",
       "4               NaN         14.145305                15.373863   \n",
       "\n",
       "   trend_visual_ichimoku_b  chikou_span  chikou_span_visual  momentum_rsi  \n",
       "0                15.372442          NaN            14.30154           NaN  \n",
       "1                15.372442          NaN            14.30169           NaN  \n",
       "2                15.372442          NaN            14.32429           NaN  \n",
       "3                15.372442          NaN            14.41788           NaN  \n",
       "4                15.372442          NaN            14.40633           NaN  "
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indicators_info = {\n",
    "    'ichimoku': {\n",
    "        'tenkan_period': 9,\n",
    "        'kijun_period': 26,\n",
    "        'chikou_period': 26,\n",
    "        'senkou_b_period': 24\n",
    "    },\n",
    "    'rsi': {\n",
    "        'periods': 14\n",
    "    }\n",
    "}\n",
    "data_with_indicators = research.add_indicators_to_raw(filepath='../my_stuff/USDZARi60.csv', has_headers=False, \n",
    "                                                      headers=['Date','Time','Open', 'High', 'Low', 'Close', 'Volume'],\n",
    "                                                      indicators_info=indicators_info)\n",
    "data_with_indicators.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define helper plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_index_range(datetime1, datetime2, datetimes):\n",
    "    i1 = -1\n",
    "    i2 = -1\n",
    "    if datetime1 <= datetime2:\n",
    "        for i in range(len(datetimes)):\n",
    "            i1 = i\n",
    "            if datetimes[i] == datetime1:\n",
    "                break\n",
    "            if datetimes[i] > datetime1:\n",
    "                i1 = i-1 if i-1 >= 0 else 0\n",
    "                break\n",
    "        for i in range(i1, len(datetimes)):\n",
    "            i2 = i\n",
    "            if datetimes[i] == datetime2:\n",
    "                break\n",
    "            if datetimes[i] > datetime2:\n",
    "                i2 = i-1 if i-1 >= 0 else 0\n",
    "                break\n",
    "    return i1, i2\n",
    "\n",
    "# date format 'yyyy.mm.dd'\n",
    "def show_data_from_range(df, date1, date2, main_indicator, sub_indicators = [], visualize_crosses=False, crosses=None,\n",
    "                         visualize_labels=False, labels_df=None, labels=None):\n",
    "    \n",
    "    start, stop = get_index_range(pd.Timestamp.fromisoformat(date1), pd.Timestamp.fromisoformat(date2), df['datetime'].to_numpy())\n",
    "    if start < 0 or stop < 0:\n",
    "        print(f'invalid dates (start i = {start}, stop i = {stop})')\n",
    "        return\n",
    "    \n",
    "    data_range = df.iloc[start:stop+1]\n",
    "    chart_count = len(sub_indicators) + 1\n",
    "    \n",
    "    top_chart_ratio = 1\n",
    "    sub_chart_ratio = 0\n",
    "    if chart_count == 2:\n",
    "        top_chart_ratio = 3\n",
    "        sub_chart_ratio = 2 / (chart_count-1)\n",
    "    if chart_count > 2:\n",
    "        top_chart_ratio = 1\n",
    "        sub_chart_ratio = 1 / (chart_count-1)\n",
    "    height_ratios = [top_chart_ratio]\n",
    "    height_ratios.extend([sub_chart_ratio]*(chart_count-1))\n",
    "    fig, axes = plt.subplots(chart_count,1,sharex='col', gridspec_kw={'height_ratios':height_ratios})\n",
    "    fig.tight_layout(pad=1.8, h_pad=0.0)\n",
    "    \n",
    "    top_ax = None\n",
    "    bottom_ax = None\n",
    "    if chart_count > 1:\n",
    "        top_ax = axes[0]\n",
    "        bottom_ax = axes[len(axes)-1]\n",
    "    else:\n",
    "        bottom_ax = top_ax = axes\n",
    "    top_ax.plot(data_range.Close.to_list(), label='Close',color='brown')\n",
    "    \n",
    "    plot_indicator_funcs = {\n",
    "        'ichimoku': lambda ax, dataf: add_ichimoku_to_plot(ax, dataf, visualize_crosses, crosses),\n",
    "        'rsi': lambda ax, dataf: add_rsi_to_plot(ax, dataf),\n",
    "        'extra': lambda ax, extra_df, plot_range: add_extra_data_to_plot(ax, extra_df, plot_range)\n",
    "    }\n",
    "    \n",
    "    plot_indicator_funcs[main_indicator](top_ax, data_range)\n",
    "    \n",
    "    for i in range(len(sub_indicators)):\n",
    "        item = sub_indicators[i]\n",
    "        if isinstance(item, str):\n",
    "            plot_indicator_funcs[sub_indicators[i]](axes[i+1], data_range)\n",
    "        elif isinstance(item, pd.DataFrame):\n",
    "            plot_indicator_funcs['extra'](axes[i+1], item, (start, stop))\n",
    "        \n",
    "    if visualize_labels and labels_df is not None:\n",
    "        add_labels_to_plot(top_ax, df, labels_df, (start, stop), labels)\n",
    "\n",
    "    bottom_ax.set_xticks(np.arange(len(data_range)))\n",
    "    x_labels = [dt.strftime('%Y-%m-%d %H:%M') * ((i+1)%2) for i,dt in enumerate(data_range['datetime'])]\n",
    "    bottom_ax.set_xticklabels(x_labels,rotation=80, wrap=True)\n",
    "    \n",
    "    if chart_count > 1:\n",
    "        for ax in axes:\n",
    "            ax.legend()\n",
    "    else:\n",
    "        top_ax.legend()\n",
    "        \n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "Functions for adding indicators to a matplotlib chart\n",
    "\"\"\"\n",
    "\n",
    "def add_ichimoku_to_plot(ax, df, visualize_crosses = False, crosses=None):\n",
    "    ax.plot(df.trend_visual_ichimoku_a.to_list(), label='Senkou-Span a',linestyle='--',color='green')\n",
    "    ax.plot(df.trend_visual_ichimoku_b.to_list(), label='Senkou-Span b',linestyle='--',color='red')\n",
    "    ax.fill_between(np.arange(len(df)),df.trend_visual_ichimoku_a,\n",
    "                    df.trend_visual_ichimoku_b,alpha=0.2,color='green',\n",
    "                    where=(df.trend_visual_ichimoku_a > df.trend_visual_ichimoku_b))\n",
    "    ax.fill_between(np.arange(len(df)),df.trend_visual_ichimoku_a,\n",
    "                    df.trend_visual_ichimoku_b,alpha=0.2,color='red',\n",
    "                    where=(df.trend_visual_ichimoku_a <= df.trend_visual_ichimoku_b))\n",
    "    ax.plot(df.trend_ichimoku_conv.to_list(), label='Tenkan-Sen (conversion)',color='cyan')\n",
    "    ax.plot(df.trend_ichimoku_base.to_list(), label='Kijun Sen (base)',color='blue')\n",
    "    ax.plot(df.chikou_span_visual.to_list(), label='chikou span',linestyle=':',color='orange')\n",
    "    \n",
    "    if visualize_crosses:\n",
    "        colors = {\n",
    "            'tk_cross': 'hotpink',\n",
    "            'tk_price_cross': 'brown',\n",
    "            'senkou_cross': 'blue',\n",
    "            'chikou_cross': 'orange',\n",
    "            'kumo_breakout': 'purple'\n",
    "        }\n",
    "        \n",
    "        df_idx = {df.columns[i]: i for i in range(len(df.columns))}\n",
    "        data = df.to_numpy()\n",
    "        \n",
    "        if crosses is None:\n",
    "            crosses = set(['tk_cross', 'tk_price_cross', 'senkou_cross', 'chikou_cross', 'kumo_breakout'])\n",
    "        else:\n",
    "            crosses = set(crosses)\n",
    "                \n",
    "        for i in range(len(data)):   \n",
    "            close = data[i][df_idx['Close']]            \n",
    "            vert_occupied = False\n",
    "            filler = ''\n",
    "            \n",
    "            # tk cross\n",
    "            if 'tk_cross' in crosses:\n",
    "                tk_cross_bull_strength = abs(data[i][df_idx['tk_cross_bull_strength']])\n",
    "                tk_cross_bear_strength = abs(data[i][df_idx['tk_cross_bear_strength']])\n",
    "                tk_cross_length_bull = data[i][df_idx['tk_cross_bull_length']]\n",
    "                tk_cross_length_bear = data[i][df_idx['tk_cross_bear_length']]\n",
    "            \n",
    "                if not np.isnan(tk_cross_bull_strength) and tk_cross_bull_strength > 0:\n",
    "                    ax.axvline(x = i, color = colors['tk_cross'])\n",
    "                    ax.text(x = i, y = close, color = colors['tk_cross'],\n",
    "                            s = f'^ TK Cross Bull\\nstrength={tk_cross_bull_strength}\\nlength={tk_cross_length_bull}')\n",
    "                    vert_occupied = True\n",
    "\n",
    "                if not np.isnan(tk_cross_bear_strength) and tk_cross_bear_strength > 0:\n",
    "                    if vert_occupied:\n",
    "                        filler += '\\n'*3\n",
    "                    ax.axvline(x = i, color = colors['tk_cross'])\n",
    "                    ax.text(x = i, y = close, color = colors['tk_cross'],\n",
    "                            s = f'_ TK Cross Bear\\nstrength={tk_cross_bear_strength}'\n",
    "                                f'\\nlength={tk_cross_length_bear}{filler}')\n",
    "                    vert_occupied = True\n",
    "            \n",
    "            # tk price cross\n",
    "            if 'tk_price_cross' in crosses:\n",
    "                tk_price_cross_bull_strength = abs(data[i][df_idx['tk_price_cross_bull_strength']])\n",
    "                tk_price_cross_bear_strength = abs(data[i][df_idx['tk_price_cross_bear_strength']])\n",
    "                tk_price_cross_length_bull = data[i][df_idx['tk_price_cross_bull_length']]\n",
    "                tk_price_cross_length_bear = data[i][df_idx['tk_price_cross_bear_length']]\n",
    "                \n",
    "                if not np.isnan(tk_price_cross_bull_strength) and tk_price_cross_bull_strength > 0:\n",
    "                    if vert_occupied:\n",
    "                        filler += '\\n'*3\n",
    "                    ax.axvline(x = i, color = colors['tk_price_cross'])\n",
    "                    ax.text(x = i, y = close, color = colors['tk_price_cross'],\n",
    "                            s = f'^ TK Price Cross Bull\\nstrength={tk_price_cross_bull_strength}'\n",
    "                                f'\\nlength={tk_price_cross_length_bull}{filler}')\n",
    "                    vert_occupied = True\n",
    "\n",
    "                if not np.isnan(tk_price_cross_bear_strength) and tk_price_cross_bear_strength > 0:\n",
    "                    if vert_occupied:\n",
    "                        filler += '\\n'*3\n",
    "                    ax.axvline(x = i, color = colors['tk_price_cross'])\n",
    "                    ax.text(x = i, y = close, color = colors['tk_price_cross'],\n",
    "                            s = f'_ TK Price Cross Bear\\nstrength={tk_price_cross_bear_strength}'\n",
    "                                f'\\nlength={tk_price_cross_length_bear}{filler}')\n",
    "                    vert_occupied = True\n",
    "            \n",
    "            # senkou cross\n",
    "            if 'senkou_cross' in crosses:\n",
    "                senkou_cross_bull_strength = abs(data[i][df_idx['senkou_cross_bull_strength']])\n",
    "                senkou_cross_bear_strength = abs(data[i][df_idx['senkou_cross_bear_strength']])\n",
    "                senkou_cross_length_bull = data[i][df_idx['senkou_cross_bull_length']]\n",
    "                senkou_cross_length_bear = data[i][df_idx['senkou_cross_bear_length']]\n",
    "                \n",
    "                if not np.isnan(senkou_cross_bull_strength) and senkou_cross_bull_strength > 0:\n",
    "                    if vert_occupied:\n",
    "                        filler += '\\n'*3\n",
    "                    ax.axvline(x = i, color = colors['senkou_cross'])\n",
    "                    ax.text(x = i, y = close, color = colors['senkou_cross'],\n",
    "                            s = f'^ Senkou Cross Bull\\nstrength={senkou_cross_bull_strength}'\n",
    "                                f'\\nlength={senkou_cross_length_bull}{filler}')\n",
    "                    vert_occupied = True\n",
    "\n",
    "                if not np.isnan(senkou_cross_bear_strength) and senkou_cross_bear_strength > 0:\n",
    "                    if vert_occupied:\n",
    "                        filler += '\\n'*3\n",
    "                    ax.axvline(x = i, color = colors['senkou_cross'])\n",
    "                    ax.text(x = i, y = close, color = colors['senkou_cross'],\n",
    "                            s = f'_ Senkou Cross Bear\\nstrength={senkou_cross_bear_strength}'\n",
    "                                f'\\nlength={senkou_cross_length_bear}{filler}')\n",
    "                    vert_occupied = True\n",
    "                \n",
    "            # chikou cross\n",
    "            if 'chikou_cross' in crosses:\n",
    "                chikou_cross_bull_strength = abs(data[i][df_idx['chikou_cross_bull_strength']])\n",
    "                chikou_cross_bear_strength = abs(data[i][df_idx['chikou_cross_bear_strength']])\n",
    "                chikou_cross_length_bull = data[i][df_idx['chikou_cross_bull_length']]\n",
    "                chikou_cross_length_bear = data[i][df_idx['chikou_cross_bear_length']]\n",
    "                \n",
    "                if not np.isnan(chikou_cross_bull_strength) and chikou_cross_bull_strength > 0:\n",
    "                    if vert_occupied:\n",
    "                        filler += '\\n'*3\n",
    "                    ax.axvline(x = i, color = colors['chikou_cross'])\n",
    "                    ax.text(x = i, y = close, color = colors['chikou_cross'],\n",
    "                            s = f'^ Chikou Cross Bull\\nstrength={chikou_cross_bull_strength}'\n",
    "                                f'\\nlength={chikou_cross_length_bull}{filler}')\n",
    "                    vert_occupied = True\n",
    "\n",
    "                if not np.isnan(chikou_cross_bear_strength) and chikou_cross_bear_strength > 0:\n",
    "                    if vert_occupied:\n",
    "                        filler += '\\n'*3\n",
    "                    ax.axvline(x = i, color = colors['chikou_cross'])\n",
    "                    ax.text(x = i, y = close, color = colors['chikou_cross'],\n",
    "                            s = f'_ Chikou Cross Bear\\nstrength={chikou_cross_bear_strength}'\n",
    "                                f'\\nlength={chikou_cross_length_bear}{filler}')\n",
    "                    vert_occupied = True\n",
    "            \n",
    "            # kumo breakout\n",
    "            if 'kumo_breakout' in crosses:\n",
    "                cloud_breakout_bull = data[i][df_idx['cloud_breakout_bull']]\n",
    "                cloud_breakout_bear = data[i][df_idx['cloud_breakout_bear']]\n",
    "                \n",
    "                if cloud_breakout_bull:\n",
    "                    if vert_occupied:\n",
    "                        filler += '\\n'*3\n",
    "                    ax.axvline(x = i, color = colors['kumo_breakout'])\n",
    "                    ax.text(x = i, y = close, color = colors['kumo_breakout'], s = f'^ Kumo Breakout Bullish{filler}')\n",
    "                    vert_occupied = True\n",
    "\n",
    "                if cloud_breakout_bear:\n",
    "                    if vert_occupied:\n",
    "                        filler += '\\n'*3\n",
    "                    ax.axvline(x = i, color = colors['kumo_breakout'])\n",
    "                    ax.text(x = i, y = close, color = colors['kumo_breakout'], s = f'_ Kumo Breakout Bearish{filler}')\n",
    "                    vert_occupied = True\n",
    "        \n",
    "\n",
    "def add_rsi_to_plot(ax, df):\n",
    "    ax.plot(df.momentum_rsi.to_list(), label='RSI', color='purple')\n",
    "    ax.plot([30]*len(df),color='gray',alpha=0.5)\n",
    "    ax.plot([70]*len(df),color='gray',alpha=0.5)\n",
    "    ax.fill_between(np.arange(len(df)),[30]*len(df),[70]*len(df),color='gray',alpha=0.2)\n",
    "    ax.set_ylim(15,85)\n",
    "    ax.set_yticks(np.arange(20,100,20))\n",
    "\n",
    "def add_labels_to_plot(ax, all_feat_df, labels_df, plot_range, labels=None):\n",
    "    if labels is None:\n",
    "        labels = set(['first_decision','ticks_till_best_profit_first_decision', 'best_profit_first_decision', 'profit_peak_first_decision',\n",
    "                      'second_decision', 'ticks_till_best_profit_second_decision', 'best_profit_second_decision', 'profit_peak_second_decision',\n",
    "                      'decision_pred','ticks_till_best_profit_decision_pred', 'best_profit_decision_pred', 'profit_peak_decision_pred'])\n",
    "    else:\n",
    "        labels = set(labels)\n",
    "    \n",
    "    colors = {\n",
    "        'buy': 'green',\n",
    "        'sell': 'red',\n",
    "    }\n",
    "    \n",
    "    start, stop = plot_range\n",
    "    plot_data_len = stop-start+1\n",
    "    \n",
    "    feat_data = all_feat_df.to_numpy()\n",
    "    labels_data = labels_df.to_numpy()\n",
    "    feat_df_idx = {all_feat_df.columns[i]: i for i in range(len(all_feat_df.columns))}\n",
    "    labels_df_idx = {labels_df.columns[i]: i for i in range(len(labels_df.columns))}\n",
    "    \n",
    "    verts_occupied = {}\n",
    "    for i in range(plot_data_len):\n",
    "        labels_i = i + start\n",
    "        close = feat_data[labels_i][feat_df_idx['Close']]            \n",
    "        \n",
    "        # the 1st and 2nd decisions should never occupy the same vert\n",
    "        printed_causes = False\n",
    "        for label_name in ['first_decision', 'second_decision', 'decision_pred']: \n",
    "            decision = None if label_name not in labels else labels_data[labels_i][labels_df_idx[label_name]]\n",
    "            if not pd.isnull(decision):\n",
    "                decision_type = 'true'\n",
    "                if label_name == 'decision_pred':\n",
    "                    decision_type = 'prediction'\n",
    "                \n",
    "                lines = 2\n",
    "                filler = '\\n'\n",
    "                if i in verts_occupied:\n",
    "                    filler = '\\n' * (verts_occupied[i] + 1)\n",
    "                \n",
    "                color = colors[decision]\n",
    "                txt = [f'{filler}---------------------------------------',\n",
    "                       f'{decision_type} {label_name}: {decision}']\n",
    "\n",
    "                if f'best_profit_{label_name}' in labels:\n",
    "                    profit = labels_data[labels_i][labels_df_idx[f'best_profit_{label_name}']] \n",
    "                    txt.append(f'best profit: {profit}')\n",
    "                    lines+=1\n",
    "\n",
    "                    if f'profit_peak_{label_name}' in labels:\n",
    "                        peak_idx = int(labels_data[labels_i][labels_df_idx[f'profit_peak_{label_name}']])\n",
    "                        plot_idx = peak_idx - start\n",
    "                        txt.append(f'best profit datetime: {feat_data[peak_idx][feat_df_idx[\"datetime\"]].strftime(\"%Y-%m-%d %H:%M\")}')\n",
    "                        lines+=1\n",
    "\n",
    "                        if plot_idx < plot_data_len:\n",
    "                            peak_close = feat_data[peak_idx][feat_df_idx['Close']]   \n",
    "                            ax.plot(plot_idx, peak_close, marker='o', markersize=12, color='black')\n",
    "                            filler_2 = '\\n'\n",
    "                            if plot_idx in verts_occupied:\n",
    "                                filler_2 = ' \\n' * (verts_occupied[plot_idx] + 1)\n",
    "                                verts_occupied[plot_idx] += 2\n",
    "                            else:\n",
    "                                verts_occupied[plot_idx] = 2\n",
    "                            ax.text(x=plot_idx, y=peak_close, color=color, verticalalignment='top',\n",
    "                                    s=f'{filler_2}closed {decision_type} {decision} from '\n",
    "                                      f'{feat_data[labels_i][feat_df_idx[\"datetime\"]].strftime(\"%Y-%m-%d %H:%M\")}\\nprofit: {profit}')\n",
    "\n",
    "                if f'ticks_till_best_profit_{label_name}' in labels:\n",
    "                    ticks = int(labels_data[labels_i][labels_df_idx[f'ticks_till_best_profit_{label_name}']]) \n",
    "                    txt.append(f'ticks till best: {ticks}')\n",
    "                    lines+=1\n",
    "\n",
    "                if 'causes' in labels and not printed_causes:\n",
    "                    causes = labels_data[labels_i][labels_df_idx['causes']] \n",
    "                    txt.append(f'causes: {causes}')\n",
    "                    printed_causes = True\n",
    "                    lines+=1\n",
    "                \n",
    "                txt = '\\n'.join(txt)\n",
    "                ax.plot(i, close, marker='o', markersize=12, color='black')\n",
    "                ax.text(x=i, y=close, color=color, verticalalignment='top',\n",
    "                        s=txt)\n",
    "            \n",
    "                if i in verts_occupied:\n",
    "                    verts_occupied[i] += lines\n",
    "                else:\n",
    "                    verts_occupied[i] = lines\n",
    "\n",
    "def add_extra_data_to_plot(ax, extra_df, plot_range):\n",
    "    start, stop = plot_range\n",
    "    extra_df = extra_df.iloc[start:stop+1]\n",
    "    for col in extra_df:\n",
    "        ax.plot(extra_df[col].to_numpy(), label=col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trying loading data from mt5 terminal w/ ForexMachine package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tick_data_filepath = research.download_mt5_data(\"EURUSD\", 'H1', '2012-01-02', '2020-06-06')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicators_info = {\n",
    "    'ichimoku': {\n",
    "        'tenkan_period': 9,\n",
    "        'kijun_period': 26,\n",
    "        'chikou_period': 26,\n",
    "        'senkou_b_period': 24\n",
    "    },\n",
    "    'rsi': {\n",
    "        'periods': 14\n",
    "    }\n",
    "}\n",
    "data_with_indicators_2 = research.add_indicators_to_raw(filepath=tick_data_filepath, \n",
    "                                                        indicators_info=indicators_info, \n",
    "                                                        datetime_col='datetime')\n",
    "data_with_ichi_2 = research.add_ichimoku_features(data_with_indicators_2)\n",
    "data_with_ichi_2.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crosses = ['tk_cross', 'tk_price_cross', 'senkou_cross', 'chikou_cross', 'kumo_breakout']\n",
    "crosses = ['kumo_breakout']\n",
    "show_data_from_range(data_with_ichi_2, '2019-01-01', '2019-02-04', main_indicator='ichimoku', sub_indicators=['rsi'], visualize_crosses=True, crosses=crosses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = research.save_data_with_indicators(data_with_ichi_2,filename=f'ichimoku_sigs-{tick_data_filepath.stem}')\n",
    "str(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test preprocessing funcs from ForexMachine package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81.49999999999999"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trades = {\n",
    "#     1: {\n",
    "#         'decision_label': 1,\n",
    "#         'lots': 0.11,\n",
    "#         'open_price': 1.22176,\n",
    "#     },\n",
    "#     2: {\n",
    "#         'decision_label': 1,\n",
    "#         'lots': 0.76,\n",
    "#         'open_price': 1.22175,\n",
    "#     },\n",
    "#     3: {\n",
    "#         'decision_label': 1,\n",
    "#         'lots': 0.14,\n",
    "#         'open_price': 1.22175,\n",
    "#     },\n",
    "#     4: {\n",
    "#         'decision_label': 0,\n",
    "#         'lots': 1.28,\n",
    "#         'open_price': 1.22169,\n",
    "#     },\n",
    "#     5: {\n",
    "#         'decision_label': 0,\n",
    "#         'lots': 0.55,\n",
    "#         'open_price': 1.22167,\n",
    "#     },\n",
    "# }\n",
    "\n",
    "# get_margin(trades, buy_label=1, sell_label=0, contract_size=100000, leverage=1000, tradersway_commodity=False, in_quote_currency=True, hedged_margin=50000)\n",
    "\n",
    "trades = {\n",
    "    1: {\n",
    "        'decision_label': 1,\n",
    "        'lots': 1.14,\n",
    "        'open_price': 1.27019,\n",
    "    },\n",
    "    2: {\n",
    "        'decision_label': 0,\n",
    "        'lots': 0.14,\n",
    "        'open_price': 1.27008,\n",
    "    },\n",
    "    3: {\n",
    "        'decision_label': 0,\n",
    "        'lots': 0.51,\n",
    "        'open_price': 1.27011,\n",
    "    },\n",
    "}\n",
    "\n",
    "research.get_margin(trades, buy_label=1, sell_label=0, contract_size=100000, leverage=1000, tradersway_commodity=False, in_quote_currency=False, hedged_margin=50000,\n",
    "                    trade_indices=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### try out different models w/ diff hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "XGBoost param tuning guide:\n",
    "https://towardsdatascience.com/fine-tuning-xgboost-in-python-like-a-boss-b4543ed8b1e\n",
    "\"\"\"\n",
    "\n",
    "contract_size = 100_000   # size of 1 lot is typically 100,000 (100 for gold, becuase 1 lot = 100 oz of gold)\n",
    "in_quote_currency = True\n",
    "pip_resolution = 0.0001\n",
    "labels_dict = {1: 'buy', 0: 'sell'}\n",
    "\n",
    "profit_percentages = [(pp/1000,pp/1000) for pp in range(1,101,3)]\n",
    "\n",
    "param_grid = {\n",
    "    'ichi_settings': [(9,26,52),(8,22,24),(9,30,60)],\n",
    "    'labeling_params': [{\n",
    "        'label_non_signals': [False],\n",
    "        'profit_percentages': profit_percentages,\n",
    "        'lots_per_trade': [0.2],\n",
    "    }],\n",
    "    'xgboost_params': [{\n",
    "        'n_estimators': [3000],\n",
    "        'max_depth': [2],\n",
    "        'learning_rate': [0.1],\n",
    "        'subsample': [1],\n",
    "        'colsample_bytree': [1],\n",
    "        'gamma': [1]\n",
    "    }]\n",
    "}\n",
    "\n",
    "param_grid = ParameterGrid(param_grid)\n",
    "param_grid = random.sample(list(param_grid), len(param_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filepath = research.download_mt5_data(\"EURUSD\", 'H1', '2012-01-02', '2020-12-18')\n",
    "train_split = 0.7\n",
    "results = []\n",
    "best_params_first_decision = None\n",
    "best_score_first_decision = None\n",
    "best_params_second_decision = None\n",
    "best_score_second_decision = None\n",
    "num_class = 3 # buy, sell, wait\n",
    "signals_to_consider = ['cloud_breakout_bull','cloud_breakout_bear',                       # cloud breakout\n",
    "                       'tk_cross_bull_strength', 'tk_cross_bear_strength',                # Tenkan Sen / Kijun Sen Cross\n",
    "                       'tk_price_cross_bull_strength', 'tk_price_cross_bear_strength',    # price crossing both the Tenkan Sen / Kijun Sen\n",
    "                       'senkou_cross_bull_strength', 'senkou_cross_bear_strength',        # Senkou Span Cross\n",
    "                       'chikou_cross_bull_strength', 'chikou_cross_bear_strength']        # Chikou Span Cross\n",
    "pc_cols = ['Open','High','Low','Close','Volume',\n",
    "           'trend_ichimoku_base','trend_ichimoku_conv',\n",
    "           'trend_ichimoku_a', 'trend_ichimoku_b']\n",
    "\n",
    "start_time = time.time()\n",
    "for i, params in enumerate(param_grid):\n",
    "    ichi_settings = params['ichi_settings']\n",
    "    labeling_params = params['labeling_params']\n",
    "    xgboost_params = params['xgboost_params']\n",
    "    \n",
    "    labeling_params = ParameterGrid(labeling_params)\n",
    "    labeling_params = random.sample(list(labeling_params), len(labeling_params))\n",
    "    xgboost_params = ParameterGrid(xgboost_params)\n",
    "    xgboost_params = random.sample(list(xgboost_params), len(xgboost_params))\n",
    "    \n",
    "    indicators_info = {\n",
    "        'ichimoku': {\n",
    "            'tenkan_period': ichi_settings[0],\n",
    "            'kijun_period': ichi_settings[1],\n",
    "            'chikou_period': ichi_settings[1],\n",
    "            'senkou_b_period': ichi_settings[2]\n",
    "        },\n",
    "        'rsi': {\n",
    "            'periods': 14\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # load in and split data\n",
    "    \n",
    "    data_with_ta_indicators = research.add_indicators_to_raw(filepath=filepath, \n",
    "                                                             indicators_info=indicators_info, \n",
    "                                                             datetime_col='datetime')\n",
    "    data_with_ichi_signals = research.add_ichimoku_features(data_with_ta_indicators)\n",
    "    start_idx, end_idx = no_missing_data_idx_range(data_with_ichi_signals, early_ending_cols=['chikou_span_visual'])\n",
    "    data_with_ichi_signals = data_with_ichi_signals[start_idx:].reset_index(drop=True)\n",
    "    \n",
    "    if train_split > 1:\n",
    "        print(f'train_split ({train_split}) is greater than 1, stopping.')\n",
    "    \n",
    "    train_p = train_split\n",
    "    num_rows = len(data_with_ichi_signals)\n",
    "    train_data_count = int(train_p * num_rows)\n",
    "    \n",
    "    train_data_orig = data_with_ichi_signals.iloc[:train_data_count]\n",
    "    validation_data_orig = data_with_ichi_signals.iloc[train_data_count:]\n",
    "    \n",
    "    for j, label_params in enumerate(labeling_params):\n",
    "        label_non_signals = label_params['label_non_signals']\n",
    "        min_profit_percent, profit_noise_percent = label_params['profit_percentages']\n",
    "        lots_per_trade = label_params['lots_per_trade']\n",
    "    \n",
    "        # generate labels for data\n",
    "\n",
    "        train_data_labels = generate_ichimoku_labels(train_data_orig, label_non_signals=label_non_signals, min_profit_percent=min_profit_percent, \n",
    "                                                     profit_noise_percent=profit_noise_percent, signals_to_consider=signals_to_consider, \n",
    "                                                     contract_size=contract_size, lots_per_trade=lots_per_trade,\n",
    "                                                     in_quote_currency=in_quote_currency,pip_resolution=pip_resolution, print_debug=False)\n",
    "        validation_data_labels = generate_ichimoku_labels(validation_data_orig, label_non_signals=label_non_signals, min_profit_percent=min_profit_percent, \n",
    "                                                          profit_noise_percent=profit_noise_percent, signals_to_consider=signals_to_consider, \n",
    "                                                          contract_size=contract_size, lots_per_trade=lots_per_trade,\n",
    "                                                          in_quote_currency=in_quote_currency, pip_resolution=pip_resolution, print_debug=False)\n",
    "        \n",
    "        train_data = apply_perc_change(train_data_orig, cols=pc_cols, limit=1)\n",
    "        start_idx, end_idx = no_missing_data_idx_range(train_data, early_ending_cols=['chikou_span_visual'])\n",
    "        train_data = train_data.iloc[start_idx:end_idx+1]\n",
    "        train_data_labels = train_data_labels.iloc[start_idx:end_idx+1]\n",
    "        \n",
    "        validation_data = apply_perc_change(validation_data_orig, cols=pc_cols, limit=1)\n",
    "        start_idx, end_idx = no_missing_data_idx_range(validation_data, early_ending_cols=['chikou_span_visual'])\n",
    "        validation_data = validation_data.iloc[start_idx:end_idx+1]\n",
    "        validation_data_labels = validation_data_labels.iloc[start_idx:end_idx+1]\n",
    "\n",
    "        x_train_first_decisions, y_train_first_decisions = missing_labels_preprocess(train_data, train_data_labels, 'first_decision')\n",
    "        x_valid_first_decisions, y_valid_first_decisions = missing_labels_preprocess(validation_data, validation_data_labels, 'first_decision')\n",
    "        x_train_first_decisions_profits, y_train_first_decisions_profits = missing_labels_preprocess(train_data, train_data_labels, \n",
    "                                                                                                     'best_profit_first_decision')\n",
    "        x_valid_first_decisions_profits, y_valid_first_decisions_profits = missing_labels_preprocess(validation_data, validation_data_labels, \n",
    "                                                                                                     'best_profit_first_decision')\n",
    "\n",
    "        x_train_second_decisions, y_train_second_decisions = missing_labels_preprocess(train_data, train_data_labels, 'second_decision')\n",
    "        x_valid_second_decisions, y_valid_second_decisions = missing_labels_preprocess(validation_data, validation_data_labels, 'second_decision')\n",
    "        x_train_second_decisions_profits, y_train_second_decisions_profits = missing_labels_preprocess(train_data, train_data_labels, \n",
    "                                                                                                       'best_profit_second_decision')\n",
    "        x_valid_second_decisions_profits, y_valid_second_decisions_profits = missing_labels_preprocess(validation_data, validation_data_labels, \n",
    "                                                                                                       'best_profit_second_decision')\n",
    "\n",
    "        # generate predictions w/ XGBoost model\n",
    "        for k, xgb_params in enumerate(xgboost_params):\n",
    "            n_estimators = xgb_params['n_estimators']\n",
    "            max_depth = xgb_params['max_depth']\n",
    "            learning_rate = xgb_params['learning_rate']\n",
    "            subsample = xgb_params['subsample']\n",
    "            colsample_bytree = xgb_params['colsample_bytree']\n",
    "            gamma = xgb_params['gamma']\n",
    "            \n",
    "            if min_profit_percent==profit_noise_percent:\n",
    "                # binrary classification problem (buy or sell)\n",
    "                error_metric_name = 'error'\n",
    "                xgb_params = {'max_depth':max_depth, 'learning_rate':learning_rate, 'objective':'binary:logistic', 'eval_metric': error_metric_name, \n",
    "                              'gamma':gamma, 'colsample_bytree':colsample_bytree, 'subsample':subsample}\n",
    "            else:\n",
    "                # multi-class classification problem (buy, sell, or wiat)\n",
    "                error_metric_name = 'merror'\n",
    "                xgb_params = {'max_depth':max_depth, 'learning_rate':learning_rate, 'objective':'multi:softmax', 'num_class': num_class,\n",
    "                              'eval_metric': error_metric_name, 'gamma':gamma, 'colsample_bytree':colsample_bytree, 'subsample':subsample}\n",
    "            \n",
    "            ### first decisions\n",
    "\n",
    "            y_train_true, labels_dict = convert_class_labels(y_train_first_decisions, labels_dict=labels_dict)\n",
    "            y_valid_true, labels_dict = convert_class_labels(y_valid_first_decisions, labels_dict=labels_dict)\n",
    "\n",
    "            dtrain = xgb.DMatrix(x_train_first_decisions, label=y_train_true)\n",
    "            dvalidation = [(xgb.DMatrix(x_train_first_decisions, label=y_train_true),'train'), \n",
    "                           (xgb.DMatrix(x_valid_first_decisions, label=y_valid_true),'validation')]\n",
    "            dtest = xgb.DMatrix(x_valid_first_decisions)\n",
    "            \n",
    "            evals_result = {}\n",
    "            decision_predictor = xgb.train(xgb_params, dtrain, num_boost_round=n_estimators, evals=dvalidation, \n",
    "                                           evals_result=evals_result, verbose_eval=False)\n",
    "            \n",
    "            train_error = evals_result['train'][error_metric_name][-1]\n",
    "            train_accuracy_first_decision = 1 - train_error\n",
    "            \n",
    "            validation_error = evals_result['validation'][error_metric_name][-1]\n",
    "            validation_accuracy_first_decision = 1 - validation_error\n",
    "            \n",
    "            y_test_probs = decision_predictor.predict(dtest)\n",
    "            y_test_preds = np.around(y_test_probs)\n",
    "            y_test_preds = pd.DataFrame(y_test_preds, columns=y_valid_true.columns)\n",
    "            y_test_preds = convert_class_labels(y_test_preds, to_ints=False, labels_dict=labels_dict)[0]\n",
    "            p_profits_first_decision = potention_profits(y_valid_first_decisions, y_test_preds, y_valid_first_decisions_profits)\n",
    "\n",
    "            ### second decisions\n",
    "\n",
    "            y_train_true, labels_dict = convert_class_labels(y_train_second_decisions, labels_dict=labels_dict)\n",
    "            y_valid_true, labels_dict = convert_class_labels(y_valid_second_decisions, labels_dict=labels_dict)\n",
    "\n",
    "            dtrain = xgb.DMatrix(x_train_second_decisions, label=y_train_true)\n",
    "            dvalidation = [(xgb.DMatrix(x_train_second_decisions, label=y_train_true),'train'), \n",
    "                           (xgb.DMatrix(x_valid_second_decisions, label=y_valid_true),'validation')]\n",
    "            dtest = xgb.DMatrix(x_valid_second_decisions)\n",
    "            \n",
    "            evals_result = {}\n",
    "            decision_predictor = xgb.train(xgb_params, dtrain, num_boost_round=n_estimators, evals=dvalidation, \n",
    "                                           evals_result=evals_result, verbose_eval=False)\n",
    "\n",
    "            train_error = evals_result['train'][error_metric_name][-1]\n",
    "            train_accuracy_second_decision = 1 - train_error\n",
    "            \n",
    "            validation_error = evals_result['validation'][error_metric_name][-1]\n",
    "            validation_accuracy_second_decision = 1 - validation_error\n",
    "            \n",
    "            y_test_probs = decision_predictor.predict(dtest)\n",
    "            y_test_preds = np.around(y_test_probs)\n",
    "            y_test_preds = pd.DataFrame(y_test_preds, columns=y_valid_true.columns)\n",
    "            y_test_preds = convert_class_labels(y_test_preds, to_ints=False, labels_dict=labels_dict)[0]\n",
    "            p_profits_second_decision = potention_profits(y_valid_second_decisions, y_test_preds, y_valid_second_decisions_profits)\n",
    "        \n",
    "            all_params = {\n",
    "                'tenkan_period': ichi_settings[0],\n",
    "                'kijun_period': ichi_settings[1],\n",
    "                'chikou_period': ichi_settings[1],\n",
    "                'senkou_b_period': ichi_settings[2],\n",
    "                'label_non_signals': label_non_signals,\n",
    "                'min_profit_percent': min_profit_percent,\n",
    "                'profit_noise_percent': profit_noise_percent,\n",
    "                'lots_per_trade': lots_per_trade,\n",
    "                'n_estimators': n_estimators,\n",
    "                'max_depth': max_depth,\n",
    "                'learning_rate': learning_rate,\n",
    "                'subsample': subsample,\n",
    "                'colsample_bytree': colsample_bytree,\n",
    "                'gamma': gamma,\n",
    "                'train_accuracy_first_decision': train_accuracy_first_decision,\n",
    "                'validation_accuracy_first_decision': validation_accuracy_first_decision,\n",
    "                'train_accuracy_second_decision': train_accuracy_second_decision,\n",
    "                'validation_accuracy_second_decision': validation_accuracy_second_decision,\n",
    "                'potention_profits_first_decision': p_profits_first_decision,\n",
    "                'potention_profits_second_decision': p_profits_second_decision\n",
    "            }\n",
    "            \n",
    "            first_decision_score = validation_accuracy_first_decision\n",
    "            second_decision_score = validation_accuracy_second_decision\n",
    "            \n",
    "            if not best_score_first_decision or first_decision_score > best_score_first_decision:\n",
    "                best_score_first_decision = first_decision_score\n",
    "                best_params_first_decision = all_params\n",
    "                \n",
    "            if not best_score_second_decision or second_decision_score > best_score_second_decision:\n",
    "                best_score_second_decision = second_decision_score\n",
    "                best_params_second_decision = all_params\n",
    "            \n",
    "            results.append(all_params)\n",
    "            \n",
    "            print('--------------------------------------------------------------------')\n",
    "            print(f'{k+1}/{len(xgboost_params)} xgb params evaulated')\n",
    "            print('--------------------------------------------------------------------\\n')\n",
    "            print(f'last params evaluated:')\n",
    "            print(f'{all_params}\\n')\n",
    "            print(f'best first decision params evaluated:')\n",
    "            print(f'{best_params_first_decision}\\n')\n",
    "            print(f'best second decision params evaluated:')\n",
    "            print(f'{best_params_second_decision}\\n')\n",
    "\n",
    "        print('--------------------------------------------------------------------')\n",
    "        print(f'{j+1}/{len(labeling_params)} labeling params evaulated')\n",
    "        print('--------------------------------------------------------------------\\n')\n",
    "        \n",
    "        results_sorted = sorted(results, key=lambda d: d['validation_accuracy_first_decision'], reverse=True)\n",
    "        results_sorted_df = pd.DataFrame(results_sorted)\n",
    "        results_sorted_df.to_csv('../my_stuff/grid_search_results.csv')\n",
    "        \n",
    "    print('--------------------------------------------------------------------')\n",
    "    print(f'{i+1}/{len(param_grid)} ichimoku settings evaulated')\n",
    "    print('--------------------------------------------------------------------\\n')\n",
    "    \n",
    "results_sorted = sorted(results, key=lambda d: d['validation_accuracy_first_decision'], reverse=True)\n",
    "results_sorted_df = pd.DataFrame(results_sorted)\n",
    "results_sorted_df.to_csv('../my_stuff/grid_search_results.csv')\n",
    "print(f'runtime: {(time.time()-start_time)/60} min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train model for backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 60431 rows of tick data from C:\\GitHub Repos\\ForexMachine\\Data\\.cache\\mt5_EURUSD_h1_ticks_2011-01-01T00;00UTC_to_2020-10-01T00;00UTC.csv\n",
      "saved 60431 rows of EURUSD h1 tick data to C:\\GitHub Repos\\ForexMachine\\Data\\RawData\\mt5_EURUSD_h1_ticks_2011-01-01T00;00UTC_to_2020-10-01T00;00UTC.csv, done.\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "\n",
    "label_non_signals = False\n",
    "min_profit_percent, profit_noise_percent = 0.01, 0.01\n",
    "contract_size = 100_000   # size of 1 lot is typically 100,000 (100 for gold, becuase 1 lot = 100 oz of gold)\n",
    "lots_per_trade = 0.2  \n",
    "currency_side = 'right'\n",
    "in_quote_currency = True if currency_side == 'right' else False\n",
    "pip_resolution = 0.0001\n",
    "\n",
    "labels_dict = {1: 'buy', 0: 'sell'}\n",
    "n_estimators = 3000\n",
    "max_depth = 2\n",
    "learning_rate = 0.1\n",
    "subsample = 1\n",
    "colsample_bytree = 1\n",
    "gamma = 1\n",
    "tenkan_period = 9\n",
    "kijun_period = 30\n",
    "senkou_b_period = 60\n",
    "indicators_info = {\n",
    "    'ichimoku': {\n",
    "        'tenkan_period': tenkan_period,\n",
    "        'kijun_period': kijun_period,\n",
    "        'chikou_period': kijun_period,\n",
    "        'senkou_b_period': senkou_b_period\n",
    "    },\n",
    "    'rsi': {\n",
    "        'periods': 14\n",
    "    }\n",
    "}\n",
    "\n",
    "signals_to_consider = ['cloud_breakout_bull','cloud_breakout_bear',                       # cloud breakout\n",
    "                       'tk_cross_bull_strength', 'tk_cross_bear_strength',                # Tenkan Sen / Kijun Sen Cross\n",
    "                       'tk_price_cross_bull_strength', 'tk_price_cross_bear_strength',    # price crossing both the Tenkan Sen / Kijun Sen\n",
    "                       'senkou_cross_bull_strength', 'senkou_cross_bear_strength',        # Senkou Span Cross\n",
    "                       'chikou_cross_bull_strength', 'chikou_cross_bear_strength']        # Chikou Span Cross\n",
    "sigs_for_filename = 'cb-tk-tkp-sen-chi'\n",
    "\n",
    "# get data\n",
    "\n",
    "cur_pair = 'EURUSD'\n",
    "timeframe = 'H1'\n",
    "tick_data_filepath = research.download_mt5_data(cur_pair, timeframe, global_train_data_range_start, global_train_data_range_end)\n",
    "data_with_indicators = research.add_indicators_to_raw(filepath=tick_data_filepath, \n",
    "                                                      indicators_info=indicators_info, \n",
    "                                                      datetime_col='datetime')\n",
    "train_data = research.add_ichimoku_features(data_with_indicators)\n",
    "\n",
    "train_data_labels = generate_ichimoku_labels(train_data, label_non_signals=label_non_signals, min_profit_percent=min_profit_percent, \n",
    "                                             profit_noise_percent=profit_noise_percent, signals_to_consider=signals_to_consider, \n",
    "                                             contract_size=contract_size, lots_per_trade=lots_per_trade,\n",
    "                                             in_quote_currency=in_quote_currency,pip_resolution=pip_resolution)\n",
    "\n",
    "pc_cols = ['Open','High','Low','Close','Volume',\n",
    "           'trend_ichimoku_base','trend_ichimoku_conv',\n",
    "           'trend_ichimoku_a', 'trend_ichimoku_b']\n",
    "train_data = apply_perc_change(train_data, cols=pc_cols, limit=1)\n",
    "start_idx, end_idx = no_missing_data_idx_range(train_data)\n",
    "train_data = train_data.iloc[start_idx:end_idx+1]\n",
    "train_data_labels = train_data_labels.iloc[start_idx:end_idx+1]\n",
    "\n",
    "x_train_first_decisions, y_train_first_decisions = missing_labels_preprocess(train_data, train_data_labels, 'first_decision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-error:0.45984\n",
      "[1]\ttrain-error:0.45647\n",
      "[2]\ttrain-error:0.45815\n",
      "[3]\ttrain-error:0.45771\n",
      "[4]\ttrain-error:0.45984\n",
      "[5]\ttrain-error:0.45771\n",
      "[6]\ttrain-error:0.45771\n",
      "[7]\ttrain-error:0.45753\n",
      "[8]\ttrain-error:0.45771\n",
      "[9]\ttrain-error:0.45584\n",
      "[10]\ttrain-error:0.45771\n",
      "[11]\ttrain-error:0.45815\n",
      "[12]\ttrain-error:0.45735\n",
      "[13]\ttrain-error:0.45718\n",
      "[14]\ttrain-error:0.45744\n",
      "[15]\ttrain-error:0.45744\n",
      "[16]\ttrain-error:0.45744\n",
      "[17]\ttrain-error:0.45620\n",
      "[18]\ttrain-error:0.45593\n",
      "[19]\ttrain-error:0.45522\n",
      "[20]\ttrain-error:0.45238\n",
      "[21]\ttrain-error:0.45274\n",
      "[22]\ttrain-error:0.45354\n",
      "[23]\ttrain-error:0.45327\n",
      "[24]\ttrain-error:0.45336\n",
      "[25]\ttrain-error:0.45425\n",
      "[26]\ttrain-error:0.45363\n",
      "[27]\ttrain-error:0.45300\n",
      "[28]\ttrain-error:0.45150\n",
      "[29]\ttrain-error:0.45123\n",
      "[30]\ttrain-error:0.44732\n",
      "[31]\ttrain-error:0.44697\n",
      "[32]\ttrain-error:0.44510\n",
      "[33]\ttrain-error:0.44555\n",
      "[34]\ttrain-error:0.44484\n",
      "[35]\ttrain-error:0.44413\n",
      "[36]\ttrain-error:0.44457\n",
      "[37]\ttrain-error:0.44448\n",
      "[38]\ttrain-error:0.44546\n",
      "[39]\ttrain-error:0.44413\n",
      "[40]\ttrain-error:0.44280\n",
      "[41]\ttrain-error:0.44164\n",
      "[42]\ttrain-error:0.44262\n",
      "[43]\ttrain-error:0.44102\n",
      "[44]\ttrain-error:0.44049\n",
      "[45]\ttrain-error:0.44040\n",
      "[46]\ttrain-error:0.43916\n",
      "[47]\ttrain-error:0.43738\n",
      "[48]\ttrain-error:0.43747\n",
      "[49]\ttrain-error:0.43747\n",
      "[50]\ttrain-error:0.43650\n",
      "[51]\ttrain-error:0.43659\n",
      "[52]\ttrain-error:0.43729\n",
      "[53]\ttrain-error:0.43712\n",
      "[54]\ttrain-error:0.43756\n",
      "[55]\ttrain-error:0.43694\n",
      "[56]\ttrain-error:0.43694\n",
      "[57]\ttrain-error:0.43588\n",
      "[58]\ttrain-error:0.43499\n",
      "[59]\ttrain-error:0.43463\n",
      "[60]\ttrain-error:0.43383\n",
      "[61]\ttrain-error:0.43401\n",
      "[62]\ttrain-error:0.43330\n",
      "[63]\ttrain-error:0.43401\n",
      "[64]\ttrain-error:0.43357\n",
      "[65]\ttrain-error:0.43339\n",
      "[66]\ttrain-error:0.43304\n",
      "[67]\ttrain-error:0.43197\n",
      "[68]\ttrain-error:0.43153\n",
      "[69]\ttrain-error:0.43055\n",
      "[70]\ttrain-error:0.43046\n",
      "[71]\ttrain-error:0.42993\n",
      "[72]\ttrain-error:0.43002\n",
      "[73]\ttrain-error:0.42993\n",
      "[74]\ttrain-error:0.42922\n",
      "[75]\ttrain-error:0.42895\n",
      "[76]\ttrain-error:0.42824\n",
      "[77]\ttrain-error:0.42815\n",
      "[78]\ttrain-error:0.42682\n",
      "[79]\ttrain-error:0.42700\n",
      "[80]\ttrain-error:0.42602\n",
      "[81]\ttrain-error:0.42416\n",
      "[82]\ttrain-error:0.42540\n",
      "[83]\ttrain-error:0.42513\n",
      "[84]\ttrain-error:0.42496\n",
      "[85]\ttrain-error:0.42389\n",
      "[86]\ttrain-error:0.42398\n",
      "[87]\ttrain-error:0.42265\n",
      "[88]\ttrain-error:0.42150\n",
      "[89]\ttrain-error:0.42398\n",
      "[90]\ttrain-error:0.42372\n",
      "[91]\ttrain-error:0.42372\n",
      "[92]\ttrain-error:0.42363\n",
      "[93]\ttrain-error:0.42363\n",
      "[94]\ttrain-error:0.42256\n",
      "[95]\ttrain-error:0.42229\n",
      "[96]\ttrain-error:0.42229\n",
      "[97]\ttrain-error:0.42167\n",
      "[98]\ttrain-error:0.42123\n",
      "[99]\ttrain-error:0.42052\n",
      "[100]\ttrain-error:0.42008\n",
      "[101]\ttrain-error:0.42008\n",
      "[102]\ttrain-error:0.41937\n",
      "[103]\ttrain-error:0.41928\n",
      "[104]\ttrain-error:0.41901\n",
      "[105]\ttrain-error:0.41946\n",
      "[106]\ttrain-error:0.41874\n",
      "[107]\ttrain-error:0.41874\n",
      "[108]\ttrain-error:0.41812\n",
      "[109]\ttrain-error:0.41830\n",
      "[110]\ttrain-error:0.41803\n",
      "[111]\ttrain-error:0.41821\n",
      "[112]\ttrain-error:0.41803\n",
      "[113]\ttrain-error:0.41795\n",
      "[114]\ttrain-error:0.41768\n",
      "[115]\ttrain-error:0.41688\n",
      "[116]\ttrain-error:0.41715\n",
      "[117]\ttrain-error:0.41741\n",
      "[118]\ttrain-error:0.41733\n",
      "[119]\ttrain-error:0.41715\n",
      "[120]\ttrain-error:0.41715\n",
      "[121]\ttrain-error:0.41697\n",
      "[122]\ttrain-error:0.41653\n",
      "[123]\ttrain-error:0.41653\n",
      "[124]\ttrain-error:0.41635\n",
      "[125]\ttrain-error:0.41599\n",
      "[126]\ttrain-error:0.41564\n",
      "[127]\ttrain-error:0.41511\n",
      "[128]\ttrain-error:0.41475\n",
      "[129]\ttrain-error:0.41333\n",
      "[130]\ttrain-error:0.41351\n",
      "[131]\ttrain-error:0.41378\n",
      "[132]\ttrain-error:0.41360\n",
      "[133]\ttrain-error:0.41191\n",
      "[134]\ttrain-error:0.41209\n",
      "[135]\ttrain-error:0.41182\n",
      "[136]\ttrain-error:0.41191\n",
      "[137]\ttrain-error:0.41218\n",
      "[138]\ttrain-error:0.41200\n",
      "[139]\ttrain-error:0.41191\n",
      "[140]\ttrain-error:0.41173\n",
      "[141]\ttrain-error:0.41093\n",
      "[142]\ttrain-error:0.41076\n",
      "[143]\ttrain-error:0.41085\n",
      "[144]\ttrain-error:0.41076\n",
      "[145]\ttrain-error:0.41058\n",
      "[146]\ttrain-error:0.41067\n",
      "[147]\ttrain-error:0.40996\n",
      "[148]\ttrain-error:0.40880\n",
      "[149]\ttrain-error:0.40943\n",
      "[150]\ttrain-error:0.40934\n",
      "[151]\ttrain-error:0.40845\n",
      "[152]\ttrain-error:0.40889\n",
      "[153]\ttrain-error:0.40898\n",
      "[154]\ttrain-error:0.40747\n",
      "[155]\ttrain-error:0.40783\n",
      "[156]\ttrain-error:0.40721\n",
      "[157]\ttrain-error:0.40818\n",
      "[158]\ttrain-error:0.40809\n",
      "[159]\ttrain-error:0.40809\n",
      "[160]\ttrain-error:0.40765\n",
      "[161]\ttrain-error:0.40730\n",
      "[162]\ttrain-error:0.40738\n",
      "[163]\ttrain-error:0.40588\n",
      "[164]\ttrain-error:0.40490\n",
      "[165]\ttrain-error:0.40428\n",
      "[166]\ttrain-error:0.40472\n",
      "[167]\ttrain-error:0.40490\n",
      "[168]\ttrain-error:0.40481\n",
      "[169]\ttrain-error:0.40463\n",
      "[170]\ttrain-error:0.40463\n",
      "[171]\ttrain-error:0.40525\n",
      "[172]\ttrain-error:0.40508\n",
      "[173]\ttrain-error:0.40517\n",
      "[174]\ttrain-error:0.40454\n",
      "[175]\ttrain-error:0.40428\n",
      "[176]\ttrain-error:0.40419\n",
      "[177]\ttrain-error:0.40383\n",
      "[178]\ttrain-error:0.40383\n",
      "[179]\ttrain-error:0.40348\n",
      "[180]\ttrain-error:0.40303\n",
      "[181]\ttrain-error:0.40303\n",
      "[182]\ttrain-error:0.40206\n",
      "[183]\ttrain-error:0.40197\n",
      "[184]\ttrain-error:0.40197\n",
      "[185]\ttrain-error:0.40188\n",
      "[186]\ttrain-error:0.40037\n",
      "[187]\ttrain-error:0.40073\n",
      "[188]\ttrain-error:0.40073\n",
      "[189]\ttrain-error:0.40055\n",
      "[190]\ttrain-error:0.40046\n",
      "[191]\ttrain-error:0.40020\n",
      "[192]\ttrain-error:0.39975\n",
      "[193]\ttrain-error:0.39993\n",
      "[194]\ttrain-error:0.39975\n",
      "[195]\ttrain-error:0.39957\n",
      "[196]\ttrain-error:0.39966\n",
      "[197]\ttrain-error:0.39966\n",
      "[198]\ttrain-error:0.39975\n",
      "[199]\ttrain-error:0.39966\n",
      "[200]\ttrain-error:0.39975\n",
      "[201]\ttrain-error:0.40002\n",
      "[202]\ttrain-error:0.39975\n",
      "[203]\ttrain-error:0.39975\n",
      "[204]\ttrain-error:0.40028\n",
      "[205]\ttrain-error:0.39984\n",
      "[206]\ttrain-error:0.40011\n",
      "[207]\ttrain-error:0.39948\n",
      "[208]\ttrain-error:0.39753\n",
      "[209]\ttrain-error:0.39744\n",
      "[210]\ttrain-error:0.39700\n",
      "[211]\ttrain-error:0.39665\n",
      "[212]\ttrain-error:0.39665\n",
      "[213]\ttrain-error:0.39673\n",
      "[214]\ttrain-error:0.39620\n",
      "[215]\ttrain-error:0.39629\n",
      "[216]\ttrain-error:0.39593\n",
      "[217]\ttrain-error:0.39576\n",
      "[218]\ttrain-error:0.39620\n",
      "[219]\ttrain-error:0.39576\n",
      "[220]\ttrain-error:0.39549\n",
      "[221]\ttrain-error:0.39505\n",
      "[222]\ttrain-error:0.39398\n",
      "[223]\ttrain-error:0.39425\n",
      "[224]\ttrain-error:0.39301\n",
      "[225]\ttrain-error:0.39327\n",
      "[226]\ttrain-error:0.39354\n",
      "[227]\ttrain-error:0.39336\n",
      "[228]\ttrain-error:0.39372\n",
      "[229]\ttrain-error:0.39292\n",
      "[230]\ttrain-error:0.39247\n",
      "[231]\ttrain-error:0.39265\n",
      "[232]\ttrain-error:0.39265\n",
      "[233]\ttrain-error:0.39221\n",
      "[234]\ttrain-error:0.39114\n",
      "[235]\ttrain-error:0.39088\n",
      "[236]\ttrain-error:0.39061\n",
      "[237]\ttrain-error:0.38990\n",
      "[238]\ttrain-error:0.38981\n",
      "[239]\ttrain-error:0.38954\n",
      "[240]\ttrain-error:0.38937\n",
      "[241]\ttrain-error:0.38954\n",
      "[242]\ttrain-error:0.38954\n",
      "[243]\ttrain-error:0.38928\n",
      "[244]\ttrain-error:0.38928\n",
      "[245]\ttrain-error:0.38910\n",
      "[246]\ttrain-error:0.38883\n",
      "[247]\ttrain-error:0.38875\n",
      "[248]\ttrain-error:0.38875\n",
      "[249]\ttrain-error:0.38910\n",
      "[250]\ttrain-error:0.38795\n",
      "[251]\ttrain-error:0.38795\n",
      "[252]\ttrain-error:0.38804\n",
      "[253]\ttrain-error:0.38777\n",
      "[254]\ttrain-error:0.38777\n",
      "[255]\ttrain-error:0.38812\n",
      "[256]\ttrain-error:0.38724\n",
      "[257]\ttrain-error:0.38653\n",
      "[258]\ttrain-error:0.38599\n",
      "[259]\ttrain-error:0.38591\n",
      "[260]\ttrain-error:0.38599\n",
      "[261]\ttrain-error:0.38653\n",
      "[262]\ttrain-error:0.38644\n",
      "[263]\ttrain-error:0.38671\n",
      "[264]\ttrain-error:0.38635\n",
      "[265]\ttrain-error:0.38555\n",
      "[266]\ttrain-error:0.38511\n",
      "[267]\ttrain-error:0.38431\n",
      "[268]\ttrain-error:0.38395\n",
      "[269]\ttrain-error:0.38378\n",
      "[270]\ttrain-error:0.38449\n",
      "[271]\ttrain-error:0.38360\n",
      "[272]\ttrain-error:0.38378\n",
      "[273]\ttrain-error:0.38324\n",
      "[274]\ttrain-error:0.38218\n",
      "[275]\ttrain-error:0.38200\n",
      "[276]\ttrain-error:0.38147\n",
      "[277]\ttrain-error:0.38067\n",
      "[278]\ttrain-error:0.38005\n",
      "[279]\ttrain-error:0.37996\n",
      "[280]\ttrain-error:0.38022\n",
      "[281]\ttrain-error:0.37996\n",
      "[282]\ttrain-error:0.37996\n",
      "[283]\ttrain-error:0.37943\n",
      "[284]\ttrain-error:0.37934\n",
      "[285]\ttrain-error:0.37951\n",
      "[286]\ttrain-error:0.37907\n",
      "[287]\ttrain-error:0.37916\n",
      "[288]\ttrain-error:0.37916\n",
      "[289]\ttrain-error:0.37960\n",
      "[290]\ttrain-error:0.38040\n",
      "[291]\ttrain-error:0.38049\n",
      "[292]\ttrain-error:0.38058\n",
      "[293]\ttrain-error:0.38049\n",
      "[294]\ttrain-error:0.38014\n",
      "[295]\ttrain-error:0.38014\n",
      "[296]\ttrain-error:0.38005\n",
      "[297]\ttrain-error:0.38005\n",
      "[298]\ttrain-error:0.37934\n",
      "[299]\ttrain-error:0.37898\n",
      "[300]\ttrain-error:0.37916\n",
      "[301]\ttrain-error:0.37943\n",
      "[302]\ttrain-error:0.37925\n",
      "[303]\ttrain-error:0.37845\n",
      "[304]\ttrain-error:0.37818\n",
      "[305]\ttrain-error:0.37774\n",
      "[306]\ttrain-error:0.37792\n",
      "[307]\ttrain-error:0.37801\n",
      "[308]\ttrain-error:0.37810\n",
      "[309]\ttrain-error:0.37801\n",
      "[310]\ttrain-error:0.37801\n",
      "[311]\ttrain-error:0.37756\n",
      "[312]\ttrain-error:0.37739\n",
      "[313]\ttrain-error:0.37739\n",
      "[314]\ttrain-error:0.37747\n",
      "[315]\ttrain-error:0.37747\n",
      "[316]\ttrain-error:0.37721\n",
      "[317]\ttrain-error:0.37623\n",
      "[318]\ttrain-error:0.37623\n",
      "[319]\ttrain-error:0.37667\n",
      "[320]\ttrain-error:0.37694\n",
      "[321]\ttrain-error:0.37650\n",
      "[322]\ttrain-error:0.37650\n",
      "[323]\ttrain-error:0.37632\n",
      "[324]\ttrain-error:0.37561\n",
      "[325]\ttrain-error:0.37552\n",
      "[326]\ttrain-error:0.37667\n",
      "[327]\ttrain-error:0.37605\n",
      "[328]\ttrain-error:0.37543\n",
      "[329]\ttrain-error:0.37455\n",
      "[330]\ttrain-error:0.37437\n",
      "[331]\ttrain-error:0.37392\n",
      "[332]\ttrain-error:0.37304\n",
      "[333]\ttrain-error:0.37259\n",
      "[334]\ttrain-error:0.37250\n",
      "[335]\ttrain-error:0.37215\n",
      "[336]\ttrain-error:0.37197\n",
      "[337]\ttrain-error:0.37188\n",
      "[338]\ttrain-error:0.37215\n",
      "[339]\ttrain-error:0.37171\n",
      "[340]\ttrain-error:0.37144\n",
      "[341]\ttrain-error:0.37179\n",
      "[342]\ttrain-error:0.37126\n",
      "[343]\ttrain-error:0.37135\n",
      "[344]\ttrain-error:0.37135\n",
      "[345]\ttrain-error:0.37153\n",
      "[346]\ttrain-error:0.37171\n",
      "[347]\ttrain-error:0.37241\n",
      "[348]\ttrain-error:0.37241\n",
      "[349]\ttrain-error:0.37233\n",
      "[350]\ttrain-error:0.37144\n",
      "[351]\ttrain-error:0.37117\n",
      "[352]\ttrain-error:0.37135\n",
      "[353]\ttrain-error:0.37126\n",
      "[354]\ttrain-error:0.37126\n",
      "[355]\ttrain-error:0.37162\n",
      "[356]\ttrain-error:0.37153\n",
      "[357]\ttrain-error:0.37046\n",
      "[358]\ttrain-error:0.37091\n",
      "[359]\ttrain-error:0.37135\n",
      "[360]\ttrain-error:0.37135\n",
      "[361]\ttrain-error:0.37091\n",
      "[362]\ttrain-error:0.37073\n",
      "[363]\ttrain-error:0.37020\n",
      "[364]\ttrain-error:0.36966\n",
      "[365]\ttrain-error:0.36966\n",
      "[366]\ttrain-error:0.36966\n",
      "[367]\ttrain-error:0.36975\n",
      "[368]\ttrain-error:0.36966\n",
      "[369]\ttrain-error:0.36966\n",
      "[370]\ttrain-error:0.37011\n",
      "[371]\ttrain-error:0.37002\n",
      "[372]\ttrain-error:0.36984\n",
      "[373]\ttrain-error:0.36993\n",
      "[374]\ttrain-error:0.36957\n",
      "[375]\ttrain-error:0.36975\n",
      "[376]\ttrain-error:0.36940\n",
      "[377]\ttrain-error:0.36913\n",
      "[378]\ttrain-error:0.36940\n",
      "[379]\ttrain-error:0.36966\n",
      "[380]\ttrain-error:0.36904\n",
      "[381]\ttrain-error:0.36895\n",
      "[382]\ttrain-error:0.36886\n",
      "[383]\ttrain-error:0.36860\n",
      "[384]\ttrain-error:0.36904\n",
      "[385]\ttrain-error:0.36904\n",
      "[386]\ttrain-error:0.36878\n",
      "[387]\ttrain-error:0.36860\n",
      "[388]\ttrain-error:0.36807\n",
      "[389]\ttrain-error:0.36824\n",
      "[390]\ttrain-error:0.36798\n",
      "[391]\ttrain-error:0.36771\n",
      "[392]\ttrain-error:0.36745\n",
      "[393]\ttrain-error:0.36709\n",
      "[394]\ttrain-error:0.36700\n",
      "[395]\ttrain-error:0.36718\n",
      "[396]\ttrain-error:0.36665\n",
      "[397]\ttrain-error:0.36727\n",
      "[398]\ttrain-error:0.36727\n",
      "[399]\ttrain-error:0.36709\n",
      "[400]\ttrain-error:0.36682\n",
      "[401]\ttrain-error:0.36691\n",
      "[402]\ttrain-error:0.36620\n",
      "[403]\ttrain-error:0.36700\n",
      "[404]\ttrain-error:0.36762\n",
      "[405]\ttrain-error:0.36798\n",
      "[406]\ttrain-error:0.36798\n",
      "[407]\ttrain-error:0.36816\n",
      "[408]\ttrain-error:0.36816\n",
      "[409]\ttrain-error:0.36718\n",
      "[410]\ttrain-error:0.36727\n",
      "[411]\ttrain-error:0.36753\n",
      "[412]\ttrain-error:0.36647\n",
      "[413]\ttrain-error:0.36647\n",
      "[414]\ttrain-error:0.36638\n",
      "[415]\ttrain-error:0.36647\n",
      "[416]\ttrain-error:0.36629\n",
      "[417]\ttrain-error:0.36567\n",
      "[418]\ttrain-error:0.36514\n",
      "[419]\ttrain-error:0.36567\n",
      "[420]\ttrain-error:0.36585\n",
      "[421]\ttrain-error:0.36585\n",
      "[422]\ttrain-error:0.36558\n",
      "[423]\ttrain-error:0.36585\n",
      "[424]\ttrain-error:0.36576\n",
      "[425]\ttrain-error:0.36594\n",
      "[426]\ttrain-error:0.36469\n",
      "[427]\ttrain-error:0.36381\n",
      "[428]\ttrain-error:0.36363\n",
      "[429]\ttrain-error:0.36345\n",
      "[430]\ttrain-error:0.36398\n",
      "[431]\ttrain-error:0.36390\n",
      "[432]\ttrain-error:0.36416\n",
      "[433]\ttrain-error:0.36390\n",
      "[434]\ttrain-error:0.36363\n",
      "[435]\ttrain-error:0.36292\n",
      "[436]\ttrain-error:0.36283\n",
      "[437]\ttrain-error:0.36150\n",
      "[438]\ttrain-error:0.36230\n",
      "[439]\ttrain-error:0.36221\n",
      "[440]\ttrain-error:0.36203\n",
      "[441]\ttrain-error:0.36203\n",
      "[442]\ttrain-error:0.36194\n",
      "[443]\ttrain-error:0.36185\n",
      "[444]\ttrain-error:0.36185\n",
      "[445]\ttrain-error:0.36168\n",
      "[446]\ttrain-error:0.36292\n",
      "[447]\ttrain-error:0.36239\n",
      "[448]\ttrain-error:0.36292\n",
      "[449]\ttrain-error:0.36283\n",
      "[450]\ttrain-error:0.36212\n",
      "[451]\ttrain-error:0.36168\n",
      "[452]\ttrain-error:0.36168\n",
      "[453]\ttrain-error:0.36105\n",
      "[454]\ttrain-error:0.36061\n",
      "[455]\ttrain-error:0.35990\n",
      "[456]\ttrain-error:0.36043\n",
      "[457]\ttrain-error:0.35981\n",
      "[458]\ttrain-error:0.36026\n",
      "[459]\ttrain-error:0.36061\n",
      "[460]\ttrain-error:0.36034\n",
      "[461]\ttrain-error:0.36043\n",
      "[462]\ttrain-error:0.36017\n",
      "[463]\ttrain-error:0.35999\n",
      "[464]\ttrain-error:0.35972\n",
      "[465]\ttrain-error:0.35990\n",
      "[466]\ttrain-error:0.35990\n",
      "[467]\ttrain-error:0.35999\n",
      "[468]\ttrain-error:0.35981\n",
      "[469]\ttrain-error:0.35963\n",
      "[470]\ttrain-error:0.35990\n",
      "[471]\ttrain-error:0.35990\n",
      "[472]\ttrain-error:0.35875\n",
      "[473]\ttrain-error:0.35866\n",
      "[474]\ttrain-error:0.35839\n",
      "[475]\ttrain-error:0.35848\n",
      "[476]\ttrain-error:0.35813\n",
      "[477]\ttrain-error:0.35706\n",
      "[478]\ttrain-error:0.35733\n",
      "[479]\ttrain-error:0.35715\n",
      "[480]\ttrain-error:0.35679\n",
      "[481]\ttrain-error:0.35617\n",
      "[482]\ttrain-error:0.35582\n",
      "[483]\ttrain-error:0.35600\n",
      "[484]\ttrain-error:0.35546\n",
      "[485]\ttrain-error:0.35546\n",
      "[486]\ttrain-error:0.35573\n",
      "[487]\ttrain-error:0.35546\n",
      "[488]\ttrain-error:0.35520\n",
      "[489]\ttrain-error:0.35529\n",
      "[490]\ttrain-error:0.35511\n",
      "[491]\ttrain-error:0.35484\n",
      "[492]\ttrain-error:0.35493\n",
      "[493]\ttrain-error:0.35466\n",
      "[494]\ttrain-error:0.35466\n",
      "[495]\ttrain-error:0.35422\n",
      "[496]\ttrain-error:0.35440\n",
      "[497]\ttrain-error:0.35440\n",
      "[498]\ttrain-error:0.35440\n",
      "[499]\ttrain-error:0.35449\n",
      "[500]\ttrain-error:0.35431\n",
      "[501]\ttrain-error:0.35440\n",
      "[502]\ttrain-error:0.35422\n",
      "[503]\ttrain-error:0.35395\n",
      "[504]\ttrain-error:0.35395\n",
      "[505]\ttrain-error:0.35395\n",
      "[506]\ttrain-error:0.35404\n",
      "[507]\ttrain-error:0.35422\n",
      "[508]\ttrain-error:0.35395\n",
      "[509]\ttrain-error:0.35386\n",
      "[510]\ttrain-error:0.35378\n",
      "[511]\ttrain-error:0.35386\n",
      "[512]\ttrain-error:0.35404\n",
      "[513]\ttrain-error:0.35315\n",
      "[514]\ttrain-error:0.35289\n",
      "[515]\ttrain-error:0.35271\n",
      "[516]\ttrain-error:0.35262\n",
      "[517]\ttrain-error:0.35245\n",
      "[518]\ttrain-error:0.35245\n",
      "[519]\ttrain-error:0.35245\n",
      "[520]\ttrain-error:0.35245\n",
      "[521]\ttrain-error:0.35245\n",
      "[522]\ttrain-error:0.35245\n",
      "[523]\ttrain-error:0.35245\n",
      "[524]\ttrain-error:0.35245\n",
      "[525]\ttrain-error:0.35245\n",
      "[526]\ttrain-error:0.35245\n",
      "[527]\ttrain-error:0.35245\n",
      "[528]\ttrain-error:0.35245\n",
      "[529]\ttrain-error:0.35245\n",
      "[530]\ttrain-error:0.35245\n",
      "[531]\ttrain-error:0.35245\n",
      "[532]\ttrain-error:0.35245\n",
      "[533]\ttrain-error:0.35245\n",
      "[534]\ttrain-error:0.35245\n",
      "[535]\ttrain-error:0.35245\n",
      "[536]\ttrain-error:0.35245\n",
      "[537]\ttrain-error:0.35245\n",
      "[538]\ttrain-error:0.35245\n",
      "[539]\ttrain-error:0.35245\n",
      "[540]\ttrain-error:0.35245\n",
      "[541]\ttrain-error:0.35245\n",
      "[542]\ttrain-error:0.35245\n",
      "[543]\ttrain-error:0.35245\n",
      "[544]\ttrain-error:0.35245\n",
      "[545]\ttrain-error:0.35245\n",
      "[546]\ttrain-error:0.35245\n",
      "[547]\ttrain-error:0.35245\n",
      "[548]\ttrain-error:0.35245\n",
      "[549]\ttrain-error:0.35245\n",
      "[550]\ttrain-error:0.35245\n",
      "[551]\ttrain-error:0.35245\n",
      "[552]\ttrain-error:0.35245\n",
      "[553]\ttrain-error:0.35245\n",
      "[554]\ttrain-error:0.35245\n",
      "[555]\ttrain-error:0.35245\n",
      "[556]\ttrain-error:0.35245\n",
      "[557]\ttrain-error:0.35245\n",
      "[558]\ttrain-error:0.35245\n",
      "[559]\ttrain-error:0.35245\n",
      "[560]\ttrain-error:0.35245\n",
      "[561]\ttrain-error:0.35245\n",
      "[562]\ttrain-error:0.35245\n",
      "[563]\ttrain-error:0.35245\n",
      "[564]\ttrain-error:0.35245\n",
      "[565]\ttrain-error:0.35245\n",
      "[566]\ttrain-error:0.35245\n",
      "[567]\ttrain-error:0.35245\n",
      "[568]\ttrain-error:0.35245\n",
      "[569]\ttrain-error:0.35245\n",
      "[570]\ttrain-error:0.35245\n",
      "[571]\ttrain-error:0.35245\n",
      "[572]\ttrain-error:0.35245\n",
      "[573]\ttrain-error:0.35245\n",
      "[574]\ttrain-error:0.35245\n",
      "[575]\ttrain-error:0.35245\n",
      "[576]\ttrain-error:0.35245\n",
      "[577]\ttrain-error:0.35245\n",
      "[578]\ttrain-error:0.35245\n",
      "[579]\ttrain-error:0.35245\n",
      "[580]\ttrain-error:0.35245\n",
      "[581]\ttrain-error:0.35245\n",
      "[582]\ttrain-error:0.35245\n",
      "[583]\ttrain-error:0.35245\n",
      "[584]\ttrain-error:0.35245\n",
      "[585]\ttrain-error:0.35245\n",
      "[586]\ttrain-error:0.35245\n",
      "[587]\ttrain-error:0.35245\n",
      "[588]\ttrain-error:0.35245\n",
      "[589]\ttrain-error:0.35245\n",
      "[590]\ttrain-error:0.35245\n",
      "[591]\ttrain-error:0.35245\n",
      "[592]\ttrain-error:0.35245\n",
      "[593]\ttrain-error:0.35245\n",
      "[594]\ttrain-error:0.35245\n",
      "[595]\ttrain-error:0.35245\n",
      "[596]\ttrain-error:0.35245\n",
      "[597]\ttrain-error:0.35245\n",
      "[598]\ttrain-error:0.35245\n",
      "[599]\ttrain-error:0.35245\n",
      "[600]\ttrain-error:0.35245\n",
      "[601]\ttrain-error:0.35245\n",
      "[602]\ttrain-error:0.35245\n",
      "[603]\ttrain-error:0.35245\n",
      "[604]\ttrain-error:0.35245\n",
      "[605]\ttrain-error:0.35245\n",
      "[606]\ttrain-error:0.35245\n",
      "[607]\ttrain-error:0.35245\n",
      "[608]\ttrain-error:0.35245\n",
      "[609]\ttrain-error:0.35245\n",
      "[610]\ttrain-error:0.35245\n",
      "[611]\ttrain-error:0.35245\n",
      "[612]\ttrain-error:0.35245\n",
      "[613]\ttrain-error:0.35245\n",
      "[614]\ttrain-error:0.35245\n",
      "[615]\ttrain-error:0.35245\n",
      "[616]\ttrain-error:0.35245\n",
      "[617]\ttrain-error:0.35245\n",
      "[618]\ttrain-error:0.35245\n",
      "[619]\ttrain-error:0.35245\n",
      "[620]\ttrain-error:0.35245\n",
      "[621]\ttrain-error:0.35245\n",
      "[622]\ttrain-error:0.35245\n",
      "[623]\ttrain-error:0.35245\n",
      "[624]\ttrain-error:0.35245\n",
      "[625]\ttrain-error:0.35245\n",
      "[626]\ttrain-error:0.35245\n",
      "[627]\ttrain-error:0.35245\n",
      "[628]\ttrain-error:0.35245\n",
      "[629]\ttrain-error:0.35245\n",
      "[630]\ttrain-error:0.35245\n",
      "[631]\ttrain-error:0.35245\n",
      "[632]\ttrain-error:0.35245\n",
      "[633]\ttrain-error:0.35245\n",
      "[634]\ttrain-error:0.35245\n",
      "[635]\ttrain-error:0.35245\n",
      "[636]\ttrain-error:0.35245\n",
      "[637]\ttrain-error:0.35245\n",
      "[638]\ttrain-error:0.35245\n",
      "[639]\ttrain-error:0.35245\n",
      "[640]\ttrain-error:0.35245\n",
      "[641]\ttrain-error:0.35245\n",
      "[642]\ttrain-error:0.35245\n",
      "[643]\ttrain-error:0.35245\n",
      "[644]\ttrain-error:0.35245\n",
      "[645]\ttrain-error:0.35245\n",
      "[646]\ttrain-error:0.35245\n",
      "[647]\ttrain-error:0.35245\n",
      "[648]\ttrain-error:0.35245\n",
      "[649]\ttrain-error:0.35245\n",
      "[650]\ttrain-error:0.35245\n",
      "[651]\ttrain-error:0.35245\n",
      "[652]\ttrain-error:0.35245\n",
      "[653]\ttrain-error:0.35245\n",
      "[654]\ttrain-error:0.35245\n",
      "[655]\ttrain-error:0.35245\n",
      "[656]\ttrain-error:0.35245\n",
      "[657]\ttrain-error:0.35245\n",
      "[658]\ttrain-error:0.35245\n",
      "[659]\ttrain-error:0.35245\n",
      "[660]\ttrain-error:0.35245\n",
      "[661]\ttrain-error:0.35245\n",
      "[662]\ttrain-error:0.35245\n",
      "[663]\ttrain-error:0.35245\n",
      "[664]\ttrain-error:0.35245\n",
      "[665]\ttrain-error:0.35245\n",
      "[666]\ttrain-error:0.35245\n",
      "[667]\ttrain-error:0.35245\n",
      "[668]\ttrain-error:0.35245\n",
      "[669]\ttrain-error:0.35245\n",
      "[670]\ttrain-error:0.35245\n",
      "[671]\ttrain-error:0.35245\n",
      "[672]\ttrain-error:0.35245\n",
      "[673]\ttrain-error:0.35245\n",
      "[674]\ttrain-error:0.35245\n",
      "[675]\ttrain-error:0.35245\n",
      "[676]\ttrain-error:0.35245\n",
      "[677]\ttrain-error:0.35245\n",
      "[678]\ttrain-error:0.35245\n",
      "[679]\ttrain-error:0.35245\n",
      "[680]\ttrain-error:0.35245\n",
      "[681]\ttrain-error:0.35245\n",
      "[682]\ttrain-error:0.35245\n",
      "[683]\ttrain-error:0.35245\n",
      "[684]\ttrain-error:0.35245\n",
      "[685]\ttrain-error:0.35245\n",
      "[686]\ttrain-error:0.35245\n",
      "[687]\ttrain-error:0.35245\n",
      "[688]\ttrain-error:0.35245\n",
      "[689]\ttrain-error:0.35245\n",
      "[690]\ttrain-error:0.35245\n",
      "[691]\ttrain-error:0.35245\n",
      "[692]\ttrain-error:0.35245\n",
      "[693]\ttrain-error:0.35245\n",
      "[694]\ttrain-error:0.35245\n",
      "[695]\ttrain-error:0.35245\n",
      "[696]\ttrain-error:0.35245\n",
      "[697]\ttrain-error:0.35245\n",
      "[698]\ttrain-error:0.35245\n",
      "[699]\ttrain-error:0.35245\n",
      "[700]\ttrain-error:0.35245\n",
      "[701]\ttrain-error:0.35245\n",
      "[702]\ttrain-error:0.35245\n",
      "[703]\ttrain-error:0.35245\n",
      "[704]\ttrain-error:0.35245\n",
      "[705]\ttrain-error:0.35245\n",
      "[706]\ttrain-error:0.35245\n",
      "[707]\ttrain-error:0.35245\n",
      "[708]\ttrain-error:0.35245\n",
      "[709]\ttrain-error:0.35245\n",
      "[710]\ttrain-error:0.35245\n",
      "[711]\ttrain-error:0.35245\n",
      "[712]\ttrain-error:0.35245\n",
      "[713]\ttrain-error:0.35245\n",
      "[714]\ttrain-error:0.35245\n",
      "[715]\ttrain-error:0.35245\n",
      "[716]\ttrain-error:0.35245\n",
      "[717]\ttrain-error:0.35245\n",
      "[718]\ttrain-error:0.35245\n",
      "[719]\ttrain-error:0.35245\n",
      "[720]\ttrain-error:0.35245\n",
      "[721]\ttrain-error:0.35245\n",
      "[722]\ttrain-error:0.35245\n",
      "[723]\ttrain-error:0.35245\n",
      "[724]\ttrain-error:0.35245\n",
      "[725]\ttrain-error:0.35245\n",
      "[726]\ttrain-error:0.35245\n",
      "[727]\ttrain-error:0.35245\n",
      "[728]\ttrain-error:0.35245\n",
      "[729]\ttrain-error:0.35245\n",
      "[730]\ttrain-error:0.35245\n",
      "[731]\ttrain-error:0.35245\n",
      "[732]\ttrain-error:0.35245\n",
      "[733]\ttrain-error:0.35245\n",
      "[734]\ttrain-error:0.35245\n",
      "[735]\ttrain-error:0.35245\n",
      "[736]\ttrain-error:0.35245\n",
      "[737]\ttrain-error:0.35245\n",
      "[738]\ttrain-error:0.35245\n",
      "[739]\ttrain-error:0.35245\n",
      "[740]\ttrain-error:0.35245\n",
      "[741]\ttrain-error:0.35245\n",
      "[742]\ttrain-error:0.35245\n",
      "[743]\ttrain-error:0.35245\n",
      "[744]\ttrain-error:0.35245\n",
      "[745]\ttrain-error:0.35245\n",
      "[746]\ttrain-error:0.35245\n",
      "[747]\ttrain-error:0.35245\n",
      "[748]\ttrain-error:0.35245\n",
      "[749]\ttrain-error:0.35245\n",
      "[750]\ttrain-error:0.35245\n",
      "[751]\ttrain-error:0.35245\n",
      "[752]\ttrain-error:0.35245\n",
      "[753]\ttrain-error:0.35245\n",
      "[754]\ttrain-error:0.35245\n",
      "[755]\ttrain-error:0.35245\n",
      "[756]\ttrain-error:0.35245\n",
      "[757]\ttrain-error:0.35245\n",
      "[758]\ttrain-error:0.35245\n",
      "[759]\ttrain-error:0.35245\n",
      "[760]\ttrain-error:0.35245\n",
      "[761]\ttrain-error:0.35245\n",
      "[762]\ttrain-error:0.35245\n",
      "[763]\ttrain-error:0.35245\n",
      "[764]\ttrain-error:0.35245\n",
      "[765]\ttrain-error:0.35245\n",
      "[766]\ttrain-error:0.35245\n",
      "[767]\ttrain-error:0.35245\n",
      "[768]\ttrain-error:0.35245\n",
      "[769]\ttrain-error:0.35245\n",
      "[770]\ttrain-error:0.35245\n",
      "[771]\ttrain-error:0.35245\n",
      "[772]\ttrain-error:0.35245\n",
      "[773]\ttrain-error:0.35245\n",
      "[774]\ttrain-error:0.35245\n",
      "[775]\ttrain-error:0.35245\n",
      "[776]\ttrain-error:0.35245\n",
      "[777]\ttrain-error:0.35245\n",
      "[778]\ttrain-error:0.35245\n",
      "[779]\ttrain-error:0.35245\n",
      "[780]\ttrain-error:0.35245\n",
      "[781]\ttrain-error:0.35245\n",
      "[782]\ttrain-error:0.35245\n",
      "[783]\ttrain-error:0.35245\n",
      "[784]\ttrain-error:0.35245\n",
      "[785]\ttrain-error:0.35245\n",
      "[786]\ttrain-error:0.35245\n",
      "[787]\ttrain-error:0.35245\n",
      "[788]\ttrain-error:0.35245\n",
      "[789]\ttrain-error:0.35245\n",
      "[790]\ttrain-error:0.35245\n",
      "[791]\ttrain-error:0.35245\n",
      "[792]\ttrain-error:0.35245\n",
      "[793]\ttrain-error:0.35245\n",
      "[794]\ttrain-error:0.35245\n",
      "[795]\ttrain-error:0.35245\n",
      "[796]\ttrain-error:0.35245\n",
      "[797]\ttrain-error:0.35245\n",
      "[798]\ttrain-error:0.35245\n",
      "[799]\ttrain-error:0.35245\n",
      "[800]\ttrain-error:0.35245\n",
      "[801]\ttrain-error:0.35245\n",
      "[802]\ttrain-error:0.35245\n",
      "[803]\ttrain-error:0.35245\n",
      "[804]\ttrain-error:0.35245\n",
      "[805]\ttrain-error:0.35245\n",
      "[806]\ttrain-error:0.35245\n",
      "[807]\ttrain-error:0.35245\n",
      "[808]\ttrain-error:0.35245\n",
      "[809]\ttrain-error:0.35245\n",
      "[810]\ttrain-error:0.35245\n",
      "[811]\ttrain-error:0.35245\n",
      "[812]\ttrain-error:0.35245\n",
      "[813]\ttrain-error:0.35245\n",
      "[814]\ttrain-error:0.35245\n",
      "[815]\ttrain-error:0.35245\n",
      "[816]\ttrain-error:0.35245\n",
      "[817]\ttrain-error:0.35245\n",
      "[818]\ttrain-error:0.35245\n",
      "[819]\ttrain-error:0.35245\n",
      "[820]\ttrain-error:0.35245\n",
      "[821]\ttrain-error:0.35245\n",
      "[822]\ttrain-error:0.35245\n",
      "[823]\ttrain-error:0.35245\n",
      "[824]\ttrain-error:0.35245\n",
      "[825]\ttrain-error:0.35245\n",
      "[826]\ttrain-error:0.35245\n",
      "[827]\ttrain-error:0.35245\n",
      "[828]\ttrain-error:0.35245\n",
      "[829]\ttrain-error:0.35245\n",
      "[830]\ttrain-error:0.35245\n",
      "[831]\ttrain-error:0.35245\n",
      "[832]\ttrain-error:0.35245\n",
      "[833]\ttrain-error:0.35245\n",
      "[834]\ttrain-error:0.35245\n",
      "[835]\ttrain-error:0.35245\n",
      "[836]\ttrain-error:0.35245\n",
      "[837]\ttrain-error:0.35245\n",
      "[838]\ttrain-error:0.35245\n",
      "[839]\ttrain-error:0.35245\n",
      "[840]\ttrain-error:0.35245\n",
      "[841]\ttrain-error:0.35245\n",
      "[842]\ttrain-error:0.35245\n",
      "[843]\ttrain-error:0.35245\n",
      "[844]\ttrain-error:0.35245\n",
      "[845]\ttrain-error:0.35245\n",
      "[846]\ttrain-error:0.35245\n",
      "[847]\ttrain-error:0.35245\n",
      "[848]\ttrain-error:0.35245\n",
      "[849]\ttrain-error:0.35245\n",
      "[850]\ttrain-error:0.35245\n",
      "[851]\ttrain-error:0.35245\n",
      "[852]\ttrain-error:0.35245\n",
      "[853]\ttrain-error:0.35245\n",
      "[854]\ttrain-error:0.35245\n",
      "[855]\ttrain-error:0.35245\n",
      "[856]\ttrain-error:0.35245\n",
      "[857]\ttrain-error:0.35245\n",
      "[858]\ttrain-error:0.35245\n",
      "[859]\ttrain-error:0.35245\n",
      "[860]\ttrain-error:0.35245\n",
      "[861]\ttrain-error:0.35245\n",
      "[862]\ttrain-error:0.35245\n",
      "[863]\ttrain-error:0.35245\n",
      "[864]\ttrain-error:0.35245\n",
      "[865]\ttrain-error:0.35245\n",
      "[866]\ttrain-error:0.35245\n",
      "[867]\ttrain-error:0.35245\n",
      "[868]\ttrain-error:0.35245\n",
      "[869]\ttrain-error:0.35245\n",
      "[870]\ttrain-error:0.35245\n",
      "[871]\ttrain-error:0.35245\n",
      "[872]\ttrain-error:0.35245\n",
      "[873]\ttrain-error:0.35245\n",
      "[874]\ttrain-error:0.35245\n",
      "[875]\ttrain-error:0.35245\n",
      "[876]\ttrain-error:0.35245\n",
      "[877]\ttrain-error:0.35245\n",
      "[878]\ttrain-error:0.35245\n",
      "[879]\ttrain-error:0.35245\n",
      "[880]\ttrain-error:0.35245\n",
      "[881]\ttrain-error:0.35245\n",
      "[882]\ttrain-error:0.35245\n",
      "[883]\ttrain-error:0.35245\n",
      "[884]\ttrain-error:0.35245\n",
      "[885]\ttrain-error:0.35245\n",
      "[886]\ttrain-error:0.35245\n",
      "[887]\ttrain-error:0.35245\n",
      "[888]\ttrain-error:0.35245\n",
      "[889]\ttrain-error:0.35245\n",
      "[890]\ttrain-error:0.35245\n",
      "[891]\ttrain-error:0.35245\n",
      "[892]\ttrain-error:0.35245\n",
      "[893]\ttrain-error:0.35245\n",
      "[894]\ttrain-error:0.35245\n",
      "[895]\ttrain-error:0.35245\n",
      "[896]\ttrain-error:0.35245\n",
      "[897]\ttrain-error:0.35245\n",
      "[898]\ttrain-error:0.35245\n",
      "[899]\ttrain-error:0.35245\n",
      "[900]\ttrain-error:0.35245\n",
      "[901]\ttrain-error:0.35245\n",
      "[902]\ttrain-error:0.35245\n",
      "[903]\ttrain-error:0.35245\n",
      "[904]\ttrain-error:0.35245\n",
      "[905]\ttrain-error:0.35245\n",
      "[906]\ttrain-error:0.35245\n",
      "[907]\ttrain-error:0.35245\n",
      "[908]\ttrain-error:0.35245\n",
      "[909]\ttrain-error:0.35245\n",
      "[910]\ttrain-error:0.35245\n",
      "[911]\ttrain-error:0.35245\n",
      "[912]\ttrain-error:0.35245\n",
      "[913]\ttrain-error:0.35245\n",
      "[914]\ttrain-error:0.35245\n",
      "[915]\ttrain-error:0.35245\n",
      "[916]\ttrain-error:0.35245\n",
      "[917]\ttrain-error:0.35245\n",
      "[918]\ttrain-error:0.35245\n",
      "[919]\ttrain-error:0.35245\n",
      "[920]\ttrain-error:0.35245\n",
      "[921]\ttrain-error:0.35245\n",
      "[922]\ttrain-error:0.35245\n",
      "[923]\ttrain-error:0.35245\n",
      "[924]\ttrain-error:0.35245\n",
      "[925]\ttrain-error:0.35245\n",
      "[926]\ttrain-error:0.35245\n",
      "[927]\ttrain-error:0.35245\n",
      "[928]\ttrain-error:0.35245\n",
      "[929]\ttrain-error:0.35245\n",
      "[930]\ttrain-error:0.35245\n",
      "[931]\ttrain-error:0.35245\n",
      "[932]\ttrain-error:0.35245\n",
      "[933]\ttrain-error:0.35245\n",
      "[934]\ttrain-error:0.35245\n",
      "[935]\ttrain-error:0.35245\n",
      "[936]\ttrain-error:0.35245\n",
      "[937]\ttrain-error:0.35245\n",
      "[938]\ttrain-error:0.35245\n",
      "[939]\ttrain-error:0.35245\n",
      "[940]\ttrain-error:0.35245\n",
      "[941]\ttrain-error:0.35245\n",
      "[942]\ttrain-error:0.35245\n",
      "[943]\ttrain-error:0.35245\n",
      "[944]\ttrain-error:0.35245\n",
      "[945]\ttrain-error:0.35245\n",
      "[946]\ttrain-error:0.35245\n",
      "[947]\ttrain-error:0.35245\n",
      "[948]\ttrain-error:0.35245\n",
      "[949]\ttrain-error:0.35245\n",
      "[950]\ttrain-error:0.35245\n",
      "[951]\ttrain-error:0.35245\n",
      "[952]\ttrain-error:0.35245\n",
      "[953]\ttrain-error:0.35245\n",
      "[954]\ttrain-error:0.35245\n",
      "[955]\ttrain-error:0.35245\n",
      "[956]\ttrain-error:0.35245\n",
      "[957]\ttrain-error:0.35245\n",
      "[958]\ttrain-error:0.35245\n",
      "[959]\ttrain-error:0.35245\n",
      "[960]\ttrain-error:0.35245\n",
      "[961]\ttrain-error:0.35245\n",
      "[962]\ttrain-error:0.35245\n",
      "[963]\ttrain-error:0.35245\n",
      "[964]\ttrain-error:0.35245\n",
      "[965]\ttrain-error:0.35245\n",
      "[966]\ttrain-error:0.35245\n",
      "[967]\ttrain-error:0.35245\n",
      "[968]\ttrain-error:0.35245\n",
      "[969]\ttrain-error:0.35245\n",
      "[970]\ttrain-error:0.35245\n",
      "[971]\ttrain-error:0.35245\n",
      "[972]\ttrain-error:0.35245\n",
      "[973]\ttrain-error:0.35245\n",
      "[974]\ttrain-error:0.35245\n",
      "[975]\ttrain-error:0.35245\n",
      "[976]\ttrain-error:0.35245\n",
      "[977]\ttrain-error:0.35245\n",
      "[978]\ttrain-error:0.35245\n",
      "[979]\ttrain-error:0.35245\n",
      "[980]\ttrain-error:0.35245\n",
      "[981]\ttrain-error:0.35245\n",
      "[982]\ttrain-error:0.35245\n",
      "[983]\ttrain-error:0.35245\n",
      "[984]\ttrain-error:0.35245\n",
      "[985]\ttrain-error:0.35245\n",
      "[986]\ttrain-error:0.35245\n",
      "[987]\ttrain-error:0.35245\n",
      "[988]\ttrain-error:0.35245\n",
      "[989]\ttrain-error:0.35245\n",
      "[990]\ttrain-error:0.35245\n",
      "[991]\ttrain-error:0.35245\n",
      "[992]\ttrain-error:0.35245\n",
      "[993]\ttrain-error:0.35245\n",
      "[994]\ttrain-error:0.35245\n",
      "[995]\ttrain-error:0.35245\n",
      "[996]\ttrain-error:0.35245\n",
      "[997]\ttrain-error:0.35245\n",
      "[998]\ttrain-error:0.35245\n",
      "[999]\ttrain-error:0.35245\n",
      "[1000]\ttrain-error:0.35245\n",
      "[1001]\ttrain-error:0.35245\n",
      "[1002]\ttrain-error:0.35245\n",
      "[1003]\ttrain-error:0.35245\n",
      "[1004]\ttrain-error:0.35245\n",
      "[1005]\ttrain-error:0.35245\n",
      "[1006]\ttrain-error:0.35245\n",
      "[1007]\ttrain-error:0.35245\n",
      "[1008]\ttrain-error:0.35245\n",
      "[1009]\ttrain-error:0.35245\n",
      "[1010]\ttrain-error:0.35245\n",
      "[1011]\ttrain-error:0.35245\n",
      "[1012]\ttrain-error:0.35245\n",
      "[1013]\ttrain-error:0.35245\n",
      "[1014]\ttrain-error:0.35245\n",
      "[1015]\ttrain-error:0.35245\n",
      "[1016]\ttrain-error:0.35245\n",
      "[1017]\ttrain-error:0.35245\n",
      "[1018]\ttrain-error:0.35245\n",
      "[1019]\ttrain-error:0.35245\n",
      "[1020]\ttrain-error:0.35245\n",
      "[1021]\ttrain-error:0.35245\n",
      "[1022]\ttrain-error:0.35245\n",
      "[1023]\ttrain-error:0.35245\n",
      "[1024]\ttrain-error:0.35245\n",
      "[1025]\ttrain-error:0.35245\n",
      "[1026]\ttrain-error:0.35245\n",
      "[1027]\ttrain-error:0.35245\n",
      "[1028]\ttrain-error:0.35245\n",
      "[1029]\ttrain-error:0.35245\n",
      "[1030]\ttrain-error:0.35245\n",
      "[1031]\ttrain-error:0.35245\n",
      "[1032]\ttrain-error:0.35245\n",
      "[1033]\ttrain-error:0.35245\n",
      "[1034]\ttrain-error:0.35245\n",
      "[1035]\ttrain-error:0.35245\n",
      "[1036]\ttrain-error:0.35245\n",
      "[1037]\ttrain-error:0.35245\n",
      "[1038]\ttrain-error:0.35245\n",
      "[1039]\ttrain-error:0.35245\n",
      "[1040]\ttrain-error:0.35245\n",
      "[1041]\ttrain-error:0.35245\n",
      "[1042]\ttrain-error:0.35245\n",
      "[1043]\ttrain-error:0.35245\n",
      "[1044]\ttrain-error:0.35245\n",
      "[1045]\ttrain-error:0.35245\n",
      "[1046]\ttrain-error:0.35245\n",
      "[1047]\ttrain-error:0.35245\n",
      "[1048]\ttrain-error:0.35245\n",
      "[1049]\ttrain-error:0.35245\n",
      "[1050]\ttrain-error:0.35245\n",
      "[1051]\ttrain-error:0.35245\n",
      "[1052]\ttrain-error:0.35245\n",
      "[1053]\ttrain-error:0.35245\n",
      "[1054]\ttrain-error:0.35245\n",
      "[1055]\ttrain-error:0.35245\n",
      "[1056]\ttrain-error:0.35245\n",
      "[1057]\ttrain-error:0.35245\n",
      "[1058]\ttrain-error:0.35245\n",
      "[1059]\ttrain-error:0.35245\n",
      "[1060]\ttrain-error:0.35245\n",
      "[1061]\ttrain-error:0.35245\n",
      "[1062]\ttrain-error:0.35245\n",
      "[1063]\ttrain-error:0.35245\n",
      "[1064]\ttrain-error:0.35245\n",
      "[1065]\ttrain-error:0.35245\n",
      "[1066]\ttrain-error:0.35245\n",
      "[1067]\ttrain-error:0.35245\n",
      "[1068]\ttrain-error:0.35245\n",
      "[1069]\ttrain-error:0.35245\n",
      "[1070]\ttrain-error:0.35245\n",
      "[1071]\ttrain-error:0.35245\n",
      "[1072]\ttrain-error:0.35245\n",
      "[1073]\ttrain-error:0.35245\n",
      "[1074]\ttrain-error:0.35245\n",
      "[1075]\ttrain-error:0.35245\n",
      "[1076]\ttrain-error:0.35245\n",
      "[1077]\ttrain-error:0.35245\n",
      "[1078]\ttrain-error:0.35245\n",
      "[1079]\ttrain-error:0.35245\n",
      "[1080]\ttrain-error:0.35245\n",
      "[1081]\ttrain-error:0.35245\n",
      "[1082]\ttrain-error:0.35245\n",
      "[1083]\ttrain-error:0.35245\n",
      "[1084]\ttrain-error:0.35245\n",
      "[1085]\ttrain-error:0.35245\n",
      "[1086]\ttrain-error:0.35245\n",
      "[1087]\ttrain-error:0.35245\n",
      "[1088]\ttrain-error:0.35245\n",
      "[1089]\ttrain-error:0.35245\n",
      "[1090]\ttrain-error:0.35245\n",
      "[1091]\ttrain-error:0.35245\n",
      "[1092]\ttrain-error:0.35245\n",
      "[1093]\ttrain-error:0.35245\n",
      "[1094]\ttrain-error:0.35245\n",
      "[1095]\ttrain-error:0.35245\n",
      "[1096]\ttrain-error:0.35245\n",
      "[1097]\ttrain-error:0.35245\n",
      "[1098]\ttrain-error:0.35245\n",
      "[1099]\ttrain-error:0.35245\n",
      "[1100]\ttrain-error:0.35245\n",
      "[1101]\ttrain-error:0.35245\n",
      "[1102]\ttrain-error:0.35245\n",
      "[1103]\ttrain-error:0.35245\n",
      "[1104]\ttrain-error:0.35245\n",
      "[1105]\ttrain-error:0.35245\n",
      "[1106]\ttrain-error:0.35245\n",
      "[1107]\ttrain-error:0.35245\n",
      "[1108]\ttrain-error:0.35245\n",
      "[1109]\ttrain-error:0.35245\n",
      "[1110]\ttrain-error:0.35245\n",
      "[1111]\ttrain-error:0.35245\n",
      "[1112]\ttrain-error:0.35245\n",
      "[1113]\ttrain-error:0.35245\n",
      "[1114]\ttrain-error:0.35245\n",
      "[1115]\ttrain-error:0.35245\n",
      "[1116]\ttrain-error:0.35245\n",
      "[1117]\ttrain-error:0.35245\n",
      "[1118]\ttrain-error:0.35245\n",
      "[1119]\ttrain-error:0.35245\n",
      "[1120]\ttrain-error:0.35245\n",
      "[1121]\ttrain-error:0.35245\n",
      "[1122]\ttrain-error:0.35245\n",
      "[1123]\ttrain-error:0.35245\n",
      "[1124]\ttrain-error:0.35245\n",
      "[1125]\ttrain-error:0.35245\n",
      "[1126]\ttrain-error:0.35245\n",
      "[1127]\ttrain-error:0.35245\n",
      "[1128]\ttrain-error:0.35245\n",
      "[1129]\ttrain-error:0.35245\n",
      "[1130]\ttrain-error:0.35245\n",
      "[1131]\ttrain-error:0.35245\n",
      "[1132]\ttrain-error:0.35245\n",
      "[1133]\ttrain-error:0.35245\n",
      "[1134]\ttrain-error:0.35245\n",
      "[1135]\ttrain-error:0.35245\n",
      "[1136]\ttrain-error:0.35245\n",
      "[1137]\ttrain-error:0.35245\n",
      "[1138]\ttrain-error:0.35245\n",
      "[1139]\ttrain-error:0.35245\n",
      "[1140]\ttrain-error:0.35245\n",
      "[1141]\ttrain-error:0.35245\n",
      "[1142]\ttrain-error:0.35245\n",
      "[1143]\ttrain-error:0.35245\n",
      "[1144]\ttrain-error:0.35245\n",
      "[1145]\ttrain-error:0.35245\n",
      "[1146]\ttrain-error:0.35245\n",
      "[1147]\ttrain-error:0.35245\n",
      "[1148]\ttrain-error:0.35245\n",
      "[1149]\ttrain-error:0.35245\n",
      "[1150]\ttrain-error:0.35245\n",
      "[1151]\ttrain-error:0.35245\n",
      "[1152]\ttrain-error:0.35245\n",
      "[1153]\ttrain-error:0.35245\n",
      "[1154]\ttrain-error:0.35245\n",
      "[1155]\ttrain-error:0.35245\n",
      "[1156]\ttrain-error:0.35245\n",
      "[1157]\ttrain-error:0.35245\n",
      "[1158]\ttrain-error:0.35245\n",
      "[1159]\ttrain-error:0.35245\n",
      "[1160]\ttrain-error:0.35245\n",
      "[1161]\ttrain-error:0.35245\n",
      "[1162]\ttrain-error:0.35245\n",
      "[1163]\ttrain-error:0.35245\n",
      "[1164]\ttrain-error:0.35245\n",
      "[1165]\ttrain-error:0.35245\n",
      "[1166]\ttrain-error:0.35245\n",
      "[1167]\ttrain-error:0.35245\n",
      "[1168]\ttrain-error:0.35245\n",
      "[1169]\ttrain-error:0.35245\n",
      "[1170]\ttrain-error:0.35245\n",
      "[1171]\ttrain-error:0.35245\n",
      "[1172]\ttrain-error:0.35245\n",
      "[1173]\ttrain-error:0.35245\n",
      "[1174]\ttrain-error:0.35245\n",
      "[1175]\ttrain-error:0.35245\n",
      "[1176]\ttrain-error:0.35245\n",
      "[1177]\ttrain-error:0.35245\n",
      "[1178]\ttrain-error:0.35245\n",
      "[1179]\ttrain-error:0.35245\n",
      "[1180]\ttrain-error:0.35245\n",
      "[1181]\ttrain-error:0.35245\n",
      "[1182]\ttrain-error:0.35245\n",
      "[1183]\ttrain-error:0.35245\n",
      "[1184]\ttrain-error:0.35245\n",
      "[1185]\ttrain-error:0.35245\n",
      "[1186]\ttrain-error:0.35245\n",
      "[1187]\ttrain-error:0.35245\n",
      "[1188]\ttrain-error:0.35245\n",
      "[1189]\ttrain-error:0.35245\n",
      "[1190]\ttrain-error:0.35245\n",
      "[1191]\ttrain-error:0.35245\n",
      "[1192]\ttrain-error:0.35245\n",
      "[1193]\ttrain-error:0.35245\n",
      "[1194]\ttrain-error:0.35245\n",
      "[1195]\ttrain-error:0.35245\n",
      "[1196]\ttrain-error:0.35245\n",
      "[1197]\ttrain-error:0.35245\n",
      "[1198]\ttrain-error:0.35245\n",
      "[1199]\ttrain-error:0.35245\n",
      "[1200]\ttrain-error:0.35245\n",
      "[1201]\ttrain-error:0.35245\n",
      "[1202]\ttrain-error:0.35245\n",
      "[1203]\ttrain-error:0.35245\n",
      "[1204]\ttrain-error:0.35245\n",
      "[1205]\ttrain-error:0.35245\n",
      "[1206]\ttrain-error:0.35245\n",
      "[1207]\ttrain-error:0.35245\n",
      "[1208]\ttrain-error:0.35245\n",
      "[1209]\ttrain-error:0.35245\n",
      "[1210]\ttrain-error:0.35245\n",
      "[1211]\ttrain-error:0.35245\n",
      "[1212]\ttrain-error:0.35245\n",
      "[1213]\ttrain-error:0.35245\n",
      "[1214]\ttrain-error:0.35245\n",
      "[1215]\ttrain-error:0.35245\n",
      "[1216]\ttrain-error:0.35245\n",
      "[1217]\ttrain-error:0.35245\n",
      "[1218]\ttrain-error:0.35245\n",
      "[1219]\ttrain-error:0.35245\n",
      "[1220]\ttrain-error:0.35245\n",
      "[1221]\ttrain-error:0.35245\n",
      "[1222]\ttrain-error:0.35245\n",
      "[1223]\ttrain-error:0.35245\n",
      "[1224]\ttrain-error:0.35245\n",
      "[1225]\ttrain-error:0.35245\n",
      "[1226]\ttrain-error:0.35245\n",
      "[1227]\ttrain-error:0.35245\n",
      "[1228]\ttrain-error:0.35245\n",
      "[1229]\ttrain-error:0.35245\n",
      "[1230]\ttrain-error:0.35245\n",
      "[1231]\ttrain-error:0.35245\n",
      "[1232]\ttrain-error:0.35245\n",
      "[1233]\ttrain-error:0.35245\n",
      "[1234]\ttrain-error:0.35245\n",
      "[1235]\ttrain-error:0.35245\n",
      "[1236]\ttrain-error:0.35245\n",
      "[1237]\ttrain-error:0.35245\n",
      "[1238]\ttrain-error:0.35245\n",
      "[1239]\ttrain-error:0.35245\n",
      "[1240]\ttrain-error:0.35245\n",
      "[1241]\ttrain-error:0.35245\n",
      "[1242]\ttrain-error:0.35245\n",
      "[1243]\ttrain-error:0.35245\n",
      "[1244]\ttrain-error:0.35245\n",
      "[1245]\ttrain-error:0.35245\n",
      "[1246]\ttrain-error:0.35245\n",
      "[1247]\ttrain-error:0.35245\n",
      "[1248]\ttrain-error:0.35245\n",
      "[1249]\ttrain-error:0.35245\n",
      "[1250]\ttrain-error:0.35245\n",
      "[1251]\ttrain-error:0.35245\n",
      "[1252]\ttrain-error:0.35245\n",
      "[1253]\ttrain-error:0.35245\n",
      "[1254]\ttrain-error:0.35245\n",
      "[1255]\ttrain-error:0.35245\n",
      "[1256]\ttrain-error:0.35245\n",
      "[1257]\ttrain-error:0.35245\n",
      "[1258]\ttrain-error:0.35245\n",
      "[1259]\ttrain-error:0.35245\n",
      "[1260]\ttrain-error:0.35245\n",
      "[1261]\ttrain-error:0.35245\n",
      "[1262]\ttrain-error:0.35245\n",
      "[1263]\ttrain-error:0.35245\n",
      "[1264]\ttrain-error:0.35245\n",
      "[1265]\ttrain-error:0.35245\n",
      "[1266]\ttrain-error:0.35245\n",
      "[1267]\ttrain-error:0.35245\n",
      "[1268]\ttrain-error:0.35245\n",
      "[1269]\ttrain-error:0.35245\n",
      "[1270]\ttrain-error:0.35245\n",
      "[1271]\ttrain-error:0.35245\n",
      "[1272]\ttrain-error:0.35245\n",
      "[1273]\ttrain-error:0.35245\n",
      "[1274]\ttrain-error:0.35245\n",
      "[1275]\ttrain-error:0.35245\n",
      "[1276]\ttrain-error:0.35245\n",
      "[1277]\ttrain-error:0.35245\n",
      "[1278]\ttrain-error:0.35245\n",
      "[1279]\ttrain-error:0.35245\n",
      "[1280]\ttrain-error:0.35245\n",
      "[1281]\ttrain-error:0.35245\n",
      "[1282]\ttrain-error:0.35245\n",
      "[1283]\ttrain-error:0.35245\n",
      "[1284]\ttrain-error:0.35245\n",
      "[1285]\ttrain-error:0.35245\n",
      "[1286]\ttrain-error:0.35245\n",
      "[1287]\ttrain-error:0.35245\n",
      "[1288]\ttrain-error:0.35245\n",
      "[1289]\ttrain-error:0.35245\n",
      "[1290]\ttrain-error:0.35245\n",
      "[1291]\ttrain-error:0.35245\n",
      "[1292]\ttrain-error:0.35245\n",
      "[1293]\ttrain-error:0.35245\n",
      "[1294]\ttrain-error:0.35245\n",
      "[1295]\ttrain-error:0.35245\n",
      "[1296]\ttrain-error:0.35245\n",
      "[1297]\ttrain-error:0.35245\n",
      "[1298]\ttrain-error:0.35245\n",
      "[1299]\ttrain-error:0.35245\n",
      "[1300]\ttrain-error:0.35245\n",
      "[1301]\ttrain-error:0.35245\n",
      "[1302]\ttrain-error:0.35245\n",
      "[1303]\ttrain-error:0.35245\n",
      "[1304]\ttrain-error:0.35245\n",
      "[1305]\ttrain-error:0.35245\n",
      "[1306]\ttrain-error:0.35245\n",
      "[1307]\ttrain-error:0.35245\n",
      "[1308]\ttrain-error:0.35245\n",
      "[1309]\ttrain-error:0.35245\n",
      "[1310]\ttrain-error:0.35245\n",
      "[1311]\ttrain-error:0.35245\n",
      "[1312]\ttrain-error:0.35245\n",
      "[1313]\ttrain-error:0.35245\n",
      "[1314]\ttrain-error:0.35245\n",
      "[1315]\ttrain-error:0.35245\n",
      "[1316]\ttrain-error:0.35245\n",
      "[1317]\ttrain-error:0.35245\n",
      "[1318]\ttrain-error:0.35245\n",
      "[1319]\ttrain-error:0.35245\n",
      "[1320]\ttrain-error:0.35245\n",
      "[1321]\ttrain-error:0.35245\n",
      "[1322]\ttrain-error:0.35245\n",
      "[1323]\ttrain-error:0.35245\n",
      "[1324]\ttrain-error:0.35245\n",
      "[1325]\ttrain-error:0.35245\n",
      "[1326]\ttrain-error:0.35245\n",
      "[1327]\ttrain-error:0.35245\n",
      "[1328]\ttrain-error:0.35245\n",
      "[1329]\ttrain-error:0.35245\n",
      "[1330]\ttrain-error:0.35245\n",
      "[1331]\ttrain-error:0.35245\n",
      "[1332]\ttrain-error:0.35245\n",
      "[1333]\ttrain-error:0.35245\n",
      "[1334]\ttrain-error:0.35245\n",
      "[1335]\ttrain-error:0.35245\n",
      "[1336]\ttrain-error:0.35245\n",
      "[1337]\ttrain-error:0.35245\n",
      "[1338]\ttrain-error:0.35245\n",
      "[1339]\ttrain-error:0.35245\n",
      "[1340]\ttrain-error:0.35245\n",
      "[1341]\ttrain-error:0.35245\n",
      "[1342]\ttrain-error:0.35245\n",
      "[1343]\ttrain-error:0.35245\n",
      "[1344]\ttrain-error:0.35245\n",
      "[1345]\ttrain-error:0.35245\n",
      "[1346]\ttrain-error:0.35245\n",
      "[1347]\ttrain-error:0.35245\n",
      "[1348]\ttrain-error:0.35245\n",
      "[1349]\ttrain-error:0.35245\n",
      "[1350]\ttrain-error:0.35245\n",
      "[1351]\ttrain-error:0.35245\n",
      "[1352]\ttrain-error:0.35245\n",
      "[1353]\ttrain-error:0.35245\n",
      "[1354]\ttrain-error:0.35245\n",
      "[1355]\ttrain-error:0.35245\n",
      "[1356]\ttrain-error:0.35245\n",
      "[1357]\ttrain-error:0.35245\n",
      "[1358]\ttrain-error:0.35245\n",
      "[1359]\ttrain-error:0.35245\n",
      "[1360]\ttrain-error:0.35245\n",
      "[1361]\ttrain-error:0.35245\n",
      "[1362]\ttrain-error:0.35245\n",
      "[1363]\ttrain-error:0.35245\n",
      "[1364]\ttrain-error:0.35245\n",
      "[1365]\ttrain-error:0.35245\n",
      "[1366]\ttrain-error:0.35245\n",
      "[1367]\ttrain-error:0.35245\n",
      "[1368]\ttrain-error:0.35245\n",
      "[1369]\ttrain-error:0.35245\n",
      "[1370]\ttrain-error:0.35245\n",
      "[1371]\ttrain-error:0.35245\n",
      "[1372]\ttrain-error:0.35245\n",
      "[1373]\ttrain-error:0.35245\n",
      "[1374]\ttrain-error:0.35245\n",
      "[1375]\ttrain-error:0.35245\n",
      "[1376]\ttrain-error:0.35245\n",
      "[1377]\ttrain-error:0.35245\n",
      "[1378]\ttrain-error:0.35245\n",
      "[1379]\ttrain-error:0.35245\n",
      "[1380]\ttrain-error:0.35245\n",
      "[1381]\ttrain-error:0.35245\n",
      "[1382]\ttrain-error:0.35245\n",
      "[1383]\ttrain-error:0.35245\n",
      "[1384]\ttrain-error:0.35245\n",
      "[1385]\ttrain-error:0.35245\n",
      "[1386]\ttrain-error:0.35245\n",
      "[1387]\ttrain-error:0.35245\n",
      "[1388]\ttrain-error:0.35245\n",
      "[1389]\ttrain-error:0.35245\n",
      "[1390]\ttrain-error:0.35245\n",
      "[1391]\ttrain-error:0.35245\n",
      "[1392]\ttrain-error:0.35245\n",
      "[1393]\ttrain-error:0.35245\n",
      "[1394]\ttrain-error:0.35245\n",
      "[1395]\ttrain-error:0.35245\n",
      "[1396]\ttrain-error:0.35245\n",
      "[1397]\ttrain-error:0.35245\n",
      "[1398]\ttrain-error:0.35245\n",
      "[1399]\ttrain-error:0.35245\n",
      "[1400]\ttrain-error:0.35245\n",
      "[1401]\ttrain-error:0.35245\n",
      "[1402]\ttrain-error:0.35245\n",
      "[1403]\ttrain-error:0.35245\n",
      "[1404]\ttrain-error:0.35245\n",
      "[1405]\ttrain-error:0.35245\n",
      "[1406]\ttrain-error:0.35245\n",
      "[1407]\ttrain-error:0.35245\n",
      "[1408]\ttrain-error:0.35245\n",
      "[1409]\ttrain-error:0.35245\n",
      "[1410]\ttrain-error:0.35245\n",
      "[1411]\ttrain-error:0.35245\n",
      "[1412]\ttrain-error:0.35245\n",
      "[1413]\ttrain-error:0.35245\n",
      "[1414]\ttrain-error:0.35245\n",
      "[1415]\ttrain-error:0.35245\n",
      "[1416]\ttrain-error:0.35245\n",
      "[1417]\ttrain-error:0.35245\n",
      "[1418]\ttrain-error:0.35245\n",
      "[1419]\ttrain-error:0.35245\n",
      "[1420]\ttrain-error:0.35245\n",
      "[1421]\ttrain-error:0.35245\n",
      "[1422]\ttrain-error:0.35245\n",
      "[1423]\ttrain-error:0.35245\n",
      "[1424]\ttrain-error:0.35245\n",
      "[1425]\ttrain-error:0.35245\n",
      "[1426]\ttrain-error:0.35245\n",
      "[1427]\ttrain-error:0.35245\n",
      "[1428]\ttrain-error:0.35245\n",
      "[1429]\ttrain-error:0.35245\n",
      "[1430]\ttrain-error:0.35245\n",
      "[1431]\ttrain-error:0.35245\n",
      "[1432]\ttrain-error:0.35245\n",
      "[1433]\ttrain-error:0.35245\n",
      "[1434]\ttrain-error:0.35245\n",
      "[1435]\ttrain-error:0.35245\n",
      "[1436]\ttrain-error:0.35245\n",
      "[1437]\ttrain-error:0.35245\n",
      "[1438]\ttrain-error:0.35245\n",
      "[1439]\ttrain-error:0.35245\n",
      "[1440]\ttrain-error:0.35245\n",
      "[1441]\ttrain-error:0.35245\n",
      "[1442]\ttrain-error:0.35245\n",
      "[1443]\ttrain-error:0.35245\n",
      "[1444]\ttrain-error:0.35245\n",
      "[1445]\ttrain-error:0.35245\n",
      "[1446]\ttrain-error:0.35245\n",
      "[1447]\ttrain-error:0.35245\n",
      "[1448]\ttrain-error:0.35245\n",
      "[1449]\ttrain-error:0.35245\n",
      "[1450]\ttrain-error:0.35245\n",
      "[1451]\ttrain-error:0.35245\n",
      "[1452]\ttrain-error:0.35245\n",
      "[1453]\ttrain-error:0.35245\n",
      "[1454]\ttrain-error:0.35245\n",
      "[1455]\ttrain-error:0.35245\n",
      "[1456]\ttrain-error:0.35245\n",
      "[1457]\ttrain-error:0.35245\n",
      "[1458]\ttrain-error:0.35245\n",
      "[1459]\ttrain-error:0.35245\n",
      "[1460]\ttrain-error:0.35245\n",
      "[1461]\ttrain-error:0.35245\n",
      "[1462]\ttrain-error:0.35245\n",
      "[1463]\ttrain-error:0.35245\n",
      "[1464]\ttrain-error:0.35245\n",
      "[1465]\ttrain-error:0.35245\n",
      "[1466]\ttrain-error:0.35245\n",
      "[1467]\ttrain-error:0.35245\n",
      "[1468]\ttrain-error:0.35245\n",
      "[1469]\ttrain-error:0.35245\n",
      "[1470]\ttrain-error:0.35245\n",
      "[1471]\ttrain-error:0.35245\n",
      "[1472]\ttrain-error:0.35245\n",
      "[1473]\ttrain-error:0.35245\n",
      "[1474]\ttrain-error:0.35245\n",
      "[1475]\ttrain-error:0.35245\n",
      "[1476]\ttrain-error:0.35245\n",
      "[1477]\ttrain-error:0.35245\n",
      "[1478]\ttrain-error:0.35245\n",
      "[1479]\ttrain-error:0.35245\n",
      "[1480]\ttrain-error:0.35245\n",
      "[1481]\ttrain-error:0.35245\n",
      "[1482]\ttrain-error:0.35245\n",
      "[1483]\ttrain-error:0.35245\n",
      "[1484]\ttrain-error:0.35245\n",
      "[1485]\ttrain-error:0.35245\n",
      "[1486]\ttrain-error:0.35245\n",
      "[1487]\ttrain-error:0.35245\n",
      "[1488]\ttrain-error:0.35245\n",
      "[1489]\ttrain-error:0.35245\n",
      "[1490]\ttrain-error:0.35245\n",
      "[1491]\ttrain-error:0.35245\n",
      "[1492]\ttrain-error:0.35245\n",
      "[1493]\ttrain-error:0.35245\n",
      "[1494]\ttrain-error:0.35245\n",
      "[1495]\ttrain-error:0.35245\n",
      "[1496]\ttrain-error:0.35245\n",
      "[1497]\ttrain-error:0.35245\n",
      "[1498]\ttrain-error:0.35245\n",
      "[1499]\ttrain-error:0.35245\n",
      "[1500]\ttrain-error:0.35245\n",
      "[1501]\ttrain-error:0.35245\n",
      "[1502]\ttrain-error:0.35245\n",
      "[1503]\ttrain-error:0.35245\n",
      "[1504]\ttrain-error:0.35245\n",
      "[1505]\ttrain-error:0.35245\n",
      "[1506]\ttrain-error:0.35245\n",
      "[1507]\ttrain-error:0.35245\n",
      "[1508]\ttrain-error:0.35245\n",
      "[1509]\ttrain-error:0.35245\n",
      "[1510]\ttrain-error:0.35245\n",
      "[1511]\ttrain-error:0.35245\n",
      "[1512]\ttrain-error:0.35245\n",
      "[1513]\ttrain-error:0.35245\n",
      "[1514]\ttrain-error:0.35245\n",
      "[1515]\ttrain-error:0.35245\n",
      "[1516]\ttrain-error:0.35245\n",
      "[1517]\ttrain-error:0.35245\n",
      "[1518]\ttrain-error:0.35245\n",
      "[1519]\ttrain-error:0.35245\n",
      "[1520]\ttrain-error:0.35245\n",
      "[1521]\ttrain-error:0.35245\n",
      "[1522]\ttrain-error:0.35245\n",
      "[1523]\ttrain-error:0.35245\n",
      "[1524]\ttrain-error:0.35245\n",
      "[1525]\ttrain-error:0.35245\n",
      "[1526]\ttrain-error:0.35245\n",
      "[1527]\ttrain-error:0.35245\n",
      "[1528]\ttrain-error:0.35245\n",
      "[1529]\ttrain-error:0.35245\n",
      "[1530]\ttrain-error:0.35245\n",
      "[1531]\ttrain-error:0.35245\n",
      "[1532]\ttrain-error:0.35245\n",
      "[1533]\ttrain-error:0.35245\n",
      "[1534]\ttrain-error:0.35245\n",
      "[1535]\ttrain-error:0.35245\n",
      "[1536]\ttrain-error:0.35245\n",
      "[1537]\ttrain-error:0.35245\n",
      "[1538]\ttrain-error:0.35245\n",
      "[1539]\ttrain-error:0.35245\n",
      "[1540]\ttrain-error:0.35245\n",
      "[1541]\ttrain-error:0.35245\n",
      "[1542]\ttrain-error:0.35245\n",
      "[1543]\ttrain-error:0.35245\n",
      "[1544]\ttrain-error:0.35245\n",
      "[1545]\ttrain-error:0.35245\n",
      "[1546]\ttrain-error:0.35245\n",
      "[1547]\ttrain-error:0.35245\n",
      "[1548]\ttrain-error:0.35245\n",
      "[1549]\ttrain-error:0.35245\n",
      "[1550]\ttrain-error:0.35245\n",
      "[1551]\ttrain-error:0.35245\n",
      "[1552]\ttrain-error:0.35245\n",
      "[1553]\ttrain-error:0.35245\n",
      "[1554]\ttrain-error:0.35245\n",
      "[1555]\ttrain-error:0.35245\n",
      "[1556]\ttrain-error:0.35245\n",
      "[1557]\ttrain-error:0.35245\n",
      "[1558]\ttrain-error:0.35245\n",
      "[1559]\ttrain-error:0.35245\n",
      "[1560]\ttrain-error:0.35245\n",
      "[1561]\ttrain-error:0.35245\n",
      "[1562]\ttrain-error:0.35245\n",
      "[1563]\ttrain-error:0.35245\n",
      "[1564]\ttrain-error:0.35245\n",
      "[1565]\ttrain-error:0.35245\n",
      "[1566]\ttrain-error:0.35245\n",
      "[1567]\ttrain-error:0.35245\n",
      "[1568]\ttrain-error:0.35245\n",
      "[1569]\ttrain-error:0.35245\n",
      "[1570]\ttrain-error:0.35245\n",
      "[1571]\ttrain-error:0.35245\n",
      "[1572]\ttrain-error:0.35245\n",
      "[1573]\ttrain-error:0.35245\n",
      "[1574]\ttrain-error:0.35245\n",
      "[1575]\ttrain-error:0.35245\n",
      "[1576]\ttrain-error:0.35245\n",
      "[1577]\ttrain-error:0.35245\n",
      "[1578]\ttrain-error:0.35245\n",
      "[1579]\ttrain-error:0.35245\n",
      "[1580]\ttrain-error:0.35245\n",
      "[1581]\ttrain-error:0.35245\n",
      "[1582]\ttrain-error:0.35245\n",
      "[1583]\ttrain-error:0.35245\n",
      "[1584]\ttrain-error:0.35245\n",
      "[1585]\ttrain-error:0.35245\n",
      "[1586]\ttrain-error:0.35245\n",
      "[1587]\ttrain-error:0.35245\n",
      "[1588]\ttrain-error:0.35245\n",
      "[1589]\ttrain-error:0.35245\n",
      "[1590]\ttrain-error:0.35245\n",
      "[1591]\ttrain-error:0.35245\n",
      "[1592]\ttrain-error:0.35245\n",
      "[1593]\ttrain-error:0.35245\n",
      "[1594]\ttrain-error:0.35245\n",
      "[1595]\ttrain-error:0.35245\n",
      "[1596]\ttrain-error:0.35245\n",
      "[1597]\ttrain-error:0.35245\n",
      "[1598]\ttrain-error:0.35245\n",
      "[1599]\ttrain-error:0.35245\n",
      "[1600]\ttrain-error:0.35245\n",
      "[1601]\ttrain-error:0.35245\n",
      "[1602]\ttrain-error:0.35245\n",
      "[1603]\ttrain-error:0.35245\n",
      "[1604]\ttrain-error:0.35245\n",
      "[1605]\ttrain-error:0.35245\n",
      "[1606]\ttrain-error:0.35245\n",
      "[1607]\ttrain-error:0.35245\n",
      "[1608]\ttrain-error:0.35245\n",
      "[1609]\ttrain-error:0.35245\n",
      "[1610]\ttrain-error:0.35245\n",
      "[1611]\ttrain-error:0.35245\n",
      "[1612]\ttrain-error:0.35245\n",
      "[1613]\ttrain-error:0.35245\n",
      "[1614]\ttrain-error:0.35245\n",
      "[1615]\ttrain-error:0.35245\n",
      "[1616]\ttrain-error:0.35245\n",
      "[1617]\ttrain-error:0.35245\n",
      "[1618]\ttrain-error:0.35245\n",
      "[1619]\ttrain-error:0.35245\n",
      "[1620]\ttrain-error:0.35245\n",
      "[1621]\ttrain-error:0.35245\n",
      "[1622]\ttrain-error:0.35245\n",
      "[1623]\ttrain-error:0.35245\n",
      "[1624]\ttrain-error:0.35245\n",
      "[1625]\ttrain-error:0.35245\n",
      "[1626]\ttrain-error:0.35245\n",
      "[1627]\ttrain-error:0.35245\n",
      "[1628]\ttrain-error:0.35245\n",
      "[1629]\ttrain-error:0.35245\n",
      "[1630]\ttrain-error:0.35245\n",
      "[1631]\ttrain-error:0.35245\n",
      "[1632]\ttrain-error:0.35245\n",
      "[1633]\ttrain-error:0.35245\n",
      "[1634]\ttrain-error:0.35245\n",
      "[1635]\ttrain-error:0.35245\n",
      "[1636]\ttrain-error:0.35245\n",
      "[1637]\ttrain-error:0.35245\n",
      "[1638]\ttrain-error:0.35245\n",
      "[1639]\ttrain-error:0.35245\n",
      "[1640]\ttrain-error:0.35245\n",
      "[1641]\ttrain-error:0.35245\n",
      "[1642]\ttrain-error:0.35245\n",
      "[1643]\ttrain-error:0.35245\n",
      "[1644]\ttrain-error:0.35245\n",
      "[1645]\ttrain-error:0.35245\n",
      "[1646]\ttrain-error:0.35245\n",
      "[1647]\ttrain-error:0.35245\n",
      "[1648]\ttrain-error:0.35245\n",
      "[1649]\ttrain-error:0.35245\n",
      "[1650]\ttrain-error:0.35245\n",
      "[1651]\ttrain-error:0.35245\n",
      "[1652]\ttrain-error:0.35245\n",
      "[1653]\ttrain-error:0.35245\n",
      "[1654]\ttrain-error:0.35245\n",
      "[1655]\ttrain-error:0.35245\n",
      "[1656]\ttrain-error:0.35245\n",
      "[1657]\ttrain-error:0.35245\n",
      "[1658]\ttrain-error:0.35245\n",
      "[1659]\ttrain-error:0.35245\n",
      "[1660]\ttrain-error:0.35245\n",
      "[1661]\ttrain-error:0.35245\n",
      "[1662]\ttrain-error:0.35245\n",
      "[1663]\ttrain-error:0.35245\n",
      "[1664]\ttrain-error:0.35245\n",
      "[1665]\ttrain-error:0.35245\n",
      "[1666]\ttrain-error:0.35245\n",
      "[1667]\ttrain-error:0.35245\n",
      "[1668]\ttrain-error:0.35245\n",
      "[1669]\ttrain-error:0.35245\n",
      "[1670]\ttrain-error:0.35245\n",
      "[1671]\ttrain-error:0.35245\n",
      "[1672]\ttrain-error:0.35245\n",
      "[1673]\ttrain-error:0.35245\n",
      "[1674]\ttrain-error:0.35245\n",
      "[1675]\ttrain-error:0.35245\n",
      "[1676]\ttrain-error:0.35245\n",
      "[1677]\ttrain-error:0.35245\n",
      "[1678]\ttrain-error:0.35245\n",
      "[1679]\ttrain-error:0.35245\n",
      "[1680]\ttrain-error:0.35245\n",
      "[1681]\ttrain-error:0.35245\n",
      "[1682]\ttrain-error:0.35245\n",
      "[1683]\ttrain-error:0.35245\n",
      "[1684]\ttrain-error:0.35245\n",
      "[1685]\ttrain-error:0.35245\n",
      "[1686]\ttrain-error:0.35245\n",
      "[1687]\ttrain-error:0.35245\n",
      "[1688]\ttrain-error:0.35245\n",
      "[1689]\ttrain-error:0.35245\n",
      "[1690]\ttrain-error:0.35245\n",
      "[1691]\ttrain-error:0.35245\n",
      "[1692]\ttrain-error:0.35245\n",
      "[1693]\ttrain-error:0.35245\n",
      "[1694]\ttrain-error:0.35245\n",
      "[1695]\ttrain-error:0.35245\n",
      "[1696]\ttrain-error:0.35245\n",
      "[1697]\ttrain-error:0.35245\n",
      "[1698]\ttrain-error:0.35245\n",
      "[1699]\ttrain-error:0.35245\n",
      "[1700]\ttrain-error:0.35245\n",
      "[1701]\ttrain-error:0.35245\n",
      "[1702]\ttrain-error:0.35245\n",
      "[1703]\ttrain-error:0.35245\n",
      "[1704]\ttrain-error:0.35245\n",
      "[1705]\ttrain-error:0.35245\n",
      "[1706]\ttrain-error:0.35245\n",
      "[1707]\ttrain-error:0.35245\n",
      "[1708]\ttrain-error:0.35245\n",
      "[1709]\ttrain-error:0.35245\n",
      "[1710]\ttrain-error:0.35245\n",
      "[1711]\ttrain-error:0.35245\n",
      "[1712]\ttrain-error:0.35245\n",
      "[1713]\ttrain-error:0.35245\n",
      "[1714]\ttrain-error:0.35245\n",
      "[1715]\ttrain-error:0.35245\n",
      "[1716]\ttrain-error:0.35245\n",
      "[1717]\ttrain-error:0.35245\n",
      "[1718]\ttrain-error:0.35245\n",
      "[1719]\ttrain-error:0.35245\n",
      "[1720]\ttrain-error:0.35245\n",
      "[1721]\ttrain-error:0.35245\n",
      "[1722]\ttrain-error:0.35245\n",
      "[1723]\ttrain-error:0.35245\n",
      "[1724]\ttrain-error:0.35245\n",
      "[1725]\ttrain-error:0.35245\n",
      "[1726]\ttrain-error:0.35245\n",
      "[1727]\ttrain-error:0.35245\n",
      "[1728]\ttrain-error:0.35245\n",
      "[1729]\ttrain-error:0.35245\n",
      "[1730]\ttrain-error:0.35245\n",
      "[1731]\ttrain-error:0.35245\n",
      "[1732]\ttrain-error:0.35245\n",
      "[1733]\ttrain-error:0.35245\n",
      "[1734]\ttrain-error:0.35245\n",
      "[1735]\ttrain-error:0.35245\n",
      "[1736]\ttrain-error:0.35245\n",
      "[1737]\ttrain-error:0.35245\n",
      "[1738]\ttrain-error:0.35245\n",
      "[1739]\ttrain-error:0.35245\n",
      "[1740]\ttrain-error:0.35245\n",
      "[1741]\ttrain-error:0.35245\n",
      "[1742]\ttrain-error:0.35245\n",
      "[1743]\ttrain-error:0.35245\n",
      "[1744]\ttrain-error:0.35245\n",
      "[1745]\ttrain-error:0.35245\n",
      "[1746]\ttrain-error:0.35245\n",
      "[1747]\ttrain-error:0.35245\n",
      "[1748]\ttrain-error:0.35245\n",
      "[1749]\ttrain-error:0.35245\n",
      "[1750]\ttrain-error:0.35245\n",
      "[1751]\ttrain-error:0.35245\n",
      "[1752]\ttrain-error:0.35245\n",
      "[1753]\ttrain-error:0.35245\n",
      "[1754]\ttrain-error:0.35245\n",
      "[1755]\ttrain-error:0.35245\n",
      "[1756]\ttrain-error:0.35245\n",
      "[1757]\ttrain-error:0.35245\n",
      "[1758]\ttrain-error:0.35245\n",
      "[1759]\ttrain-error:0.35245\n",
      "[1760]\ttrain-error:0.35245\n",
      "[1761]\ttrain-error:0.35245\n",
      "[1762]\ttrain-error:0.35245\n",
      "[1763]\ttrain-error:0.35245\n",
      "[1764]\ttrain-error:0.35245\n",
      "[1765]\ttrain-error:0.35245\n",
      "[1766]\ttrain-error:0.35245\n",
      "[1767]\ttrain-error:0.35245\n",
      "[1768]\ttrain-error:0.35245\n",
      "[1769]\ttrain-error:0.35245\n",
      "[1770]\ttrain-error:0.35245\n",
      "[1771]\ttrain-error:0.35245\n",
      "[1772]\ttrain-error:0.35245\n",
      "[1773]\ttrain-error:0.35245\n",
      "[1774]\ttrain-error:0.35245\n",
      "[1775]\ttrain-error:0.35245\n",
      "[1776]\ttrain-error:0.35245\n",
      "[1777]\ttrain-error:0.35245\n",
      "[1778]\ttrain-error:0.35245\n",
      "[1779]\ttrain-error:0.35245\n",
      "[1780]\ttrain-error:0.35245\n",
      "[1781]\ttrain-error:0.35245\n",
      "[1782]\ttrain-error:0.35245\n",
      "[1783]\ttrain-error:0.35245\n",
      "[1784]\ttrain-error:0.35245\n",
      "[1785]\ttrain-error:0.35245\n",
      "[1786]\ttrain-error:0.35245\n",
      "[1787]\ttrain-error:0.35245\n",
      "[1788]\ttrain-error:0.35245\n",
      "[1789]\ttrain-error:0.35245\n",
      "[1790]\ttrain-error:0.35245\n",
      "[1791]\ttrain-error:0.35245\n",
      "[1792]\ttrain-error:0.35245\n",
      "[1793]\ttrain-error:0.35245\n",
      "[1794]\ttrain-error:0.35245\n",
      "[1795]\ttrain-error:0.35245\n",
      "[1796]\ttrain-error:0.35245\n",
      "[1797]\ttrain-error:0.35245\n",
      "[1798]\ttrain-error:0.35245\n",
      "[1799]\ttrain-error:0.35245\n",
      "[1800]\ttrain-error:0.35245\n",
      "[1801]\ttrain-error:0.35245\n",
      "[1802]\ttrain-error:0.35245\n",
      "[1803]\ttrain-error:0.35245\n",
      "[1804]\ttrain-error:0.35245\n",
      "[1805]\ttrain-error:0.35245\n",
      "[1806]\ttrain-error:0.35245\n",
      "[1807]\ttrain-error:0.35245\n",
      "[1808]\ttrain-error:0.35245\n",
      "[1809]\ttrain-error:0.35245\n",
      "[1810]\ttrain-error:0.35245\n",
      "[1811]\ttrain-error:0.35245\n",
      "[1812]\ttrain-error:0.35245\n",
      "[1813]\ttrain-error:0.35245\n",
      "[1814]\ttrain-error:0.35245\n",
      "[1815]\ttrain-error:0.35245\n",
      "[1816]\ttrain-error:0.35245\n",
      "[1817]\ttrain-error:0.35245\n",
      "[1818]\ttrain-error:0.35245\n",
      "[1819]\ttrain-error:0.35245\n",
      "[1820]\ttrain-error:0.35245\n",
      "[1821]\ttrain-error:0.35245\n",
      "[1822]\ttrain-error:0.35245\n",
      "[1823]\ttrain-error:0.35245\n",
      "[1824]\ttrain-error:0.35245\n",
      "[1825]\ttrain-error:0.35245\n",
      "[1826]\ttrain-error:0.35245\n",
      "[1827]\ttrain-error:0.35245\n",
      "[1828]\ttrain-error:0.35245\n",
      "[1829]\ttrain-error:0.35245\n",
      "[1830]\ttrain-error:0.35245\n",
      "[1831]\ttrain-error:0.35245\n",
      "[1832]\ttrain-error:0.35245\n",
      "[1833]\ttrain-error:0.35245\n",
      "[1834]\ttrain-error:0.35245\n",
      "[1835]\ttrain-error:0.35245\n",
      "[1836]\ttrain-error:0.35245\n",
      "[1837]\ttrain-error:0.35245\n",
      "[1838]\ttrain-error:0.35245\n",
      "[1839]\ttrain-error:0.35245\n",
      "[1840]\ttrain-error:0.35245\n",
      "[1841]\ttrain-error:0.35245\n",
      "[1842]\ttrain-error:0.35245\n",
      "[1843]\ttrain-error:0.35245\n",
      "[1844]\ttrain-error:0.35245\n",
      "[1845]\ttrain-error:0.35245\n",
      "[1846]\ttrain-error:0.35245\n",
      "[1847]\ttrain-error:0.35245\n",
      "[1848]\ttrain-error:0.35245\n",
      "[1849]\ttrain-error:0.35245\n",
      "[1850]\ttrain-error:0.35245\n",
      "[1851]\ttrain-error:0.35245\n",
      "[1852]\ttrain-error:0.35245\n",
      "[1853]\ttrain-error:0.35245\n",
      "[1854]\ttrain-error:0.35245\n",
      "[1855]\ttrain-error:0.35245\n",
      "[1856]\ttrain-error:0.35245\n",
      "[1857]\ttrain-error:0.35245\n",
      "[1858]\ttrain-error:0.35245\n",
      "[1859]\ttrain-error:0.35245\n",
      "[1860]\ttrain-error:0.35245\n",
      "[1861]\ttrain-error:0.35245\n",
      "[1862]\ttrain-error:0.35245\n",
      "[1863]\ttrain-error:0.35245\n",
      "[1864]\ttrain-error:0.35245\n",
      "[1865]\ttrain-error:0.35245\n",
      "[1866]\ttrain-error:0.35245\n",
      "[1867]\ttrain-error:0.35245\n",
      "[1868]\ttrain-error:0.35245\n",
      "[1869]\ttrain-error:0.35245\n",
      "[1870]\ttrain-error:0.35245\n",
      "[1871]\ttrain-error:0.35245\n",
      "[1872]\ttrain-error:0.35245\n",
      "[1873]\ttrain-error:0.35245\n",
      "[1874]\ttrain-error:0.35245\n",
      "[1875]\ttrain-error:0.35245\n",
      "[1876]\ttrain-error:0.35245\n",
      "[1877]\ttrain-error:0.35245\n",
      "[1878]\ttrain-error:0.35245\n",
      "[1879]\ttrain-error:0.35245\n",
      "[1880]\ttrain-error:0.35245\n",
      "[1881]\ttrain-error:0.35245\n",
      "[1882]\ttrain-error:0.35245\n",
      "[1883]\ttrain-error:0.35245\n",
      "[1884]\ttrain-error:0.35245\n",
      "[1885]\ttrain-error:0.35245\n",
      "[1886]\ttrain-error:0.35245\n",
      "[1887]\ttrain-error:0.35245\n",
      "[1888]\ttrain-error:0.35245\n",
      "[1889]\ttrain-error:0.35245\n",
      "[1890]\ttrain-error:0.35245\n",
      "[1891]\ttrain-error:0.35245\n",
      "[1892]\ttrain-error:0.35245\n",
      "[1893]\ttrain-error:0.35245\n",
      "[1894]\ttrain-error:0.35245\n",
      "[1895]\ttrain-error:0.35245\n",
      "[1896]\ttrain-error:0.35245\n",
      "[1897]\ttrain-error:0.35245\n",
      "[1898]\ttrain-error:0.35245\n",
      "[1899]\ttrain-error:0.35245\n",
      "[1900]\ttrain-error:0.35245\n",
      "[1901]\ttrain-error:0.35245\n",
      "[1902]\ttrain-error:0.35245\n",
      "[1903]\ttrain-error:0.35245\n",
      "[1904]\ttrain-error:0.35245\n",
      "[1905]\ttrain-error:0.35245\n",
      "[1906]\ttrain-error:0.35245\n",
      "[1907]\ttrain-error:0.35245\n",
      "[1908]\ttrain-error:0.35245\n",
      "[1909]\ttrain-error:0.35245\n",
      "[1910]\ttrain-error:0.35245\n",
      "[1911]\ttrain-error:0.35245\n",
      "[1912]\ttrain-error:0.35245\n",
      "[1913]\ttrain-error:0.35245\n",
      "[1914]\ttrain-error:0.35245\n",
      "[1915]\ttrain-error:0.35245\n",
      "[1916]\ttrain-error:0.35245\n",
      "[1917]\ttrain-error:0.35245\n",
      "[1918]\ttrain-error:0.35245\n",
      "[1919]\ttrain-error:0.35245\n",
      "[1920]\ttrain-error:0.35245\n",
      "[1921]\ttrain-error:0.35245\n",
      "[1922]\ttrain-error:0.35245\n",
      "[1923]\ttrain-error:0.35245\n",
      "[1924]\ttrain-error:0.35245\n",
      "[1925]\ttrain-error:0.35245\n",
      "[1926]\ttrain-error:0.35245\n",
      "[1927]\ttrain-error:0.35245\n",
      "[1928]\ttrain-error:0.35245\n",
      "[1929]\ttrain-error:0.35245\n",
      "[1930]\ttrain-error:0.35245\n",
      "[1931]\ttrain-error:0.35245\n",
      "[1932]\ttrain-error:0.35245\n",
      "[1933]\ttrain-error:0.35245\n",
      "[1934]\ttrain-error:0.35245\n",
      "[1935]\ttrain-error:0.35245\n",
      "[1936]\ttrain-error:0.35245\n",
      "[1937]\ttrain-error:0.35245\n",
      "[1938]\ttrain-error:0.35245\n",
      "[1939]\ttrain-error:0.35245\n",
      "[1940]\ttrain-error:0.35245\n",
      "[1941]\ttrain-error:0.35245\n",
      "[1942]\ttrain-error:0.35245\n",
      "[1943]\ttrain-error:0.35245\n",
      "[1944]\ttrain-error:0.35245\n",
      "[1945]\ttrain-error:0.35245\n",
      "[1946]\ttrain-error:0.35245\n",
      "[1947]\ttrain-error:0.35245\n",
      "[1948]\ttrain-error:0.35245\n",
      "[1949]\ttrain-error:0.35245\n",
      "[1950]\ttrain-error:0.35245\n",
      "[1951]\ttrain-error:0.35245\n",
      "[1952]\ttrain-error:0.35245\n",
      "[1953]\ttrain-error:0.35245\n",
      "[1954]\ttrain-error:0.35245\n",
      "[1955]\ttrain-error:0.35245\n",
      "[1956]\ttrain-error:0.35245\n",
      "[1957]\ttrain-error:0.35245\n",
      "[1958]\ttrain-error:0.35245\n",
      "[1959]\ttrain-error:0.35245\n",
      "[1960]\ttrain-error:0.35245\n",
      "[1961]\ttrain-error:0.35245\n",
      "[1962]\ttrain-error:0.35245\n",
      "[1963]\ttrain-error:0.35245\n",
      "[1964]\ttrain-error:0.35245\n",
      "[1965]\ttrain-error:0.35245\n",
      "[1966]\ttrain-error:0.35245\n",
      "[1967]\ttrain-error:0.35245\n",
      "[1968]\ttrain-error:0.35245\n",
      "[1969]\ttrain-error:0.35245\n",
      "[1970]\ttrain-error:0.35245\n",
      "[1971]\ttrain-error:0.35245\n",
      "[1972]\ttrain-error:0.35245\n",
      "[1973]\ttrain-error:0.35245\n",
      "[1974]\ttrain-error:0.35245\n",
      "[1975]\ttrain-error:0.35245\n",
      "[1976]\ttrain-error:0.35245\n",
      "[1977]\ttrain-error:0.35245\n",
      "[1978]\ttrain-error:0.35245\n",
      "[1979]\ttrain-error:0.35245\n",
      "[1980]\ttrain-error:0.35245\n",
      "[1981]\ttrain-error:0.35245\n",
      "[1982]\ttrain-error:0.35245\n",
      "[1983]\ttrain-error:0.35245\n",
      "[1984]\ttrain-error:0.35245\n",
      "[1985]\ttrain-error:0.35245\n",
      "[1986]\ttrain-error:0.35245\n",
      "[1987]\ttrain-error:0.35245\n",
      "[1988]\ttrain-error:0.35245\n",
      "[1989]\ttrain-error:0.35245\n",
      "[1990]\ttrain-error:0.35245\n",
      "[1991]\ttrain-error:0.35245\n",
      "[1992]\ttrain-error:0.35245\n",
      "[1993]\ttrain-error:0.35245\n",
      "[1994]\ttrain-error:0.35245\n",
      "[1995]\ttrain-error:0.35245\n",
      "[1996]\ttrain-error:0.35245\n",
      "[1997]\ttrain-error:0.35245\n",
      "[1998]\ttrain-error:0.35245\n",
      "[1999]\ttrain-error:0.35245\n",
      "[2000]\ttrain-error:0.35245\n",
      "[2001]\ttrain-error:0.35245\n",
      "[2002]\ttrain-error:0.35245\n",
      "[2003]\ttrain-error:0.35245\n",
      "[2004]\ttrain-error:0.35245\n",
      "[2005]\ttrain-error:0.35245\n",
      "[2006]\ttrain-error:0.35245\n",
      "[2007]\ttrain-error:0.35245\n",
      "[2008]\ttrain-error:0.35245\n",
      "[2009]\ttrain-error:0.35245\n",
      "[2010]\ttrain-error:0.35245\n",
      "[2011]\ttrain-error:0.35245\n",
      "[2012]\ttrain-error:0.35245\n",
      "[2013]\ttrain-error:0.35245\n",
      "[2014]\ttrain-error:0.35245\n",
      "[2015]\ttrain-error:0.35245\n",
      "[2016]\ttrain-error:0.35245\n",
      "[2017]\ttrain-error:0.35245\n",
      "[2018]\ttrain-error:0.35245\n",
      "[2019]\ttrain-error:0.35245\n",
      "[2020]\ttrain-error:0.35245\n",
      "[2021]\ttrain-error:0.35245\n",
      "[2022]\ttrain-error:0.35245\n",
      "[2023]\ttrain-error:0.35245\n",
      "[2024]\ttrain-error:0.35245\n",
      "[2025]\ttrain-error:0.35245\n",
      "[2026]\ttrain-error:0.35245\n",
      "[2027]\ttrain-error:0.35245\n",
      "[2028]\ttrain-error:0.35245\n",
      "[2029]\ttrain-error:0.35245\n",
      "[2030]\ttrain-error:0.35245\n",
      "[2031]\ttrain-error:0.35245\n",
      "[2032]\ttrain-error:0.35245\n",
      "[2033]\ttrain-error:0.35245\n",
      "[2034]\ttrain-error:0.35245\n",
      "[2035]\ttrain-error:0.35245\n",
      "[2036]\ttrain-error:0.35245\n",
      "[2037]\ttrain-error:0.35245\n",
      "[2038]\ttrain-error:0.35245\n",
      "[2039]\ttrain-error:0.35245\n",
      "[2040]\ttrain-error:0.35245\n",
      "[2041]\ttrain-error:0.35245\n",
      "[2042]\ttrain-error:0.35245\n",
      "[2043]\ttrain-error:0.35245\n",
      "[2044]\ttrain-error:0.35245\n",
      "[2045]\ttrain-error:0.35245\n",
      "[2046]\ttrain-error:0.35245\n",
      "[2047]\ttrain-error:0.35245\n",
      "[2048]\ttrain-error:0.35245\n",
      "[2049]\ttrain-error:0.35245\n",
      "[2050]\ttrain-error:0.35245\n",
      "[2051]\ttrain-error:0.35245\n",
      "[2052]\ttrain-error:0.35245\n",
      "[2053]\ttrain-error:0.35245\n",
      "[2054]\ttrain-error:0.35245\n",
      "[2055]\ttrain-error:0.35245\n",
      "[2056]\ttrain-error:0.35245\n",
      "[2057]\ttrain-error:0.35245\n",
      "[2058]\ttrain-error:0.35245\n",
      "[2059]\ttrain-error:0.35245\n",
      "[2060]\ttrain-error:0.35245\n",
      "[2061]\ttrain-error:0.35245\n",
      "[2062]\ttrain-error:0.35245\n",
      "[2063]\ttrain-error:0.35245\n",
      "[2064]\ttrain-error:0.35245\n",
      "[2065]\ttrain-error:0.35245\n",
      "[2066]\ttrain-error:0.35245\n",
      "[2067]\ttrain-error:0.35245\n",
      "[2068]\ttrain-error:0.35245\n",
      "[2069]\ttrain-error:0.35245\n",
      "[2070]\ttrain-error:0.35245\n",
      "[2071]\ttrain-error:0.35245\n",
      "[2072]\ttrain-error:0.35245\n",
      "[2073]\ttrain-error:0.35245\n",
      "[2074]\ttrain-error:0.35245\n",
      "[2075]\ttrain-error:0.35245\n",
      "[2076]\ttrain-error:0.35245\n",
      "[2077]\ttrain-error:0.35245\n",
      "[2078]\ttrain-error:0.35245\n",
      "[2079]\ttrain-error:0.35245\n",
      "[2080]\ttrain-error:0.35245\n",
      "[2081]\ttrain-error:0.35245\n",
      "[2082]\ttrain-error:0.35245\n",
      "[2083]\ttrain-error:0.35245\n",
      "[2084]\ttrain-error:0.35245\n",
      "[2085]\ttrain-error:0.35245\n",
      "[2086]\ttrain-error:0.35245\n",
      "[2087]\ttrain-error:0.35245\n",
      "[2088]\ttrain-error:0.35245\n",
      "[2089]\ttrain-error:0.35245\n",
      "[2090]\ttrain-error:0.35245\n",
      "[2091]\ttrain-error:0.35245\n",
      "[2092]\ttrain-error:0.35245\n",
      "[2093]\ttrain-error:0.35245\n",
      "[2094]\ttrain-error:0.35245\n",
      "[2095]\ttrain-error:0.35245\n",
      "[2096]\ttrain-error:0.35245\n",
      "[2097]\ttrain-error:0.35245\n",
      "[2098]\ttrain-error:0.35245\n",
      "[2099]\ttrain-error:0.35245\n",
      "[2100]\ttrain-error:0.35245\n",
      "[2101]\ttrain-error:0.35245\n",
      "[2102]\ttrain-error:0.35245\n",
      "[2103]\ttrain-error:0.35245\n",
      "[2104]\ttrain-error:0.35245\n",
      "[2105]\ttrain-error:0.35245\n",
      "[2106]\ttrain-error:0.35245\n",
      "[2107]\ttrain-error:0.35245\n",
      "[2108]\ttrain-error:0.35245\n",
      "[2109]\ttrain-error:0.35245\n",
      "[2110]\ttrain-error:0.35245\n",
      "[2111]\ttrain-error:0.35245\n",
      "[2112]\ttrain-error:0.35245\n",
      "[2113]\ttrain-error:0.35245\n",
      "[2114]\ttrain-error:0.35245\n",
      "[2115]\ttrain-error:0.35245\n",
      "[2116]\ttrain-error:0.35245\n",
      "[2117]\ttrain-error:0.35245\n",
      "[2118]\ttrain-error:0.35245\n",
      "[2119]\ttrain-error:0.35245\n",
      "[2120]\ttrain-error:0.35245\n",
      "[2121]\ttrain-error:0.35245\n",
      "[2122]\ttrain-error:0.35245\n",
      "[2123]\ttrain-error:0.35245\n",
      "[2124]\ttrain-error:0.35245\n",
      "[2125]\ttrain-error:0.35245\n",
      "[2126]\ttrain-error:0.35245\n",
      "[2127]\ttrain-error:0.35245\n",
      "[2128]\ttrain-error:0.35245\n",
      "[2129]\ttrain-error:0.35245\n",
      "[2130]\ttrain-error:0.35245\n",
      "[2131]\ttrain-error:0.35245\n",
      "[2132]\ttrain-error:0.35245\n",
      "[2133]\ttrain-error:0.35245\n",
      "[2134]\ttrain-error:0.35245\n",
      "[2135]\ttrain-error:0.35245\n",
      "[2136]\ttrain-error:0.35245\n",
      "[2137]\ttrain-error:0.35245\n",
      "[2138]\ttrain-error:0.35245\n",
      "[2139]\ttrain-error:0.35245\n",
      "[2140]\ttrain-error:0.35245\n",
      "[2141]\ttrain-error:0.35245\n",
      "[2142]\ttrain-error:0.35245\n",
      "[2143]\ttrain-error:0.35245\n",
      "[2144]\ttrain-error:0.35245\n",
      "[2145]\ttrain-error:0.35245\n",
      "[2146]\ttrain-error:0.35245\n",
      "[2147]\ttrain-error:0.35245\n",
      "[2148]\ttrain-error:0.35245\n",
      "[2149]\ttrain-error:0.35245\n",
      "[2150]\ttrain-error:0.35245\n",
      "[2151]\ttrain-error:0.35245\n",
      "[2152]\ttrain-error:0.35245\n",
      "[2153]\ttrain-error:0.35245\n",
      "[2154]\ttrain-error:0.35245\n",
      "[2155]\ttrain-error:0.35245\n",
      "[2156]\ttrain-error:0.35245\n",
      "[2157]\ttrain-error:0.35245\n",
      "[2158]\ttrain-error:0.35245\n",
      "[2159]\ttrain-error:0.35245\n",
      "[2160]\ttrain-error:0.35245\n",
      "[2161]\ttrain-error:0.35245\n",
      "[2162]\ttrain-error:0.35245\n",
      "[2163]\ttrain-error:0.35245\n",
      "[2164]\ttrain-error:0.35245\n",
      "[2165]\ttrain-error:0.35245\n",
      "[2166]\ttrain-error:0.35245\n",
      "[2167]\ttrain-error:0.35245\n",
      "[2168]\ttrain-error:0.35245\n",
      "[2169]\ttrain-error:0.35245\n",
      "[2170]\ttrain-error:0.35245\n",
      "[2171]\ttrain-error:0.35245\n",
      "[2172]\ttrain-error:0.35245\n",
      "[2173]\ttrain-error:0.35245\n",
      "[2174]\ttrain-error:0.35245\n",
      "[2175]\ttrain-error:0.35245\n",
      "[2176]\ttrain-error:0.35245\n",
      "[2177]\ttrain-error:0.35245\n",
      "[2178]\ttrain-error:0.35245\n",
      "[2179]\ttrain-error:0.35245\n",
      "[2180]\ttrain-error:0.35245\n",
      "[2181]\ttrain-error:0.35245\n",
      "[2182]\ttrain-error:0.35245\n",
      "[2183]\ttrain-error:0.35245\n",
      "[2184]\ttrain-error:0.35245\n",
      "[2185]\ttrain-error:0.35245\n",
      "[2186]\ttrain-error:0.35245\n",
      "[2187]\ttrain-error:0.35245\n",
      "[2188]\ttrain-error:0.35245\n",
      "[2189]\ttrain-error:0.35245\n",
      "[2190]\ttrain-error:0.35245\n",
      "[2191]\ttrain-error:0.35245\n",
      "[2192]\ttrain-error:0.35245\n",
      "[2193]\ttrain-error:0.35245\n",
      "[2194]\ttrain-error:0.35245\n",
      "[2195]\ttrain-error:0.35245\n",
      "[2196]\ttrain-error:0.35245\n",
      "[2197]\ttrain-error:0.35245\n",
      "[2198]\ttrain-error:0.35245\n",
      "[2199]\ttrain-error:0.35245\n",
      "[2200]\ttrain-error:0.35245\n",
      "[2201]\ttrain-error:0.35245\n",
      "[2202]\ttrain-error:0.35245\n",
      "[2203]\ttrain-error:0.35245\n",
      "[2204]\ttrain-error:0.35245\n",
      "[2205]\ttrain-error:0.35245\n",
      "[2206]\ttrain-error:0.35245\n",
      "[2207]\ttrain-error:0.35245\n",
      "[2208]\ttrain-error:0.35245\n",
      "[2209]\ttrain-error:0.35245\n",
      "[2210]\ttrain-error:0.35245\n",
      "[2211]\ttrain-error:0.35245\n",
      "[2212]\ttrain-error:0.35245\n",
      "[2213]\ttrain-error:0.35245\n",
      "[2214]\ttrain-error:0.35245\n",
      "[2215]\ttrain-error:0.35245\n",
      "[2216]\ttrain-error:0.35245\n",
      "[2217]\ttrain-error:0.35245\n",
      "[2218]\ttrain-error:0.35245\n",
      "[2219]\ttrain-error:0.35245\n",
      "[2220]\ttrain-error:0.35245\n",
      "[2221]\ttrain-error:0.35245\n",
      "[2222]\ttrain-error:0.35245\n",
      "[2223]\ttrain-error:0.35245\n",
      "[2224]\ttrain-error:0.35245\n",
      "[2225]\ttrain-error:0.35245\n",
      "[2226]\ttrain-error:0.35245\n",
      "[2227]\ttrain-error:0.35245\n",
      "[2228]\ttrain-error:0.35245\n",
      "[2229]\ttrain-error:0.35245\n",
      "[2230]\ttrain-error:0.35245\n",
      "[2231]\ttrain-error:0.35245\n",
      "[2232]\ttrain-error:0.35245\n",
      "[2233]\ttrain-error:0.35245\n",
      "[2234]\ttrain-error:0.35245\n",
      "[2235]\ttrain-error:0.35245\n",
      "[2236]\ttrain-error:0.35245\n",
      "[2237]\ttrain-error:0.35245\n",
      "[2238]\ttrain-error:0.35245\n",
      "[2239]\ttrain-error:0.35245\n",
      "[2240]\ttrain-error:0.35245\n",
      "[2241]\ttrain-error:0.35245\n",
      "[2242]\ttrain-error:0.35245\n",
      "[2243]\ttrain-error:0.35245\n",
      "[2244]\ttrain-error:0.35245\n",
      "[2245]\ttrain-error:0.35245\n",
      "[2246]\ttrain-error:0.35245\n",
      "[2247]\ttrain-error:0.35245\n",
      "[2248]\ttrain-error:0.35245\n",
      "[2249]\ttrain-error:0.35245\n",
      "[2250]\ttrain-error:0.35245\n",
      "[2251]\ttrain-error:0.35245\n",
      "[2252]\ttrain-error:0.35245\n",
      "[2253]\ttrain-error:0.35245\n",
      "[2254]\ttrain-error:0.35245\n",
      "[2255]\ttrain-error:0.35245\n",
      "[2256]\ttrain-error:0.35245\n",
      "[2257]\ttrain-error:0.35245\n",
      "[2258]\ttrain-error:0.35245\n",
      "[2259]\ttrain-error:0.35245\n",
      "[2260]\ttrain-error:0.35245\n",
      "[2261]\ttrain-error:0.35245\n",
      "[2262]\ttrain-error:0.35245\n",
      "[2263]\ttrain-error:0.35245\n",
      "[2264]\ttrain-error:0.35245\n",
      "[2265]\ttrain-error:0.35245\n",
      "[2266]\ttrain-error:0.35245\n",
      "[2267]\ttrain-error:0.35245\n",
      "[2268]\ttrain-error:0.35245\n",
      "[2269]\ttrain-error:0.35245\n",
      "[2270]\ttrain-error:0.35245\n",
      "[2271]\ttrain-error:0.35245\n",
      "[2272]\ttrain-error:0.35245\n",
      "[2273]\ttrain-error:0.35245\n",
      "[2274]\ttrain-error:0.35245\n",
      "[2275]\ttrain-error:0.35245\n",
      "[2276]\ttrain-error:0.35245\n",
      "[2277]\ttrain-error:0.35245\n",
      "[2278]\ttrain-error:0.35245\n",
      "[2279]\ttrain-error:0.35245\n",
      "[2280]\ttrain-error:0.35245\n",
      "[2281]\ttrain-error:0.35245\n",
      "[2282]\ttrain-error:0.35245\n",
      "[2283]\ttrain-error:0.35245\n",
      "[2284]\ttrain-error:0.35245\n",
      "[2285]\ttrain-error:0.35245\n",
      "[2286]\ttrain-error:0.35245\n",
      "[2287]\ttrain-error:0.35245\n",
      "[2288]\ttrain-error:0.35245\n",
      "[2289]\ttrain-error:0.35245\n",
      "[2290]\ttrain-error:0.35245\n",
      "[2291]\ttrain-error:0.35245\n",
      "[2292]\ttrain-error:0.35245\n",
      "[2293]\ttrain-error:0.35245\n",
      "[2294]\ttrain-error:0.35245\n",
      "[2295]\ttrain-error:0.35245\n",
      "[2296]\ttrain-error:0.35245\n",
      "[2297]\ttrain-error:0.35245\n",
      "[2298]\ttrain-error:0.35245\n",
      "[2299]\ttrain-error:0.35245\n",
      "[2300]\ttrain-error:0.35245\n",
      "[2301]\ttrain-error:0.35245\n",
      "[2302]\ttrain-error:0.35245\n",
      "[2303]\ttrain-error:0.35245\n",
      "[2304]\ttrain-error:0.35245\n",
      "[2305]\ttrain-error:0.35245\n",
      "[2306]\ttrain-error:0.35245\n",
      "[2307]\ttrain-error:0.35245\n",
      "[2308]\ttrain-error:0.35245\n",
      "[2309]\ttrain-error:0.35245\n",
      "[2310]\ttrain-error:0.35245\n",
      "[2311]\ttrain-error:0.35245\n",
      "[2312]\ttrain-error:0.35245\n",
      "[2313]\ttrain-error:0.35245\n",
      "[2314]\ttrain-error:0.35245\n",
      "[2315]\ttrain-error:0.35245\n",
      "[2316]\ttrain-error:0.35245\n",
      "[2317]\ttrain-error:0.35245\n",
      "[2318]\ttrain-error:0.35245\n",
      "[2319]\ttrain-error:0.35245\n",
      "[2320]\ttrain-error:0.35245\n",
      "[2321]\ttrain-error:0.35245\n",
      "[2322]\ttrain-error:0.35245\n",
      "[2323]\ttrain-error:0.35245\n",
      "[2324]\ttrain-error:0.35245\n",
      "[2325]\ttrain-error:0.35245\n",
      "[2326]\ttrain-error:0.35245\n",
      "[2327]\ttrain-error:0.35245\n",
      "[2328]\ttrain-error:0.35245\n",
      "[2329]\ttrain-error:0.35245\n",
      "[2330]\ttrain-error:0.35245\n",
      "[2331]\ttrain-error:0.35245\n",
      "[2332]\ttrain-error:0.35245\n",
      "[2333]\ttrain-error:0.35245\n",
      "[2334]\ttrain-error:0.35245\n",
      "[2335]\ttrain-error:0.35245\n",
      "[2336]\ttrain-error:0.35245\n",
      "[2337]\ttrain-error:0.35245\n",
      "[2338]\ttrain-error:0.35245\n",
      "[2339]\ttrain-error:0.35245\n",
      "[2340]\ttrain-error:0.35245\n",
      "[2341]\ttrain-error:0.35245\n",
      "[2342]\ttrain-error:0.35245\n",
      "[2343]\ttrain-error:0.35245\n",
      "[2344]\ttrain-error:0.35245\n",
      "[2345]\ttrain-error:0.35245\n",
      "[2346]\ttrain-error:0.35245\n",
      "[2347]\ttrain-error:0.35245\n",
      "[2348]\ttrain-error:0.35245\n",
      "[2349]\ttrain-error:0.35245\n",
      "[2350]\ttrain-error:0.35245\n",
      "[2351]\ttrain-error:0.35245\n",
      "[2352]\ttrain-error:0.35245\n",
      "[2353]\ttrain-error:0.35245\n",
      "[2354]\ttrain-error:0.35245\n",
      "[2355]\ttrain-error:0.35245\n",
      "[2356]\ttrain-error:0.35245\n",
      "[2357]\ttrain-error:0.35245\n",
      "[2358]\ttrain-error:0.35245\n",
      "[2359]\ttrain-error:0.35245\n",
      "[2360]\ttrain-error:0.35245\n",
      "[2361]\ttrain-error:0.35245\n",
      "[2362]\ttrain-error:0.35245\n",
      "[2363]\ttrain-error:0.35245\n",
      "[2364]\ttrain-error:0.35245\n",
      "[2365]\ttrain-error:0.35245\n",
      "[2366]\ttrain-error:0.35245\n",
      "[2367]\ttrain-error:0.35245\n",
      "[2368]\ttrain-error:0.35245\n",
      "[2369]\ttrain-error:0.35245\n",
      "[2370]\ttrain-error:0.35245\n",
      "[2371]\ttrain-error:0.35245\n",
      "[2372]\ttrain-error:0.35245\n",
      "[2373]\ttrain-error:0.35245\n",
      "[2374]\ttrain-error:0.35245\n",
      "[2375]\ttrain-error:0.35245\n",
      "[2376]\ttrain-error:0.35245\n",
      "[2377]\ttrain-error:0.35245\n",
      "[2378]\ttrain-error:0.35245\n",
      "[2379]\ttrain-error:0.35245\n",
      "[2380]\ttrain-error:0.35245\n",
      "[2381]\ttrain-error:0.35245\n",
      "[2382]\ttrain-error:0.35245\n",
      "[2383]\ttrain-error:0.35245\n",
      "[2384]\ttrain-error:0.35245\n",
      "[2385]\ttrain-error:0.35245\n",
      "[2386]\ttrain-error:0.35245\n",
      "[2387]\ttrain-error:0.35245\n",
      "[2388]\ttrain-error:0.35245\n",
      "[2389]\ttrain-error:0.35245\n",
      "[2390]\ttrain-error:0.35245\n",
      "[2391]\ttrain-error:0.35245\n",
      "[2392]\ttrain-error:0.35245\n",
      "[2393]\ttrain-error:0.35245\n",
      "[2394]\ttrain-error:0.35245\n",
      "[2395]\ttrain-error:0.35245\n",
      "[2396]\ttrain-error:0.35245\n",
      "[2397]\ttrain-error:0.35245\n",
      "[2398]\ttrain-error:0.35245\n",
      "[2399]\ttrain-error:0.35245\n",
      "[2400]\ttrain-error:0.35245\n",
      "[2401]\ttrain-error:0.35245\n",
      "[2402]\ttrain-error:0.35245\n",
      "[2403]\ttrain-error:0.35245\n",
      "[2404]\ttrain-error:0.35245\n",
      "[2405]\ttrain-error:0.35245\n",
      "[2406]\ttrain-error:0.35245\n",
      "[2407]\ttrain-error:0.35245\n",
      "[2408]\ttrain-error:0.35245\n",
      "[2409]\ttrain-error:0.35245\n",
      "[2410]\ttrain-error:0.35245\n",
      "[2411]\ttrain-error:0.35245\n",
      "[2412]\ttrain-error:0.35245\n",
      "[2413]\ttrain-error:0.35245\n",
      "[2414]\ttrain-error:0.35245\n",
      "[2415]\ttrain-error:0.35245\n",
      "[2416]\ttrain-error:0.35245\n",
      "[2417]\ttrain-error:0.35245\n",
      "[2418]\ttrain-error:0.35245\n",
      "[2419]\ttrain-error:0.35245\n",
      "[2420]\ttrain-error:0.35245\n",
      "[2421]\ttrain-error:0.35245\n",
      "[2422]\ttrain-error:0.35245\n",
      "[2423]\ttrain-error:0.35245\n",
      "[2424]\ttrain-error:0.35245\n",
      "[2425]\ttrain-error:0.35245\n",
      "[2426]\ttrain-error:0.35245\n",
      "[2427]\ttrain-error:0.35245\n",
      "[2428]\ttrain-error:0.35245\n",
      "[2429]\ttrain-error:0.35245\n",
      "[2430]\ttrain-error:0.35245\n",
      "[2431]\ttrain-error:0.35245\n",
      "[2432]\ttrain-error:0.35245\n",
      "[2433]\ttrain-error:0.35245\n",
      "[2434]\ttrain-error:0.35245\n",
      "[2435]\ttrain-error:0.35245\n",
      "[2436]\ttrain-error:0.35245\n",
      "[2437]\ttrain-error:0.35245\n",
      "[2438]\ttrain-error:0.35245\n",
      "[2439]\ttrain-error:0.35245\n",
      "[2440]\ttrain-error:0.35245\n",
      "[2441]\ttrain-error:0.35245\n",
      "[2442]\ttrain-error:0.35245\n",
      "[2443]\ttrain-error:0.35245\n",
      "[2444]\ttrain-error:0.35245\n",
      "[2445]\ttrain-error:0.35245\n",
      "[2446]\ttrain-error:0.35245\n",
      "[2447]\ttrain-error:0.35245\n",
      "[2448]\ttrain-error:0.35245\n",
      "[2449]\ttrain-error:0.35245\n",
      "[2450]\ttrain-error:0.35245\n",
      "[2451]\ttrain-error:0.35245\n",
      "[2452]\ttrain-error:0.35245\n",
      "[2453]\ttrain-error:0.35245\n",
      "[2454]\ttrain-error:0.35245\n",
      "[2455]\ttrain-error:0.35245\n",
      "[2456]\ttrain-error:0.35245\n",
      "[2457]\ttrain-error:0.35245\n",
      "[2458]\ttrain-error:0.35245\n",
      "[2459]\ttrain-error:0.35245\n",
      "[2460]\ttrain-error:0.35245\n",
      "[2461]\ttrain-error:0.35245\n",
      "[2462]\ttrain-error:0.35245\n",
      "[2463]\ttrain-error:0.35245\n",
      "[2464]\ttrain-error:0.35245\n",
      "[2465]\ttrain-error:0.35245\n",
      "[2466]\ttrain-error:0.35245\n",
      "[2467]\ttrain-error:0.35245\n",
      "[2468]\ttrain-error:0.35245\n",
      "[2469]\ttrain-error:0.35245\n",
      "[2470]\ttrain-error:0.35245\n",
      "[2471]\ttrain-error:0.35245\n",
      "[2472]\ttrain-error:0.35245\n",
      "[2473]\ttrain-error:0.35245\n",
      "[2474]\ttrain-error:0.35245\n",
      "[2475]\ttrain-error:0.35245\n",
      "[2476]\ttrain-error:0.35245\n",
      "[2477]\ttrain-error:0.35245\n",
      "[2478]\ttrain-error:0.35245\n",
      "[2479]\ttrain-error:0.35245\n",
      "[2480]\ttrain-error:0.35245\n",
      "[2481]\ttrain-error:0.35245\n",
      "[2482]\ttrain-error:0.35245\n",
      "[2483]\ttrain-error:0.35245\n",
      "[2484]\ttrain-error:0.35245\n",
      "[2485]\ttrain-error:0.35245\n",
      "[2486]\ttrain-error:0.35245\n",
      "[2487]\ttrain-error:0.35245\n",
      "[2488]\ttrain-error:0.35245\n",
      "[2489]\ttrain-error:0.35245\n",
      "[2490]\ttrain-error:0.35245\n",
      "[2491]\ttrain-error:0.35245\n",
      "[2492]\ttrain-error:0.35245\n",
      "[2493]\ttrain-error:0.35245\n",
      "[2494]\ttrain-error:0.35245\n",
      "[2495]\ttrain-error:0.35245\n",
      "[2496]\ttrain-error:0.35245\n",
      "[2497]\ttrain-error:0.35245\n",
      "[2498]\ttrain-error:0.35245\n",
      "[2499]\ttrain-error:0.35245\n",
      "[2500]\ttrain-error:0.35245\n",
      "[2501]\ttrain-error:0.35245\n",
      "[2502]\ttrain-error:0.35245\n",
      "[2503]\ttrain-error:0.35245\n",
      "[2504]\ttrain-error:0.35245\n",
      "[2505]\ttrain-error:0.35245\n",
      "[2506]\ttrain-error:0.35245\n",
      "[2507]\ttrain-error:0.35245\n",
      "[2508]\ttrain-error:0.35245\n",
      "[2509]\ttrain-error:0.35245\n",
      "[2510]\ttrain-error:0.35245\n",
      "[2511]\ttrain-error:0.35245\n",
      "[2512]\ttrain-error:0.35245\n",
      "[2513]\ttrain-error:0.35245\n",
      "[2514]\ttrain-error:0.35245\n",
      "[2515]\ttrain-error:0.35245\n",
      "[2516]\ttrain-error:0.35245\n",
      "[2517]\ttrain-error:0.35245\n",
      "[2518]\ttrain-error:0.35245\n",
      "[2519]\ttrain-error:0.35245\n",
      "[2520]\ttrain-error:0.35245\n",
      "[2521]\ttrain-error:0.35245\n",
      "[2522]\ttrain-error:0.35245\n",
      "[2523]\ttrain-error:0.35245\n",
      "[2524]\ttrain-error:0.35245\n",
      "[2525]\ttrain-error:0.35245\n",
      "[2526]\ttrain-error:0.35245\n",
      "[2527]\ttrain-error:0.35245\n",
      "[2528]\ttrain-error:0.35245\n",
      "[2529]\ttrain-error:0.35245\n",
      "[2530]\ttrain-error:0.35245\n",
      "[2531]\ttrain-error:0.35245\n",
      "[2532]\ttrain-error:0.35245\n",
      "[2533]\ttrain-error:0.35245\n",
      "[2534]\ttrain-error:0.35245\n",
      "[2535]\ttrain-error:0.35245\n",
      "[2536]\ttrain-error:0.35245\n",
      "[2537]\ttrain-error:0.35245\n",
      "[2538]\ttrain-error:0.35245\n",
      "[2539]\ttrain-error:0.35245\n",
      "[2540]\ttrain-error:0.35245\n",
      "[2541]\ttrain-error:0.35245\n",
      "[2542]\ttrain-error:0.35245\n",
      "[2543]\ttrain-error:0.35245\n",
      "[2544]\ttrain-error:0.35245\n",
      "[2545]\ttrain-error:0.35245\n",
      "[2546]\ttrain-error:0.35245\n",
      "[2547]\ttrain-error:0.35245\n",
      "[2548]\ttrain-error:0.35245\n",
      "[2549]\ttrain-error:0.35245\n",
      "[2550]\ttrain-error:0.35245\n",
      "[2551]\ttrain-error:0.35245\n",
      "[2552]\ttrain-error:0.35245\n",
      "[2553]\ttrain-error:0.35245\n",
      "[2554]\ttrain-error:0.35245\n",
      "[2555]\ttrain-error:0.35245\n",
      "[2556]\ttrain-error:0.35245\n",
      "[2557]\ttrain-error:0.35245\n",
      "[2558]\ttrain-error:0.35245\n",
      "[2559]\ttrain-error:0.35245\n",
      "[2560]\ttrain-error:0.35245\n",
      "[2561]\ttrain-error:0.35245\n",
      "[2562]\ttrain-error:0.35245\n",
      "[2563]\ttrain-error:0.35245\n",
      "[2564]\ttrain-error:0.35245\n",
      "[2565]\ttrain-error:0.35245\n",
      "[2566]\ttrain-error:0.35245\n",
      "[2567]\ttrain-error:0.35245\n",
      "[2568]\ttrain-error:0.35245\n",
      "[2569]\ttrain-error:0.35245\n",
      "[2570]\ttrain-error:0.35245\n",
      "[2571]\ttrain-error:0.35245\n",
      "[2572]\ttrain-error:0.35245\n",
      "[2573]\ttrain-error:0.35245\n",
      "[2574]\ttrain-error:0.35245\n",
      "[2575]\ttrain-error:0.35245\n",
      "[2576]\ttrain-error:0.35245\n",
      "[2577]\ttrain-error:0.35245\n",
      "[2578]\ttrain-error:0.35245\n",
      "[2579]\ttrain-error:0.35245\n",
      "[2580]\ttrain-error:0.35245\n",
      "[2581]\ttrain-error:0.35245\n",
      "[2582]\ttrain-error:0.35245\n",
      "[2583]\ttrain-error:0.35245\n",
      "[2584]\ttrain-error:0.35245\n",
      "[2585]\ttrain-error:0.35245\n",
      "[2586]\ttrain-error:0.35245\n",
      "[2587]\ttrain-error:0.35245\n",
      "[2588]\ttrain-error:0.35245\n",
      "[2589]\ttrain-error:0.35245\n",
      "[2590]\ttrain-error:0.35245\n",
      "[2591]\ttrain-error:0.35245\n",
      "[2592]\ttrain-error:0.35245\n",
      "[2593]\ttrain-error:0.35245\n",
      "[2594]\ttrain-error:0.35245\n",
      "[2595]\ttrain-error:0.35245\n",
      "[2596]\ttrain-error:0.35245\n",
      "[2597]\ttrain-error:0.35245\n",
      "[2598]\ttrain-error:0.35245\n",
      "[2599]\ttrain-error:0.35245\n",
      "[2600]\ttrain-error:0.35245\n",
      "[2601]\ttrain-error:0.35245\n",
      "[2602]\ttrain-error:0.35245\n",
      "[2603]\ttrain-error:0.35245\n",
      "[2604]\ttrain-error:0.35245\n",
      "[2605]\ttrain-error:0.35245\n",
      "[2606]\ttrain-error:0.35245\n",
      "[2607]\ttrain-error:0.35245\n",
      "[2608]\ttrain-error:0.35245\n",
      "[2609]\ttrain-error:0.35245\n",
      "[2610]\ttrain-error:0.35245\n",
      "[2611]\ttrain-error:0.35245\n",
      "[2612]\ttrain-error:0.35245\n",
      "[2613]\ttrain-error:0.35245\n",
      "[2614]\ttrain-error:0.35245\n",
      "[2615]\ttrain-error:0.35245\n",
      "[2616]\ttrain-error:0.35245\n",
      "[2617]\ttrain-error:0.35245\n",
      "[2618]\ttrain-error:0.35245\n",
      "[2619]\ttrain-error:0.35245\n",
      "[2620]\ttrain-error:0.35245\n",
      "[2621]\ttrain-error:0.35245\n",
      "[2622]\ttrain-error:0.35245\n",
      "[2623]\ttrain-error:0.35245\n",
      "[2624]\ttrain-error:0.35245\n",
      "[2625]\ttrain-error:0.35245\n",
      "[2626]\ttrain-error:0.35245\n",
      "[2627]\ttrain-error:0.35245\n",
      "[2628]\ttrain-error:0.35245\n",
      "[2629]\ttrain-error:0.35245\n",
      "[2630]\ttrain-error:0.35245\n",
      "[2631]\ttrain-error:0.35245\n",
      "[2632]\ttrain-error:0.35245\n",
      "[2633]\ttrain-error:0.35245\n",
      "[2634]\ttrain-error:0.35245\n",
      "[2635]\ttrain-error:0.35245\n",
      "[2636]\ttrain-error:0.35245\n",
      "[2637]\ttrain-error:0.35245\n",
      "[2638]\ttrain-error:0.35245\n",
      "[2639]\ttrain-error:0.35245\n",
      "[2640]\ttrain-error:0.35245\n",
      "[2641]\ttrain-error:0.35245\n",
      "[2642]\ttrain-error:0.35245\n",
      "[2643]\ttrain-error:0.35245\n",
      "[2644]\ttrain-error:0.35245\n",
      "[2645]\ttrain-error:0.35245\n",
      "[2646]\ttrain-error:0.35245\n",
      "[2647]\ttrain-error:0.35245\n",
      "[2648]\ttrain-error:0.35245\n",
      "[2649]\ttrain-error:0.35245\n",
      "[2650]\ttrain-error:0.35245\n",
      "[2651]\ttrain-error:0.35245\n",
      "[2652]\ttrain-error:0.35245\n",
      "[2653]\ttrain-error:0.35245\n",
      "[2654]\ttrain-error:0.35245\n",
      "[2655]\ttrain-error:0.35245\n",
      "[2656]\ttrain-error:0.35245\n",
      "[2657]\ttrain-error:0.35245\n",
      "[2658]\ttrain-error:0.35245\n",
      "[2659]\ttrain-error:0.35245\n",
      "[2660]\ttrain-error:0.35245\n",
      "[2661]\ttrain-error:0.35245\n",
      "[2662]\ttrain-error:0.35245\n",
      "[2663]\ttrain-error:0.35245\n",
      "[2664]\ttrain-error:0.35245\n",
      "[2665]\ttrain-error:0.35245\n",
      "[2666]\ttrain-error:0.35245\n",
      "[2667]\ttrain-error:0.35245\n",
      "[2668]\ttrain-error:0.35245\n",
      "[2669]\ttrain-error:0.35245\n",
      "[2670]\ttrain-error:0.35245\n",
      "[2671]\ttrain-error:0.35245\n",
      "[2672]\ttrain-error:0.35245\n",
      "[2673]\ttrain-error:0.35245\n",
      "[2674]\ttrain-error:0.35245\n",
      "[2675]\ttrain-error:0.35245\n",
      "[2676]\ttrain-error:0.35245\n",
      "[2677]\ttrain-error:0.35245\n",
      "[2678]\ttrain-error:0.35245\n",
      "[2679]\ttrain-error:0.35245\n",
      "[2680]\ttrain-error:0.35245\n",
      "[2681]\ttrain-error:0.35245\n",
      "[2682]\ttrain-error:0.35245\n",
      "[2683]\ttrain-error:0.35245\n",
      "[2684]\ttrain-error:0.35245\n",
      "[2685]\ttrain-error:0.35245\n",
      "[2686]\ttrain-error:0.35245\n",
      "[2687]\ttrain-error:0.35245\n",
      "[2688]\ttrain-error:0.35245\n",
      "[2689]\ttrain-error:0.35245\n",
      "[2690]\ttrain-error:0.35245\n",
      "[2691]\ttrain-error:0.35245\n",
      "[2692]\ttrain-error:0.35245\n",
      "[2693]\ttrain-error:0.35245\n",
      "[2694]\ttrain-error:0.35245\n",
      "[2695]\ttrain-error:0.35245\n",
      "[2696]\ttrain-error:0.35245\n",
      "[2697]\ttrain-error:0.35245\n",
      "[2698]\ttrain-error:0.35245\n",
      "[2699]\ttrain-error:0.35245\n",
      "[2700]\ttrain-error:0.35245\n",
      "[2701]\ttrain-error:0.35245\n",
      "[2702]\ttrain-error:0.35245\n",
      "[2703]\ttrain-error:0.35245\n",
      "[2704]\ttrain-error:0.35245\n",
      "[2705]\ttrain-error:0.35245\n",
      "[2706]\ttrain-error:0.35245\n",
      "[2707]\ttrain-error:0.35245\n",
      "[2708]\ttrain-error:0.35245\n",
      "[2709]\ttrain-error:0.35245\n",
      "[2710]\ttrain-error:0.35245\n",
      "[2711]\ttrain-error:0.35245\n",
      "[2712]\ttrain-error:0.35245\n",
      "[2713]\ttrain-error:0.35245\n",
      "[2714]\ttrain-error:0.35245\n",
      "[2715]\ttrain-error:0.35245\n",
      "[2716]\ttrain-error:0.35245\n",
      "[2717]\ttrain-error:0.35245\n",
      "[2718]\ttrain-error:0.35245\n",
      "[2719]\ttrain-error:0.35245\n",
      "[2720]\ttrain-error:0.35245\n",
      "[2721]\ttrain-error:0.35245\n",
      "[2722]\ttrain-error:0.35245\n",
      "[2723]\ttrain-error:0.35245\n",
      "[2724]\ttrain-error:0.35245\n",
      "[2725]\ttrain-error:0.35245\n",
      "[2726]\ttrain-error:0.35245\n",
      "[2727]\ttrain-error:0.35245\n",
      "[2728]\ttrain-error:0.35245\n",
      "[2729]\ttrain-error:0.35245\n",
      "[2730]\ttrain-error:0.35245\n",
      "[2731]\ttrain-error:0.35245\n",
      "[2732]\ttrain-error:0.35245\n",
      "[2733]\ttrain-error:0.35245\n",
      "[2734]\ttrain-error:0.35245\n",
      "[2735]\ttrain-error:0.35245\n",
      "[2736]\ttrain-error:0.35245\n",
      "[2737]\ttrain-error:0.35245\n",
      "[2738]\ttrain-error:0.35245\n",
      "[2739]\ttrain-error:0.35245\n",
      "[2740]\ttrain-error:0.35245\n",
      "[2741]\ttrain-error:0.35245\n",
      "[2742]\ttrain-error:0.35245\n",
      "[2743]\ttrain-error:0.35245\n",
      "[2744]\ttrain-error:0.35245\n",
      "[2745]\ttrain-error:0.35245\n",
      "[2746]\ttrain-error:0.35245\n",
      "[2747]\ttrain-error:0.35245\n",
      "[2748]\ttrain-error:0.35245\n",
      "[2749]\ttrain-error:0.35245\n",
      "[2750]\ttrain-error:0.35245\n",
      "[2751]\ttrain-error:0.35245\n",
      "[2752]\ttrain-error:0.35245\n",
      "[2753]\ttrain-error:0.35245\n",
      "[2754]\ttrain-error:0.35245\n",
      "[2755]\ttrain-error:0.35245\n",
      "[2756]\ttrain-error:0.35245\n",
      "[2757]\ttrain-error:0.35245\n",
      "[2758]\ttrain-error:0.35245\n",
      "[2759]\ttrain-error:0.35245\n",
      "[2760]\ttrain-error:0.35245\n",
      "[2761]\ttrain-error:0.35245\n",
      "[2762]\ttrain-error:0.35245\n",
      "[2763]\ttrain-error:0.35245\n",
      "[2764]\ttrain-error:0.35245\n",
      "[2765]\ttrain-error:0.35245\n",
      "[2766]\ttrain-error:0.35245\n",
      "[2767]\ttrain-error:0.35245\n",
      "[2768]\ttrain-error:0.35245\n",
      "[2769]\ttrain-error:0.35245\n",
      "[2770]\ttrain-error:0.35245\n",
      "[2771]\ttrain-error:0.35245\n",
      "[2772]\ttrain-error:0.35245\n",
      "[2773]\ttrain-error:0.35245\n",
      "[2774]\ttrain-error:0.35245\n",
      "[2775]\ttrain-error:0.35245\n",
      "[2776]\ttrain-error:0.35245\n",
      "[2777]\ttrain-error:0.35245\n",
      "[2778]\ttrain-error:0.35245\n",
      "[2779]\ttrain-error:0.35245\n",
      "[2780]\ttrain-error:0.35245\n",
      "[2781]\ttrain-error:0.35245\n",
      "[2782]\ttrain-error:0.35245\n",
      "[2783]\ttrain-error:0.35245\n",
      "[2784]\ttrain-error:0.35245\n",
      "[2785]\ttrain-error:0.35245\n",
      "[2786]\ttrain-error:0.35245\n",
      "[2787]\ttrain-error:0.35245\n",
      "[2788]\ttrain-error:0.35245\n",
      "[2789]\ttrain-error:0.35245\n",
      "[2790]\ttrain-error:0.35245\n",
      "[2791]\ttrain-error:0.35245\n",
      "[2792]\ttrain-error:0.35245\n",
      "[2793]\ttrain-error:0.35245\n",
      "[2794]\ttrain-error:0.35245\n",
      "[2795]\ttrain-error:0.35245\n",
      "[2796]\ttrain-error:0.35245\n",
      "[2797]\ttrain-error:0.35245\n",
      "[2798]\ttrain-error:0.35245\n",
      "[2799]\ttrain-error:0.35245\n",
      "[2800]\ttrain-error:0.35245\n",
      "[2801]\ttrain-error:0.35245\n",
      "[2802]\ttrain-error:0.35245\n",
      "[2803]\ttrain-error:0.35245\n",
      "[2804]\ttrain-error:0.35245\n",
      "[2805]\ttrain-error:0.35245\n",
      "[2806]\ttrain-error:0.35245\n",
      "[2807]\ttrain-error:0.35245\n",
      "[2808]\ttrain-error:0.35245\n",
      "[2809]\ttrain-error:0.35245\n",
      "[2810]\ttrain-error:0.35245\n",
      "[2811]\ttrain-error:0.35245\n",
      "[2812]\ttrain-error:0.35245\n",
      "[2813]\ttrain-error:0.35245\n",
      "[2814]\ttrain-error:0.35245\n",
      "[2815]\ttrain-error:0.35245\n",
      "[2816]\ttrain-error:0.35245\n",
      "[2817]\ttrain-error:0.35245\n",
      "[2818]\ttrain-error:0.35245\n",
      "[2819]\ttrain-error:0.35245\n",
      "[2820]\ttrain-error:0.35245\n",
      "[2821]\ttrain-error:0.35245\n",
      "[2822]\ttrain-error:0.35245\n",
      "[2823]\ttrain-error:0.35245\n",
      "[2824]\ttrain-error:0.35245\n",
      "[2825]\ttrain-error:0.35245\n",
      "[2826]\ttrain-error:0.35245\n",
      "[2827]\ttrain-error:0.35245\n",
      "[2828]\ttrain-error:0.35245\n",
      "[2829]\ttrain-error:0.35245\n",
      "[2830]\ttrain-error:0.35245\n",
      "[2831]\ttrain-error:0.35245\n",
      "[2832]\ttrain-error:0.35245\n",
      "[2833]\ttrain-error:0.35245\n",
      "[2834]\ttrain-error:0.35245\n",
      "[2835]\ttrain-error:0.35245\n",
      "[2836]\ttrain-error:0.35245\n",
      "[2837]\ttrain-error:0.35245\n",
      "[2838]\ttrain-error:0.35245\n",
      "[2839]\ttrain-error:0.35245\n",
      "[2840]\ttrain-error:0.35245\n",
      "[2841]\ttrain-error:0.35245\n",
      "[2842]\ttrain-error:0.35245\n",
      "[2843]\ttrain-error:0.35245\n",
      "[2844]\ttrain-error:0.35245\n",
      "[2845]\ttrain-error:0.35245\n",
      "[2846]\ttrain-error:0.35245\n",
      "[2847]\ttrain-error:0.35245\n",
      "[2848]\ttrain-error:0.35245\n",
      "[2849]\ttrain-error:0.35245\n",
      "[2850]\ttrain-error:0.35245\n",
      "[2851]\ttrain-error:0.35245\n",
      "[2852]\ttrain-error:0.35245\n",
      "[2853]\ttrain-error:0.35245\n",
      "[2854]\ttrain-error:0.35245\n",
      "[2855]\ttrain-error:0.35245\n",
      "[2856]\ttrain-error:0.35245\n",
      "[2857]\ttrain-error:0.35245\n",
      "[2858]\ttrain-error:0.35245\n",
      "[2859]\ttrain-error:0.35245\n",
      "[2860]\ttrain-error:0.35245\n",
      "[2861]\ttrain-error:0.35245\n",
      "[2862]\ttrain-error:0.35245\n",
      "[2863]\ttrain-error:0.35245\n",
      "[2864]\ttrain-error:0.35245\n",
      "[2865]\ttrain-error:0.35245\n",
      "[2866]\ttrain-error:0.35245\n",
      "[2867]\ttrain-error:0.35245\n",
      "[2868]\ttrain-error:0.35245\n",
      "[2869]\ttrain-error:0.35245\n",
      "[2870]\ttrain-error:0.35245\n",
      "[2871]\ttrain-error:0.35245\n",
      "[2872]\ttrain-error:0.35245\n",
      "[2873]\ttrain-error:0.35245\n",
      "[2874]\ttrain-error:0.35245\n",
      "[2875]\ttrain-error:0.35245\n",
      "[2876]\ttrain-error:0.35245\n",
      "[2877]\ttrain-error:0.35245\n",
      "[2878]\ttrain-error:0.35245\n",
      "[2879]\ttrain-error:0.35245\n",
      "[2880]\ttrain-error:0.35245\n",
      "[2881]\ttrain-error:0.35245\n",
      "[2882]\ttrain-error:0.35245\n",
      "[2883]\ttrain-error:0.35245\n",
      "[2884]\ttrain-error:0.35245\n",
      "[2885]\ttrain-error:0.35245\n",
      "[2886]\ttrain-error:0.35245\n",
      "[2887]\ttrain-error:0.35245\n",
      "[2888]\ttrain-error:0.35245\n",
      "[2889]\ttrain-error:0.35245\n",
      "[2890]\ttrain-error:0.35245\n",
      "[2891]\ttrain-error:0.35245\n",
      "[2892]\ttrain-error:0.35245\n",
      "[2893]\ttrain-error:0.35245\n",
      "[2894]\ttrain-error:0.35245\n",
      "[2895]\ttrain-error:0.35245\n",
      "[2896]\ttrain-error:0.35245\n",
      "[2897]\ttrain-error:0.35245\n",
      "[2898]\ttrain-error:0.35245\n",
      "[2899]\ttrain-error:0.35245\n",
      "[2900]\ttrain-error:0.35245\n",
      "[2901]\ttrain-error:0.35245\n",
      "[2902]\ttrain-error:0.35245\n",
      "[2903]\ttrain-error:0.35245\n",
      "[2904]\ttrain-error:0.35245\n",
      "[2905]\ttrain-error:0.35245\n",
      "[2906]\ttrain-error:0.35245\n",
      "[2907]\ttrain-error:0.35245\n",
      "[2908]\ttrain-error:0.35245\n",
      "[2909]\ttrain-error:0.35245\n",
      "[2910]\ttrain-error:0.35245\n",
      "[2911]\ttrain-error:0.35245\n",
      "[2912]\ttrain-error:0.35245\n",
      "[2913]\ttrain-error:0.35245\n",
      "[2914]\ttrain-error:0.35245\n",
      "[2915]\ttrain-error:0.35245\n",
      "[2916]\ttrain-error:0.35245\n",
      "[2917]\ttrain-error:0.35245\n",
      "[2918]\ttrain-error:0.35245\n",
      "[2919]\ttrain-error:0.35245\n",
      "[2920]\ttrain-error:0.35245\n",
      "[2921]\ttrain-error:0.35245\n",
      "[2922]\ttrain-error:0.35245\n",
      "[2923]\ttrain-error:0.35245\n",
      "[2924]\ttrain-error:0.35245\n",
      "[2925]\ttrain-error:0.35245\n",
      "[2926]\ttrain-error:0.35245\n",
      "[2927]\ttrain-error:0.35245\n",
      "[2928]\ttrain-error:0.35245\n",
      "[2929]\ttrain-error:0.35245\n",
      "[2930]\ttrain-error:0.35245\n",
      "[2931]\ttrain-error:0.35245\n",
      "[2932]\ttrain-error:0.35245\n",
      "[2933]\ttrain-error:0.35245\n",
      "[2934]\ttrain-error:0.35245\n",
      "[2935]\ttrain-error:0.35245\n",
      "[2936]\ttrain-error:0.35245\n",
      "[2937]\ttrain-error:0.35245\n",
      "[2938]\ttrain-error:0.35245\n",
      "[2939]\ttrain-error:0.35245\n",
      "[2940]\ttrain-error:0.35245\n",
      "[2941]\ttrain-error:0.35245\n",
      "[2942]\ttrain-error:0.35245\n",
      "[2943]\ttrain-error:0.35245\n",
      "[2944]\ttrain-error:0.35245\n",
      "[2945]\ttrain-error:0.35245\n",
      "[2946]\ttrain-error:0.35245\n",
      "[2947]\ttrain-error:0.35245\n",
      "[2948]\ttrain-error:0.35245\n",
      "[2949]\ttrain-error:0.35245\n",
      "[2950]\ttrain-error:0.35245\n",
      "[2951]\ttrain-error:0.35245\n",
      "[2952]\ttrain-error:0.35245\n",
      "[2953]\ttrain-error:0.35245\n",
      "[2954]\ttrain-error:0.35245\n",
      "[2955]\ttrain-error:0.35245\n",
      "[2956]\ttrain-error:0.35245\n",
      "[2957]\ttrain-error:0.35245\n",
      "[2958]\ttrain-error:0.35245\n",
      "[2959]\ttrain-error:0.35245\n",
      "[2960]\ttrain-error:0.35245\n",
      "[2961]\ttrain-error:0.35245\n",
      "[2962]\ttrain-error:0.35245\n",
      "[2963]\ttrain-error:0.35245\n",
      "[2964]\ttrain-error:0.35245\n",
      "[2965]\ttrain-error:0.35245\n",
      "[2966]\ttrain-error:0.35245\n",
      "[2967]\ttrain-error:0.35245\n",
      "[2968]\ttrain-error:0.35245\n",
      "[2969]\ttrain-error:0.35245\n",
      "[2970]\ttrain-error:0.35245\n",
      "[2971]\ttrain-error:0.35245\n",
      "[2972]\ttrain-error:0.35245\n",
      "[2973]\ttrain-error:0.35245\n",
      "[2974]\ttrain-error:0.35245\n",
      "[2975]\ttrain-error:0.35245\n",
      "[2976]\ttrain-error:0.35245\n",
      "[2977]\ttrain-error:0.35245\n",
      "[2978]\ttrain-error:0.35245\n",
      "[2979]\ttrain-error:0.35245\n",
      "[2980]\ttrain-error:0.35245\n",
      "[2981]\ttrain-error:0.35245\n",
      "[2982]\ttrain-error:0.35245\n",
      "[2983]\ttrain-error:0.35245\n",
      "[2984]\ttrain-error:0.35245\n",
      "[2985]\ttrain-error:0.35245\n",
      "[2986]\ttrain-error:0.35245\n",
      "[2987]\ttrain-error:0.35245\n",
      "[2988]\ttrain-error:0.35245\n",
      "[2989]\ttrain-error:0.35245\n",
      "[2990]\ttrain-error:0.35245\n",
      "[2991]\ttrain-error:0.35245\n",
      "[2992]\ttrain-error:0.35245\n",
      "[2993]\ttrain-error:0.35245\n",
      "[2994]\ttrain-error:0.35245\n",
      "[2995]\ttrain-error:0.35245\n",
      "[2996]\ttrain-error:0.35245\n",
      "[2997]\ttrain-error:0.35245\n",
      "[2998]\ttrain-error:0.35245\n",
      "[2999]\ttrain-error:0.35245\n",
      "\n",
      "train error: 0.352445\n",
      "train accuracy: 0.647555\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "y_train_true, labels_dict = convert_class_labels(y_train_first_decisions, labels_dict=labels_dict)\n",
    "\n",
    "dtrain = xgb.DMatrix(x_train_first_decisions, label=y_train_true)\n",
    "dvalidation = [(xgb.DMatrix(x_train_first_decisions, label=y_train_true),'train')]\n",
    "\n",
    "if min_profit_percent==profit_noise_percent:\n",
    "    # binrary classification problem (buy or sell)\n",
    "    error_metric_name = 'error'\n",
    "    xgb_params = {'max_depth':max_depth, 'learning_rate':learning_rate, 'objective':'binary:logistic', 'eval_metric': error_metric_name, 'gamma':gamma,\n",
    "                  'colsample_bytree':colsample_bytree, 'subsample':subsample}\n",
    "else:\n",
    "    # multi-class classification problem (buy, sell, or wiat)\n",
    "    error_metric_name = 'merror'\n",
    "    xgb_params = {'max_depth':max_depth, 'learning_rate':learning_rate, 'objective':'multi:softmax', 'num_class': num_class,\n",
    "                  'eval_metric': error_metric_name, 'gamma':gamma, 'colsample_bytree':colsample_bytree, 'subsample':subsample}\n",
    "evals_result = {}\n",
    "xgb_first_decision_predictor = xgb.train(xgb_params, dtrain, num_boost_round=n_estimators, evals=dvalidation, evals_result=evals_result)\n",
    "\n",
    "# print train error\n",
    "train_error = evals_result['train']['error'][-1]\n",
    "print(f'\\ntrain error: {train_error}')\n",
    "print(f'train accuracy: {1 - train_error}')\n",
    "\n",
    "# save model\n",
    "xgb_first_decision_predictor.save_model(f'../my_stuff/{cur_pair}-{timeframe}_{min_profit_percent}-min_profit_{lots_per_trade}-lots_{currency_side}-cur_side'\n",
    "                                        f'_{tenkan_period}-{kijun_period}-{senkou_b_period}-{sigs_for_filename}-ichi_xgb_classifier.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 1320 rows of tick data from C:\\GitHub Repos\\ForexMachine\\Data\\.cache\\mt5_EURUSD_h1_ticks_2020-10-02T00;00UTC_to_2020-12-18T00;00UTC.csv\n",
      "saved 1320 rows of EURUSD h1 tick data to C:\\GitHub Repos\\ForexMachine\\Data\\RawData\\mt5_EURUSD_h1_ticks_2020-10-02T00;00UTC_to_2020-12-18T00;00UTC.csv, done.\n",
      "\n",
      "test error: 0.32158590308370044\n",
      "test accuracy: 0.6784140969162995\n",
      "potential profits from test data: 81816.39999999983\n",
      "buy/sell counts:\n",
      "buy     193\n",
      "sell     34\n",
      "Name: first_decision, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# test model on test data\n",
    "tick_data_filepath = research.download_mt5_data(\"EURUSD\", 'H1', '2020-10-02', '2020-12-18')\n",
    "data_with_indicators = research.add_indicators_to_raw(filepath=tick_data_filepath, \n",
    "                                                      indicators_info=indicators_info, \n",
    "                                                      datetime_col='datetime')\n",
    "test_data = research.add_ichimoku_features(data_with_indicators)\n",
    "\n",
    "test_data_labels = generate_ichimoku_labels(test_data, label_non_signals=label_non_signals, min_profit_percent=min_profit_percent, \n",
    "                                             profit_noise_percent=profit_noise_percent, signals_to_consider=signals_to_consider, \n",
    "                                             contract_size=contract_size, lots_per_trade=lots_per_trade,\n",
    "                                             in_quote_currency=in_quote_currency,pip_resolution=pip_resolution)\n",
    "\n",
    "test_data = apply_perc_change(test_data, cols=pc_cols, limit=1)\n",
    "start_idx, end_idx = no_missing_data_idx_range(test_data, early_ending_cols=['chikou_span_visual'])\n",
    "test_data = test_data.iloc[start_idx:end_idx+1]\n",
    "test_data_labels = test_data_labels.iloc[start_idx:end_idx+1]\n",
    "\n",
    "x_test_first_decisions, y_test_first_decisions = missing_labels_preprocess(test_data, test_data_labels, 'first_decision')\n",
    "x_test_first_decisions_profits, y_test_first_decisions_profits = missing_labels_preprocess(test_data, test_data_labels, 'best_profit_first_decision')\n",
    "\n",
    "y_test_true, labels_dict = convert_class_labels(y_test_first_decisions, to_numpy=True, labels_dict=labels_dict)\n",
    "\n",
    "dtest = xgb.DMatrix(x_test_first_decisions)\n",
    "y_test_probs = xgb_first_decision_predictor.predict(dtest)\n",
    "\n",
    "y_test_preds = np.around(y_test_probs)\n",
    "y_test_preds = pd.DataFrame(y_test_preds, columns=y_test_first_decisions.columns)\n",
    "y_test_preds = convert_class_labels(y_test_preds, to_ints=False, labels_dict=labels_dict)[0]\n",
    "\n",
    "# print results\n",
    "test_error, test_wrong_indices = error_rate(y_test_first_decisions, y_test_preds)\n",
    "p_profits_first_decision = potention_profits(y_test_first_decisions, y_test_preds, y_test_first_decisions_profits)\n",
    "\n",
    "print(f'\\ntest error: {test_error}')\n",
    "print(f'test accuracy: {1 - test_error}')\n",
    "print(f'potential profits from test data: {p_profits_first_decision}')\n",
    "print(f'buy/sell counts:\\n{y_test_preds[\"first_decision\"].value_counts()}')\n",
    "\n",
    "x = x_test_first_decisions.to_numpy()\n",
    "ytp = y_test_preds[\"first_decision\"].to_numpy()\n",
    "\n",
    "sell_inputs = []\n",
    "for i in range(len(ytp)):\n",
    "    if ytp[i] == 'sell':\n",
    "        sell_inputs.append(x[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### analyze binary probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'buy', 0: 'sell'} \n",
      "\n",
      "CORRECT: true label=1, prob=0.5895295143127441,0.41047048568725586\n",
      "CORRECT: true label=1, prob=0.5297814607620239,0.4702185392379761\n",
      "CORRECT: true label=1, prob=0.545541524887085,0.45445847511291504\n",
      "CORRECT: true label=1, prob=0.5556892156600952,0.4443107843399048\n",
      "CORRECT: true label=1, prob=0.5123186707496643,0.4876813292503357\n",
      "WRONG: true label=0, prob=0.5582850575447083,0.44171494245529175\n",
      "WRONG: true label=0, prob=0.5660315752029419,0.4339684247970581\n",
      "WRONG: true label=0, prob=0.5056567788124084,0.49434322118759155\n",
      "WRONG: true label=0, prob=0.5227710008621216,0.4772289991378784\n",
      "WRONG: true label=0, prob=0.5327770709991455,0.4672229290008545\n",
      "WRONG: true label=0, prob=0.5528644919395447,0.4471355080604553\n",
      "WRONG: true label=0, prob=0.6500375866889954,0.34996241331100464\n",
      "WRONG: true label=0, prob=0.5231617093086243,0.47683829069137573\n",
      "CORRECT: true label=1, prob=0.5124702453613281,0.4875297546386719\n",
      "WRONG: true label=1, prob=0.4195222854614258,0.4195222854614258\n",
      "CORRECT: true label=1, prob=0.5706438422203064,0.4293561577796936\n",
      "CORRECT: true label=1, prob=0.5406209230422974,0.45937907695770264\n",
      "WRONG: true label=1, prob=0.4979456663131714,0.4979456663131714\n",
      "CORRECT: true label=1, prob=0.5688799619674683,0.43112003803253174\n",
      "WRONG: true label=1, prob=0.43966972827911377,0.43966972827911377\n",
      "CORRECT: true label=1, prob=0.520990252494812,0.479009747505188\n",
      "CORRECT: true label=1, prob=0.5010769963264465,0.49892300367355347\n",
      "CORRECT: true label=1, prob=0.6185513734817505,0.3814486265182495\n",
      "CORRECT: true label=1, prob=0.5808225870132446,0.41917741298675537\n",
      "CORRECT: true label=1, prob=0.6010205745697021,0.39897942543029785\n",
      "CORRECT: true label=1, prob=0.5473210215568542,0.45267897844314575\n",
      "CORRECT: true label=1, prob=0.6631102561950684,0.33688974380493164\n",
      "CORRECT: true label=1, prob=0.5281584858894348,0.4718415141105652\n",
      "CORRECT: true label=1, prob=0.5995698571205139,0.4004301428794861\n",
      "CORRECT: true label=1, prob=0.5592742562294006,0.44072574377059937\n",
      "CORRECT: true label=1, prob=0.5449569821357727,0.4550430178642273\n",
      "CORRECT: true label=1, prob=0.5747401118278503,0.42525988817214966\n",
      "CORRECT: true label=1, prob=0.6426474452018738,0.3573525547981262\n",
      "WRONG: true label=1, prob=0.45803505182266235,0.45803505182266235\n",
      "WRONG: true label=1, prob=0.4736989736557007,0.4736989736557007\n",
      "WRONG: true label=0, prob=0.5700356960296631,0.4299643039703369\n",
      "WRONG: true label=0, prob=0.5647643804550171,0.4352356195449829\n",
      "WRONG: true label=0, prob=0.5927563905715942,0.40724360942840576\n",
      "WRONG: true label=0, prob=0.6473605036735535,0.35263949632644653\n",
      "WRONG: true label=0, prob=0.6686452627182007,0.3313547372817993\n",
      "WRONG: true label=0, prob=0.5112097263336182,0.48879027366638184\n",
      "WRONG: true label=0, prob=0.6022337675094604,0.39776623249053955\n",
      "WRONG: true label=0, prob=0.5251675844192505,0.4748324155807495\n",
      "WRONG: true label=0, prob=0.5741732716560364,0.4258267283439636\n",
      "WRONG: true label=0, prob=0.5670830607414246,0.43291693925857544\n",
      "WRONG: true label=0, prob=0.5979282855987549,0.4020717144012451\n",
      "WRONG: true label=0, prob=0.5621824264526367,0.4378175735473633\n",
      "WRONG: true label=0, prob=0.5766111016273499,0.42338889837265015\n",
      "WRONG: true label=0, prob=0.507339358329773,0.49266064167022705\n",
      "WRONG: true label=0, prob=0.5777232050895691,0.4222767949104309\n",
      "WRONG: true label=0, prob=0.5432785153388977,0.4567214846611023\n",
      "WRONG: true label=0, prob=0.5879079699516296,0.41209203004837036\n",
      "WRONG: true label=0, prob=0.591177225112915,0.40882277488708496\n",
      "WRONG: true label=0, prob=0.5204887986183167,0.47951120138168335\n",
      "WRONG: true label=0, prob=0.5420020222663879,0.45799797773361206\n",
      "WRONG: true label=0, prob=0.5031957030296326,0.49680429697036743\n",
      "WRONG: true label=0, prob=0.5652128458023071,0.43478715419769287\n",
      "WRONG: true label=0, prob=0.5664938688278198,0.4335061311721802\n",
      "WRONG: true label=0, prob=0.6217448711395264,0.37825512886047363\n",
      "WRONG: true label=0, prob=0.6187580227851868,0.38124197721481323\n",
      "WRONG: true label=0, prob=0.598871648311615,0.401128351688385\n",
      "WRONG: true label=0, prob=0.6009926795959473,0.39900732040405273\n",
      "WRONG: true label=0, prob=0.5261088013648987,0.4738911986351013\n",
      "WRONG: true label=0, prob=0.5660077333450317,0.43399226665496826\n",
      "WRONG: true label=0, prob=0.5105741024017334,0.4894258975982666\n",
      "WRONG: true label=0, prob=0.5773093104362488,0.4226906895637512\n",
      "WRONG: true label=0, prob=0.561493456363678,0.438506543636322\n",
      "CORRECT: true label=0, prob=0.4797033965587616,0.4797033965587616\n",
      "CORRECT: true label=1, prob=0.5816168189048767,0.4183831810951233\n",
      "CORRECT: true label=1, prob=0.5643792748451233,0.4356207251548767\n",
      "CORRECT: true label=1, prob=0.6164213418960571,0.38357865810394287\n",
      "WRONG: true label=1, prob=0.49137789011001587,0.49137789011001587\n",
      "CORRECT: true label=1, prob=0.5193009376525879,0.4806990623474121\n",
      "WRONG: true label=1, prob=0.49113526940345764,0.49113526940345764\n",
      "CORRECT: true label=1, prob=0.6360850930213928,0.3639149069786072\n",
      "CORRECT: true label=1, prob=0.5647526979446411,0.4352473020553589\n",
      "CORRECT: true label=1, prob=0.7250692844390869,0.2749307155609131\n",
      "CORRECT: true label=1, prob=0.6063408255577087,0.39365917444229126\n",
      "CORRECT: true label=1, prob=0.5219616293907166,0.47803837060928345\n",
      "CORRECT: true label=1, prob=0.5027216076850891,0.4972783923149109\n",
      "CORRECT: true label=1, prob=0.5216562747955322,0.4783437252044678\n",
      "CORRECT: true label=1, prob=0.6099354028701782,0.3900645971298218\n",
      "WRONG: true label=1, prob=0.47787922620773315,0.47787922620773315\n",
      "CORRECT: true label=0, prob=0.3289063274860382,0.3289063274860382\n",
      "CORRECT: true label=1, prob=0.6745615601539612,0.3254384398460388\n",
      "CORRECT: true label=1, prob=0.7030783891677856,0.29692161083221436\n",
      "WRONG: true label=1, prob=0.3754454553127289,0.3754454553127289\n",
      "WRONG: true label=1, prob=0.2905483841896057,0.2905483841896057\n",
      "CORRECT: true label=1, prob=0.6336533427238464,0.36634665727615356\n",
      "CORRECT: true label=1, prob=0.5418124794960022,0.4581875205039978\n",
      "CORRECT: true label=1, prob=0.5382311344146729,0.46176886558532715\n",
      "CORRECT: true label=1, prob=0.6099249124526978,0.39007508754730225\n",
      "CORRECT: true label=1, prob=0.5613350868225098,0.43866491317749023\n",
      "CORRECT: true label=1, prob=0.5371860861778259,0.4628139138221741\n",
      "CORRECT: true label=1, prob=0.5206968188285828,0.47930318117141724\n",
      "CORRECT: true label=1, prob=0.5876682996749878,0.4123317003250122\n",
      "WRONG: true label=1, prob=0.4183788597583771,0.4183788597583771\n",
      "WRONG: true label=1, prob=0.4969276487827301,0.4969276487827301\n",
      "CORRECT: true label=1, prob=0.5501113533973694,0.4498886466026306\n",
      "CORRECT: true label=1, prob=0.567900538444519,0.43209946155548096\n",
      "CORRECT: true label=1, prob=0.6128256916999817,0.3871743083000183\n",
      "CORRECT: true label=1, prob=0.5276408791542053,0.4723591208457947\n",
      "CORRECT: true label=1, prob=0.5534044504165649,0.44659554958343506\n",
      "CORRECT: true label=1, prob=0.5380335450172424,0.46196645498275757\n",
      "WRONG: true label=1, prob=0.4873075783252716,0.4873075783252716\n",
      "CORRECT: true label=1, prob=0.5396273732185364,0.4603726267814636\n",
      "WRONG: true label=1, prob=0.4833562970161438,0.4833562970161438\n",
      "WRONG: true label=1, prob=0.40731728076934814,0.40731728076934814\n",
      "WRONG: true label=1, prob=0.4996383488178253,0.4996383488178253\n",
      "CORRECT: true label=1, prob=0.5881766080856323,0.4118233919143677\n",
      "CORRECT: true label=1, prob=0.589648962020874,0.410351037979126\n",
      "CORRECT: true label=1, prob=0.551570475101471,0.44842952489852905\n",
      "CORRECT: true label=1, prob=0.5135141611099243,0.4864858388900757\n",
      "CORRECT: true label=1, prob=0.5309388041496277,0.4690611958503723\n",
      "CORRECT: true label=1, prob=0.5249996185302734,0.47500038146972656\n",
      "CORRECT: true label=1, prob=0.5646517276763916,0.4353482723236084\n",
      "CORRECT: true label=1, prob=0.5865254402160645,0.41347455978393555\n",
      "CORRECT: true label=1, prob=0.5665123462677002,0.4334876537322998\n",
      "WRONG: true label=1, prob=0.49343010783195496,0.49343010783195496\n",
      "CORRECT: true label=1, prob=0.5792703628540039,0.4207296371459961\n",
      "CORRECT: true label=1, prob=0.5508081912994385,0.4491918087005615\n",
      "CORRECT: true label=1, prob=0.5349986553192139,0.46500134468078613\n",
      "CORRECT: true label=1, prob=0.5612311363220215,0.4387688636779785\n",
      "CORRECT: true label=1, prob=0.5422376394271851,0.45776236057281494\n",
      "CORRECT: true label=1, prob=0.6010702252388,0.39892977476119995\n",
      "WRONG: true label=1, prob=0.4790061116218567,0.4790061116218567\n",
      "CORRECT: true label=1, prob=0.5813272595405579,0.41867274045944214\n",
      "CORRECT: true label=1, prob=0.5310149192810059,0.46898508071899414\n",
      "WRONG: true label=1, prob=0.4586901366710663,0.4586901366710663\n",
      "WRONG: true label=1, prob=0.4920947849750519,0.4920947849750519\n",
      "WRONG: true label=1, prob=0.4501139521598816,0.4501139521598816\n",
      "CORRECT: true label=1, prob=0.5367591977119446,0.4632408022880554\n",
      "CORRECT: true label=1, prob=0.5368509888648987,0.4631490111351013\n",
      "CORRECT: true label=1, prob=0.5024515986442566,0.4975484013557434\n",
      "CORRECT: true label=1, prob=0.5727136135101318,0.42728638648986816\n",
      "CORRECT: true label=1, prob=0.5537800192832947,0.4462199807167053\n",
      "CORRECT: true label=1, prob=0.5810815095901489,0.4189184904098511\n",
      "CORRECT: true label=1, prob=0.5661988854408264,0.4338011145591736\n",
      "CORRECT: true label=1, prob=0.5534300208091736,0.4465699791908264\n",
      "CORRECT: true label=1, prob=0.5523048639297485,0.44769513607025146\n",
      "WRONG: true label=1, prob=0.4087566137313843,0.4087566137313843\n",
      "CORRECT: true label=1, prob=0.6138252019882202,0.3861747980117798\n",
      "CORRECT: true label=1, prob=0.5549855828285217,0.44501441717147827\n",
      "CORRECT: true label=1, prob=0.6027216911315918,0.3972783088684082\n",
      "CORRECT: true label=1, prob=0.5572082996368408,0.4427917003631592\n",
      "CORRECT: true label=1, prob=0.5526605844497681,0.44733941555023193\n",
      "CORRECT: true label=1, prob=0.5475782752037048,0.45242172479629517\n",
      "WRONG: true label=1, prob=0.46797090768814087,0.46797090768814087\n",
      "CORRECT: true label=1, prob=0.5163910388946533,0.4836089611053467\n",
      "CORRECT: true label=1, prob=0.5200047492980957,0.4799952507019043\n",
      "CORRECT: true label=1, prob=0.5101982355117798,0.4898017644882202\n",
      "CORRECT: true label=1, prob=0.5274796485900879,0.4725203514099121\n",
      "WRONG: true label=1, prob=0.49793219566345215,0.49793219566345215\n",
      "CORRECT: true label=1, prob=0.5600163340568542,0.43998366594314575\n",
      "CORRECT: true label=1, prob=0.6164052486419678,0.3835947513580322\n",
      "CORRECT: true label=1, prob=0.5748417377471924,0.4251582622528076\n",
      "CORRECT: true label=1, prob=0.5761957764625549,0.42380422353744507\n",
      "CORRECT: true label=1, prob=0.5894020199775696,0.4105979800224304\n",
      "CORRECT: true label=1, prob=0.6760154366493225,0.3239845633506775\n",
      "CORRECT: true label=1, prob=0.5547233819961548,0.4452766180038452\n",
      "CORRECT: true label=1, prob=0.5676907896995544,0.43230921030044556\n",
      "CORRECT: true label=1, prob=0.5048219561576843,0.4951780438423157\n",
      "CORRECT: true label=1, prob=0.5749371647834778,0.4250628352165222\n",
      "CORRECT: true label=1, prob=0.532913327217102,0.46708667278289795\n",
      "CORRECT: true label=1, prob=0.5496321320533752,0.45036786794662476\n",
      "CORRECT: true label=1, prob=0.5436400175094604,0.45635998249053955\n",
      "CORRECT: true label=1, prob=0.5903477072715759,0.4096522927284241\n",
      "WRONG: true label=1, prob=0.44942012429237366,0.44942012429237366\n",
      "CORRECT: true label=1, prob=0.554402232170105,0.445597767829895\n",
      "CORRECT: true label=1, prob=0.5914955139160156,0.4085044860839844\n",
      "CORRECT: true label=1, prob=0.539625883102417,0.460374116897583\n",
      "CORRECT: true label=1, prob=0.5350063443183899,0.4649936556816101\n",
      "CORRECT: true label=1, prob=0.5429665446281433,0.4570334553718567\n",
      "WRONG: true label=0, prob=0.5313245058059692,0.46867549419403076\n",
      "CORRECT: true label=1, prob=0.5877946019172668,0.41220539808273315\n",
      "WRONG: true label=1, prob=0.45379638671875,0.45379638671875\n",
      "CORRECT: true label=1, prob=0.57973712682724,0.42026287317276\n",
      "CORRECT: true label=1, prob=0.5873993635177612,0.41260063648223877\n",
      "CORRECT: true label=1, prob=0.547541618347168,0.45245838165283203\n",
      "CORRECT: true label=1, prob=0.6245737075805664,0.3754262924194336\n",
      "CORRECT: true label=1, prob=0.6322317719459534,0.36776822805404663\n",
      "CORRECT: true label=1, prob=0.5891631245613098,0.4108368754386902\n",
      "CORRECT: true label=1, prob=0.5422240495681763,0.45777595043182373\n",
      "CORRECT: true label=1, prob=0.5950039625167847,0.40499603748321533\n",
      "CORRECT: true label=1, prob=0.592377245426178,0.407622754573822\n",
      "CORRECT: true label=1, prob=0.6073281168937683,0.3926718831062317\n",
      "CORRECT: true label=1, prob=0.555899977684021,0.444100022315979\n",
      "CORRECT: true label=1, prob=0.5541937947273254,0.44580620527267456\n",
      "WRONG: true label=1, prob=0.48503339290618896,0.48503339290618896\n",
      "CORRECT: true label=1, prob=0.5491431355476379,0.45085686445236206\n",
      "CORRECT: true label=1, prob=0.5448874831199646,0.4551125168800354\n",
      "CORRECT: true label=1, prob=0.5886242389678955,0.4113757610321045\n",
      "CORRECT: true label=1, prob=0.5040175914764404,0.49598240852355957\n",
      "CORRECT: true label=1, prob=0.5670002102851868,0.43299978971481323\n",
      "CORRECT: true label=1, prob=0.5415312051773071,0.45846879482269287\n",
      "CORRECT: true label=1, prob=0.5084475874900818,0.4915524125099182\n",
      "CORRECT: true label=1, prob=0.511529266834259,0.48847073316574097\n",
      "CORRECT: true label=1, prob=0.5498210191726685,0.45017898082733154\n",
      "CORRECT: true label=1, prob=0.6256294250488281,0.3743705749511719\n",
      "CORRECT: true label=1, prob=0.5607505440711975,0.4392494559288025\n",
      "CORRECT: true label=1, prob=0.5848297476768494,0.41517025232315063\n",
      "CORRECT: true label=1, prob=0.5093878507614136,0.4906121492385864\n",
      "CORRECT: true label=1, prob=0.5963107347488403,0.40368926525115967\n",
      "CORRECT: true label=1, prob=0.5374189615249634,0.4625810384750366\n",
      "WRONG: true label=1, prob=0.45239147543907166,0.45239147543907166\n",
      "CORRECT: true label=1, prob=0.5558121800422668,0.44418781995773315\n",
      "CORRECT: true label=1, prob=0.5438162088394165,0.4561837911605835\n",
      "CORRECT: true label=1, prob=0.5214002132415771,0.47859978675842285\n",
      "CORRECT: true label=1, prob=0.5327957272529602,0.4672042727470398\n",
      "CORRECT: true label=1, prob=0.5411856770515442,0.4588143229484558\n",
      "WRONG: true label=1, prob=0.4302701950073242,0.4302701950073242\n",
      "CORRECT: true label=1, prob=0.5314070582389832,0.46859294176101685\n",
      "CORRECT: true label=1, prob=0.5852285623550415,0.4147714376449585\n",
      "WRONG: true label=1, prob=0.48424553871154785,0.48424553871154785\n",
      "CORRECT: true label=1, prob=0.5039964318275452,0.49600356817245483\n",
      "CORRECT: true label=1, prob=0.535659670829773,0.46434032917022705\n",
      "CORRECT: true label=1, prob=0.5549498796463013,0.44505012035369873\n",
      "CORRECT: true label=1, prob=0.5510980486869812,0.4489019513130188\n",
      "CORRECT: true label=1, prob=0.664612352848053,0.335387647151947\n",
      "CORRECT: true label=1, prob=0.5524235963821411,0.4475764036178589\n",
      "CORRECT: true label=1, prob=0.5286218523979187,0.4713781476020813\n",
      "CORRECT: true label=1, prob=0.558866024017334,0.441133975982666\n",
      "CORRECT: true label=1, prob=0.5766129493713379,0.4233870506286621\n",
      "CORRECT: true label=1, prob=0.5461110472679138,0.4538889527320862\n",
      "WRONG: true label=1, prob=0.4961918890476227,0.4961918890476227\n",
      "CORRECT: true label=1, prob=0.5160596966743469,0.4839403033256531\n",
      "WRONG: true label=1, prob=0.3142797350883484,0.3142797350883484\n"
     ]
    }
   ],
   "source": [
    "print(labels_dict,'\\n')\n",
    "test_wrong_indices = set(test_wrong_indices)\n",
    "y_test_preds_np = np.around(y_test_probs)\n",
    "wrong_probs_diffs = []\n",
    "correct_probs_diffs = []\n",
    "for i in range(len(y_test_probs)):\n",
    "    if i in test_wrong_indices:\n",
    "        wrong_probs_diffs.append(abs(y_test_preds_np[i]-y_test_probs[i]))\n",
    "        print(f'WRONG: true label={y_test_true[i]}, prob={y_test_probs[i]},{wrong_probs_diffs[-1]}')\n",
    "    else:\n",
    "        correct_probs_diffs.append(abs(y_test_true[i]-y_test_probs[i]))\n",
    "        print(f'CORRECT: true label={y_test_true[i]}, prob={y_test_probs[i]},{correct_probs_diffs[-1]}')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(wrong_probs_diffs, color='red', density=True)\n",
    "ax.set_title(\"histogram of differences between wrong labels and XGB model probs\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(correct_probs_diffs, color='green', density=True)\n",
    "ax.set_title(\"histogram of differences between correct labels and XGB model probs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL w/ gym-anytrading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_anytrading\n",
    "from gym_anytrading.envs.forex_env import ForexEnv\n",
    "\n",
    "from gym_anytrading.datasets import FOREX_EURUSD_1H_ASK\n",
    "\n",
    "from stable_baselines import A2C\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "\n",
    "import quantstats as qs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### trying out sample code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gym_anytrading.datasets.STOCKS_GOOGL.copy()\n",
    "df = df.drop(['Adj Close'], axis=1)\n",
    "\n",
    "window_size = 10\n",
    "start_index = window_size\n",
    "end_index = len(df)\n",
    "\n",
    "env_maker = lambda: gym.make(\n",
    "    'stocks-v0',\n",
    "    df = df,\n",
    "    window_size = window_size,\n",
    "    frame_bound = (start_index, end_index)\n",
    ")\n",
    "\n",
    "env = DummyVecEnv([env_maker])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i, end = no_missing_data_idx_range(data_with_ichi_2, early_ending_cols=['chikou_span_visual'])\n",
    "train_df = data_with_ichi_2.iloc[i:]\n",
    "train_df.set_index('datetime', inplace=True, verify_integrity=True)\n",
    "categories_dict = {\n",
    "    'quarter': [1,2,3,4],\n",
    "    'day_of_week': [0,1,2,3,4]\n",
    "}\n",
    "train_df = dummy_and_remove_data(train_df, categories_dict=categories_dict, cols_to_remove=['momentum_rsi','month','day','minute','hour','year','spread'],\n",
    "                                 include_defaults=False)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomForexEnv(ForexEnv):\n",
    "    def _process_data(self):\n",
    "        prices = self.df.loc[:, 'Close'].to_numpy()\n",
    "\n",
    "        prices[self.frame_bound[0] - self.window_size]  # validate index (TODO: Improve validation)\n",
    "        prices = prices[self.frame_bound[0]-self.window_size:self.frame_bound[1]]\n",
    "\n",
    "        diff = np.insert(np.diff(prices), 0, 0)\n",
    "        signal_features = np.column_stack((prices, diff))\n",
    "        \n",
    "        my_features = self.df.iloc[:,4:].to_numpy()\n",
    "        signal_features = np.column_stack((signal_features, my_features))\n",
    "#         print(list(signal_features[0]))\n",
    "\n",
    "        return prices, signal_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_df\n",
    "\n",
    "window_size = 10\n",
    "start_index = window_size\n",
    "end_index = len(df)\n",
    "\n",
    "env_maker = lambda: CustomForexEnv(\n",
    "    df = train_df,\n",
    "    window_size = window_size,\n",
    "    frame_bound = (start_index, end_index),\n",
    "    unit_side = 'right'\n",
    ")\n",
    "\n",
    "env = DummyVecEnv([env_maker])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "policy_kwargs = dict(net_arch=[64, 'lstm', dict(vf=[128, 128, 128], pi=[64, 64])])\n",
    "model = A2C('MlpLstmPolicy', env, verbose=1, policy_kwargs=policy_kwargs)\n",
    "model.learn(total_timesteps=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = env_maker()\n",
    "observation = env.reset()\n",
    "actions = []\n",
    "while True:\n",
    "    observation = observation[np.newaxis, ...]\n",
    "\n",
    "    # action = env.action_space.sample()\n",
    "    action, _states = model.predict(observation)\n",
    "    actions.append(action)\n",
    "    observation, reward, done, info = env.step(action)\n",
    "\n",
    "    # env.render()\n",
    "    if done:\n",
    "        print(\"info:\", info)\n",
    "        break\n",
    "print(observation)\n",
    "# for action in actions:\n",
    "#     print(action)\n",
    "# print(len(actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "plt.figure(figsize=(16, 6))\n",
    "env.render_all()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "qs.extend_pandas()\n",
    "\n",
    "net_worth = pd.Series(env.history['total_profit'], index=df.index[start_index+1:end_index])\n",
    "returns = net_worth.pct_change().iloc[1:]\n",
    "\n",
    "qs.reports.full(returns)\n",
    "qs.reports.html(returns, output='a2c_quantstats.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-28ceb4c7b649>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "is GPU available for TF: True\n",
      "\n",
      "GPU devices: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "\n",
      "all devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(f'is GPU available for TF: {tf.test.is_gpu_available()}\\n')\n",
    "\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "print(f'GPU devices: {gpu_devices}\\n')\n",
    "\n",
    "all_devices = tf.config.list_physical_devices()\n",
    "print(f'all devices: {all_devices}')\n",
    "\n",
    "if len(gpu_devices) > 0:\n",
    "    for device in gpu_devices: \n",
    "        tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 128\n",
    "num_epochs = 400\n",
    "fast_ma_window = 3\n",
    "slow_ma_window = 7\n",
    "tenkan_period = 9\n",
    "kijun_period = 30\n",
    "senkou_b_period = 60\n",
    "cur_pair = 'EURUSD'\n",
    "timeframe = 'H1'\n",
    "indicators_info = {\n",
    "    'ichimoku': {\n",
    "        'tenkan_period': tenkan_period,\n",
    "        'kijun_period': kijun_period,\n",
    "        'chikou_period': kijun_period,\n",
    "        'senkou_b_period': senkou_b_period\n",
    "    },\n",
    "    'rsi': {\n",
    "        'periods': 14\n",
    "    }\n",
    "}\n",
    "\n",
    "ma_cols = ['Open','High','Low','Close','Volume']\n",
    "pc_cols = ['Open','High','Low','Close','Volume',]\n",
    "#            'trend_ichimoku_base','trend_ichimoku_conv',\n",
    "#            'trend_ichimoku_a', 'trend_ichimoku_b']\n",
    "normalization_groups = [['Open','High','Low','Close'],  # prices\n",
    "#                         ['trend_ichimoku_base','trend_ichimoku_conv'],  # ichi conv & base lines\n",
    "#                         ['trend_ichimoku_a', 'trend_ichimoku_b'], # ichi cloud lines\n",
    "                        ['tk_cross_bull_strength','tk_cross_bear_strength',   # tk cross strength\n",
    "                        'tk_price_cross_bull_strength','tk_price_cross_bear_strength',   # tk price cross strength\n",
    "                        'senkou_cross_bull_strength','senkou_cross_bear_strength',   # semkou cross strength\n",
    "                        'chikou_cross_bull_strength','chikou_cross_bear_strength']]   # chikou cross strength\n",
    "\n",
    "train_perc = 0.8\n",
    "val_perc = (1-train_perc)/2\n",
    "test_perc = val_perc\n",
    "split_percents = (val_perc, test_perc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get data and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tick_data_filepath = research.download_mt5_data(cur_pair, timeframe, global_train_data_range_start, global_train_data_range_end)\n",
    "data_with_indicators = research.add_indicators_to_raw(filepath=tick_data_filepath,\n",
    "                                                      indicators_info=indicators_info, \n",
    "                                                      datetime_col='datetime')\n",
    "data_with_ichi_sigs = research.add_ichimoku_features(data_with_indicators)\n",
    "\n",
    "all_data = missing_labels_preprocess(data_with_ichi_sigs,None,None)[0]\n",
    "all_data_orig = all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_ma_data = research.get_split_lstm_data(all_data, ma_window=fast_ma_window, seq_len=seq_len, split_percents=split_percents, fully_divisible_batch_sizes=True,\n",
    "                                             normalization_groups=normalization_groups, pc_cols=pc_cols, ma_cols=ma_cols, min_batch_size=1000, max_batch_size=2000)\n",
    "slow_ma_data = research.get_split_lstm_data(all_data, ma_window=slow_ma_window, seq_len=seq_len, split_percents=split_percents, fully_divisible_batch_sizes=True,\n",
    "                                             normalization_groups=normalization_groups, pc_cols=pc_cols, ma_cols=ma_cols, min_batch_size=1000, max_batch_size=2000)\n",
    "\n",
    "x_train_fast_ma, y_train_fast_ma = fast_ma_data['train_data_np']\n",
    "x_val_fast_ma, y_val_fast_ma = fast_ma_data['val_data_np']\n",
    "x_test_fast_ma, y_test_fast_ma = fast_ma_data['test_data_np']\n",
    "\n",
    "x_train_slow_ma, y_train_slow_ma = slow_ma_data['train_data_np']\n",
    "x_val_slow_ma, y_val_slow_ma = slow_ma_data['val_data_np']\n",
    "x_test_slow_ma, y_test_slow_ma = slow_ma_data['test_data_np']\n",
    "\n",
    "# process orignal price data for plotting comparison\n",
    "\n",
    "all_data_orig = apply_perc_change(all_data_orig, cols=pc_cols)\n",
    "all_data_orig.dropna(how='any', axis=0, inplace=True) # drop any NA rows due to applying percentage change\n",
    "\n",
    "train_data_df_orig = all_data_orig.iloc[:fast_ma_data['train_data_df'].index[-1]+1]\n",
    "val_data_df_orig = all_data_orig.iloc[fast_ma_data['train_data_df'].index[-1]+1:fast_ma_data['val_data_df'].index[-1]+1]\n",
    "test_data_df_orig = all_data_orig.iloc[fast_ma_data['val_data_df'].index[-1]+1:]\n",
    "\n",
    "train_data_df_orig, normalization_terms_2 = normalize_data(train_data_df_orig, train_data=True, groups=normalization_groups)   \n",
    "val_data_df_orig, normalization_terms_2 = normalize_data(val_data_df_orig, train_data=False, normalization_terms=normalization_terms_2)\n",
    "test_data_df_orig = normalize_data(test_data_df_orig, train_data=False, normalization_terms=normalization_terms_2)[0]\n",
    "\n",
    "train_data_orig = train_data_df_orig.to_numpy()\n",
    "val_data_orig = val_data_df_orig.to_numpy()\n",
    "test_data_orig = test_data_df_orig.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tup in ((fast_ma_data, 'Fast MA'), (slow_ma_data, 'Slow MA')):\n",
    "    data, marker = tup\n",
    "    \n",
    "    train_data_df = data['train_data_df']\n",
    "    val_data_df = data['val_data_df']\n",
    "    test_data_df = data['test_data_df']\n",
    "\n",
    "    train_data = train_data_df.to_numpy()\n",
    "    val_data = val_data_df.to_numpy()\n",
    "    test_data = test_data_df.to_numpy()\n",
    "\n",
    "    fig = plt.figure(figsize=(15,10))\n",
    "    st = fig.suptitle(f'{marker} Data Separation', fontsize=20)\n",
    "    st.set_y(0.92)\n",
    "\n",
    "    ###############################################################################\n",
    "\n",
    "    ax1 = fig.add_subplot(211)\n",
    "    ax1.plot(np.arange(train_data.shape[0]), train_data_df['Close'], label='Training data')\n",
    "\n",
    "    ax1.plot(np.arange(train_data.shape[0], \n",
    "                       train_data.shape[0]+val_data.shape[0]), val_data_df['Close'], label='Validation data')\n",
    "\n",
    "    ax1.plot(np.arange(train_data.shape[0]+val_data.shape[0], \n",
    "                       train_data.shape[0]+val_data.shape[0]+test_data.shape[0]), test_data_df['Close'], label='Test data')\n",
    "    ax1.set_xlabel('Date')\n",
    "    ax1.set_ylabel(f'{marker} Normalized Closing Returns')\n",
    "\n",
    "    ###############################################################################\n",
    "\n",
    "    ax2 = fig.add_subplot(212)\n",
    "    ax2.plot(np.arange(train_data.shape[0]), train_data_df['Volume'], label='Training data')\n",
    "\n",
    "    ax2.plot(np.arange(train_data.shape[0], \n",
    "                       train_data.shape[0]+val_data.shape[0]), val_data_df['Volume'], label='Validation data')\n",
    "\n",
    "    ax2.plot(np.arange(train_data.shape[0]+val_data.shape[0], \n",
    "                       train_data.shape[0]+val_data.shape[0]+test_data.shape[0]), test_data_df['Volume'], label='Test data')\n",
    "    ax2.set_xlabel('Date')\n",
    "    ax2.set_ylabel(f'{marker} Normalized Volume Changes')\n",
    "\n",
    "    plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Inception_A(layer_in, c7):\n",
    "    branch1x1_1 = layers.Conv1D(c7, kernel_size=1, padding=\"same\", use_bias=False)(layer_in)\n",
    "    branch1x1 = layers.BatchNormalization()(branch1x1_1)\n",
    "    branch1x1 = layers.ReLU()(branch1x1)\n",
    "\n",
    "    branch5x5_1 = layers.Conv1D(c7, kernel_size=1, padding='same', use_bias=False)(layer_in)\n",
    "    branch5x5 = layers.BatchNormalization()(branch5x5_1)\n",
    "    branch5x5 = layers.ReLU()(branch5x5)\n",
    "    branch5x5 = layers.Conv1D(c7, kernel_size=5, padding='same', use_bias=False)(branch5x5)\n",
    "    branch5x5 = layers.BatchNormalization()(branch5x5)\n",
    "    branch5x5 = layers.ReLU()(branch5x5)  \n",
    "\n",
    "    branch3x3_1 = layers.Conv1D(c7, kernel_size=1, padding='same', use_bias=False)(layer_in)\n",
    "    branch3x3 = layers.BatchNormalization()(branch3x3_1)\n",
    "    branch3x3 = layers.ReLU()(branch3x3)\n",
    "    branch3x3 = layers.Conv1D(c7, kernel_size=3, padding='same', use_bias=False)(branch3x3)\n",
    "    branch3x3 = layers.BatchNormalization()(branch3x3)\n",
    "    branch3x3 = layers.ReLU()(branch3x3)\n",
    "    branch3x3 = layers.Conv1D(c7, kernel_size=3, padding='same', use_bias=False)(branch3x3)\n",
    "    branch3x3 = layers.BatchNormalization()(branch3x3)\n",
    "    branch3x3 = layers.ReLU()(branch3x3) \n",
    "\n",
    "    branch_pool = layers.AveragePooling1D(pool_size=(3), strides=1, padding='same')(layer_in)\n",
    "    branch_pool = layers.Conv1D(c7, kernel_size=1, padding='same', use_bias=False)(branch_pool)\n",
    "    branch_pool = layers.BatchNormalization()(branch_pool)\n",
    "    branch_pool = layers.ReLU()(branch_pool)\n",
    "    outputs = layers.Concatenate(axis=-1)([branch1x1, branch5x5, branch3x3, branch_pool])\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def Inception_B(layer_in, c7):\n",
    "    branch3x3 = layers.Conv1D(c7, kernel_size=3, padding=\"same\", strides=2, use_bias=False)(layer_in)\n",
    "    branch3x3 = layers.BatchNormalization()(branch3x3)\n",
    "    branch3x3 = layers.ReLU()(branch3x3)  \n",
    "\n",
    "    branch3x3dbl = layers.Conv1D(c7, kernel_size=1, padding=\"same\", use_bias=False)(layer_in)\n",
    "    branch3x3dbl = layers.BatchNormalization()(branch3x3dbl)\n",
    "    branch3x3dbl = layers.ReLU()(branch3x3dbl)  \n",
    "    branch3x3dbl = layers.Conv1D(c7, kernel_size=3, padding=\"same\", use_bias=False)(branch3x3dbl)  \n",
    "    branch3x3dbl = layers.BatchNormalization()(branch3x3dbl)\n",
    "    branch3x3dbl = layers.ReLU()(branch3x3dbl)  \n",
    "    branch3x3dbl = layers.Conv1D(c7, kernel_size=3, padding=\"same\", strides=2, use_bias=False)(branch3x3dbl)    \n",
    "    branch3x3dbl = layers.BatchNormalization()(branch3x3dbl)\n",
    "    branch3x3dbl = layers.ReLU()(branch3x3dbl)   \n",
    "\n",
    "    branch_pool = layers.MaxPooling1D(pool_size=3, strides=2, padding=\"same\")(layer_in)\n",
    "\n",
    "    outputs = layers.Concatenate(axis=-1)([branch3x3, branch3x3dbl, branch_pool])\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def Inception_C(layer_in, c7):\n",
    "    branch1x1_1 = layers.Conv1D(c7, kernel_size=1, padding=\"same\", use_bias=False)(layer_in)\n",
    "    branch1x1 = layers.BatchNormalization()(branch1x1_1)\n",
    "    branch1x1 = layers.ReLU()(branch1x1)   \n",
    "\n",
    "    branch7x7_1 = layers.Conv1D(c7, kernel_size=1, padding=\"same\", use_bias=False)(layer_in)\n",
    "    branch7x7 = layers.BatchNormalization()(branch7x7_1)\n",
    "    branch7x7 = layers.ReLU()(branch7x7)   \n",
    "    branch7x7 = layers.Conv1D(c7, kernel_size=(7), padding=\"same\", use_bias=False)(branch7x7)\n",
    "    branch7x7 = layers.BatchNormalization()(branch7x7)\n",
    "    branch7x7 = layers.ReLU()(branch7x7)  \n",
    "    branch7x7 = layers.Conv1D(c7, kernel_size=(1), padding=\"same\", use_bias=False)(branch7x7)  \n",
    "    branch7x7 = layers.BatchNormalization()(branch7x7)\n",
    "    branch7x7 = layers.ReLU()(branch7x7)   \n",
    "\n",
    "    branch7x7dbl_1 = layers.Conv1D(c7, kernel_size=1, padding=\"same\", use_bias=False)(layer_in)  \n",
    "    branch7x7dbl = layers.BatchNormalization()(branch7x7dbl_1)\n",
    "    branch7x7dbl = layers.ReLU()(branch7x7dbl)  \n",
    "    branch7x7dbl = layers.Conv1D(c7, kernel_size=(7), padding=\"same\", use_bias=False)(branch7x7dbl)  \n",
    "    branch7x7dbl = layers.BatchNormalization()(branch7x7dbl)\n",
    "    branch7x7dbl = layers.ReLU()(branch7x7dbl) \n",
    "    branch7x7dbl = layers.Conv1D(c7, kernel_size=(1), padding=\"same\", use_bias=False)(branch7x7dbl)  \n",
    "    branch7x7dbl = layers.BatchNormalization()(branch7x7dbl)\n",
    "    branch7x7dbl = layers.ReLU()(branch7x7dbl)  \n",
    "    branch7x7dbl = layers.Conv1D(c7, kernel_size=(7), padding=\"same\", use_bias=False)(branch7x7dbl)  \n",
    "    branch7x7dbl = layers.BatchNormalization()(branch7x7dbl)\n",
    "    branch7x7dbl = layers.ReLU()(branch7x7dbl)  \n",
    "    branch7x7dbl = layers.Conv1D(c7, kernel_size=(1), padding=\"same\", use_bias=False)(branch7x7dbl)  \n",
    "    branch7x7dbl = layers.BatchNormalization()(branch7x7dbl)\n",
    "    branch7x7dbl = layers.ReLU()(branch7x7dbl)  \n",
    "\n",
    "    branch_pool = layers.AveragePooling1D(pool_size=3, strides=1, padding='same')(layer_in)\n",
    "    branch_pool = layers.Conv1D(c7, kernel_size=1, padding='same', use_bias=False)(branch_pool)\n",
    "    branch_pool = layers.BatchNormalization()(branch_pool)\n",
    "    branch_pool = layers.ReLU()(branch_pool)  \n",
    "\n",
    "    outputs = layers.Concatenate(axis=-1)([branch1x1, branch7x7, branch7x7dbl, branch_pool])\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def create_model(seq_len, num_features):\n",
    "    in_seq = layers.Input(shape=(seq_len, num_features))\n",
    "\n",
    "    x = Inception_A(in_seq, 32)\n",
    "    x = Inception_A(x, 32)\n",
    "    x = Inception_B(x, 32)\n",
    "    x = Inception_B(x, 32)\n",
    "    x = Inception_C(x, 32)\n",
    "    x = Inception_C(x, 32)    \n",
    "\n",
    "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x) \n",
    "\n",
    "    avg_pool = layers.GlobalAveragePooling1D()(x)\n",
    "    max_pool = layers.GlobalMaxPooling1D()(x)\n",
    "    conc = layers.concatenate([avg_pool, max_pool])\n",
    "    conc = layers.Dense(64, activation=\"relu\")(conc)\n",
    "    out = layers.Dense(1, activation=\"sigmoid\")(conc)      \n",
    "\n",
    "    model = keras.Model(inputs=in_seq, outputs=out)\n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\", metrics=['mae', 'mape'])     \n",
    "    return model\n",
    "\n",
    "# def create_model(seq_len, num_features):\n",
    "#     in_seq = layers.Input(shape = (seq_len, num_features))\n",
    "\n",
    "#     x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(in_seq)\n",
    "#     x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(x)\n",
    "#     x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x) \n",
    "\n",
    "#     avg_pool = layers.GlobalAveragePooling1D()(x)\n",
    "#     max_pool = layers.GlobalMaxPooling1D()(x)\n",
    "#     conc = layers.concatenate([avg_pool, max_pool])\n",
    "#     conc = layers.Dense(64, activation=\"relu\")(conc)\n",
    "#     out = layers.Dense(1, activation=\"linear\")(conc)      \n",
    "\n",
    "#     model = keras.Model(inputs=in_seq, outputs=out)\n",
    "#     model.compile(loss=\"mse\", optimizer=\"adam\", metrics=['mae', 'mape'])    \n",
    "#     return model\n",
    "\n",
    "def create_model_binary(seq_len, num_features):\n",
    "#     in_seq = layers.Input(shape=(seq_len, num_features))\n",
    "\n",
    "#     x = Inception_A(in_seq, 32)\n",
    "#     x = Inception_A(x, 32)\n",
    "#     x = Inception_B(x, 32)\n",
    "#     x = Inception_B(x, 32)\n",
    "#     x = Inception_C(x, 32)\n",
    "#     x = Inception_C(x, 32)    \n",
    "\n",
    "#     x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(x)\n",
    "#     x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(x)\n",
    "#     x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x) \n",
    "\n",
    "#     avg_pool = layers.GlobalAveragePooling1D()(x)\n",
    "#     max_pool = layers.GlobalMaxPooling1D()(x)\n",
    "#     conc = layers.concatenate([avg_pool, max_pool])\n",
    "#     conc = layers.Dense(64, activation=\"relu\")(conc)\n",
    "#     out = layers.Dense(1, activation=\"sigmoid\")(conc)      \n",
    "\n",
    "#     model = keras.Model(inputs=in_seq, outputs=out)\n",
    "#     model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy', 'AUC']) \n",
    "\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(seq_len, num_features)))\n",
    "    model.add(layers.Conv1D(filters=16, kernel_size=3, padding='same', activation='relu'))\n",
    "    model.add(layers.MaxPooling1D(pool_size=2))\n",
    "    model.add(layers.LSTM(64, return_sequences=False))  # should have return_sequences=False before a dense layer and true before another RNN type layer\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', 'AUC'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### try using model as buy/sell classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 60431 rows of tick data from C:\\GitHub Repos\\ForexMachine\\Data\\.cache\\mt5_EURUSD_h1_ticks_2011-01-01T00;00UTC_to_2020-10-01T00;00UTC.csv\n",
      "saved 60431 rows of EURUSD h1 tick data to C:\\GitHub Repos\\ForexMachine\\Data\\RawData\\mt5_EURUSD_h1_ticks_2011-01-01T00;00UTC_to_2020-10-01T00;00UTC.csv, done.\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "\n",
    "label_non_signals = False\n",
    "min_profit_percent, profit_noise_percent = 0.01, 0.01\n",
    "contract_size = 100_000   # size of 1 lot is typically 100,000 (100 for gold, becuase 1 lot = 100 oz of gold)\n",
    "lots_per_trade = 0.2  \n",
    "in_quote_currency = True\n",
    "pip_resolution = 0.0001\n",
    "\n",
    "labels_dict = {1: 'buy', 0: 'sell'}\n",
    "\n",
    "signals_to_consider = ['cloud_breakout_bull','cloud_breakout_bear',                       # cloud breakout\n",
    "                       'tk_cross_bull_strength', 'tk_cross_bear_strength',                # Tenkan Sen / Kijun Sen Cross\n",
    "                       'tk_price_cross_bull_strength', 'tk_price_cross_bear_strength',    # price crossing both the Tenkan Sen / Kijun Sen\n",
    "                       'senkou_cross_bull_strength', 'senkou_cross_bear_strength',        # Senkou Span Cross\n",
    "                       'chikou_cross_bull_strength', 'chikou_cross_bear_strength']        # Chikou Span Cross\n",
    "sigs_for_filename = 'cb-tk-tkp-sen-chi'\n",
    "\n",
    "# get data\n",
    "\n",
    "cur_pair = 'EURUSD'\n",
    "timeframe = 'H1'\n",
    "tick_data_filepath = research.download_mt5_data(cur_pair, timeframe, global_train_data_range_start, global_train_data_range_end)\n",
    "data_with_indicators = research.add_indicators_to_raw(filepath=tick_data_filepath, \n",
    "                                                      indicators_info=indicators_info, \n",
    "                                                      datetime_col='datetime')\n",
    "train_data = research.add_ichimoku_features(data_with_indicators)\n",
    "\n",
    "train_data_labels = generate_ichimoku_labels(train_data, label_non_signals=label_non_signals, min_profit_percent=min_profit_percent, \n",
    "                                             profit_noise_percent=profit_noise_percent, signals_to_consider=signals_to_consider, \n",
    "                                             contract_size=contract_size, lots_per_trade=lots_per_trade,\n",
    "                                             in_quote_currency=in_quote_currency,pip_resolution=pip_resolution)\n",
    "\n",
    "start_idx, end_idx = no_missing_data_idx_range(train_data, early_ending_cols=['chikou_span_visual'])\n",
    "train_data = train_data.iloc[start_idx:end_idx+1]\n",
    "train_data = dummy_and_remove_features(train_data)\n",
    "train_data_labels = train_data_labels.iloc[start_idx:end_idx+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "data w/ moving average window of None info:\n",
      "\n",
      "batch size for evaluation: 1297\n",
      "training data size reduction for evaulation: 9079 -> 9079\n",
      "batch size for final training: 1874\n",
      "training data size reduction for final training: 11245 -> 11244\n",
      "\n",
      "training data shape: x=(9079, 128, 30), y=(9079,)\n",
      "validation data shape: x=(1087, 128, 30), y=(1087,)\n",
      "test data shape: x=(1032, 128, 30), y=(1032,)\n",
      "all train data shape: x=(11244, 128, 30), y=(11244,)\n",
      "------------------------------------------------------\n",
      "Epoch 1/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.6922 - accuracy: 0.5126 - auc: 0.5218\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.40846, saving model to ../my_stuff/EURUSD-H1_0.01-min_profit_0.2-lots_right-cur_side_9-30-60-cb-tk-tkp-sen-chi-ichi_cnn-lstm_classifier.hdf5\n",
      "7/7 [==============================] - 1s 119ms/step - loss: 0.6922 - accuracy: 0.5126 - auc: 0.5218 - val_loss: 0.7079 - val_accuracy: 0.4085 - val_auc: 0.5658\n",
      "Epoch 2/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.6906 - accuracy: 0.5235 - auc: 0.5357\n",
      "Epoch 00002: val_accuracy improved from 0.40846 to 0.45262, saving model to ../my_stuff/EURUSD-H1_0.01-min_profit_0.2-lots_right-cur_side_9-30-60-cb-tk-tkp-sen-chi-ichi_cnn-lstm_classifier.hdf5\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 0.6908 - accuracy: 0.5218 - auc: 0.5328 - val_loss: 0.6998 - val_accuracy: 0.4526 - val_auc: 0.5937\n",
      "Epoch 3/400\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.6896 - accuracy: 0.5275 - auc: 0.5448\n",
      "Epoch 00003: val_accuracy did not improve from 0.45262\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.6894 - accuracy: 0.5302 - auc: 0.5455 - val_loss: 0.7150 - val_accuracy: 0.4085 - val_auc: 0.5848\n",
      "Epoch 4/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.6891 - accuracy: 0.5344 - auc: 0.5484\n",
      "Epoch 00004: val_accuracy improved from 0.45262 to 0.46274, saving model to ../my_stuff/EURUSD-H1_0.01-min_profit_0.2-lots_right-cur_side_9-30-60-cb-tk-tkp-sen-chi-ichi_cnn-lstm_classifier.hdf5\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.6891 - accuracy: 0.5344 - auc: 0.5484 - val_loss: 0.7022 - val_accuracy: 0.4627 - val_auc: 0.5726\n",
      "Epoch 5/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.6874 - accuracy: 0.5374 - auc: 0.5567\n",
      "Epoch 00005: val_accuracy did not improve from 0.46274\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.6869 - accuracy: 0.5396 - auc: 0.5590 - val_loss: 0.7259 - val_accuracy: 0.4103 - val_auc: 0.5179\n",
      "Epoch 6/400\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.6842 - accuracy: 0.5540 - auc: 0.5733\n",
      "Epoch 00006: val_accuracy improved from 0.46274 to 0.48114, saving model to ../my_stuff/EURUSD-H1_0.01-min_profit_0.2-lots_right-cur_side_9-30-60-cb-tk-tkp-sen-chi-ichi_cnn-lstm_classifier.hdf5\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.6852 - accuracy: 0.5471 - auc: 0.5686 - val_loss: 0.7038 - val_accuracy: 0.4811 - val_auc: 0.5018\n",
      "Epoch 7/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.6829 - accuracy: 0.5495 - auc: 0.5757\n",
      "Epoch 00007: val_accuracy did not improve from 0.48114\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.6829 - accuracy: 0.5495 - auc: 0.5757 - val_loss: 0.7204 - val_accuracy: 0.4756 - val_auc: 0.4841\n",
      "Epoch 8/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.6786 - accuracy: 0.5564 - auc: 0.5875\n",
      "Epoch 00008: val_accuracy did not improve from 0.48114\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.6786 - accuracy: 0.5564 - auc: 0.5875 - val_loss: 0.7347 - val_accuracy: 0.4499 - val_auc: 0.4570\n",
      "Epoch 9/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.6767 - accuracy: 0.5571 - auc: 0.5945\n",
      "Epoch 00009: val_accuracy improved from 0.48114 to 0.49954, saving model to ../my_stuff/EURUSD-H1_0.01-min_profit_0.2-lots_right-cur_side_9-30-60-cb-tk-tkp-sen-chi-ichi_cnn-lstm_classifier.hdf5\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.6767 - accuracy: 0.5571 - auc: 0.5945 - val_loss: 0.7604 - val_accuracy: 0.4995 - val_auc: 0.4976\n",
      "Epoch 10/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.6714 - accuracy: 0.5747 - auc: 0.6114\n",
      "Epoch 00010: val_accuracy did not improve from 0.49954\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.6714 - accuracy: 0.5747 - auc: 0.6114 - val_loss: 0.8026 - val_accuracy: 0.4535 - val_auc: 0.4543\n",
      "Epoch 11/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.6676 - accuracy: 0.5804 - auc: 0.6206\n",
      "Epoch 00011: val_accuracy did not improve from 0.49954\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.6667 - accuracy: 0.5800 - auc: 0.6209 - val_loss: 0.7842 - val_accuracy: 0.4609 - val_auc: 0.4661\n",
      "Epoch 12/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.6611 - accuracy: 0.5898 - auc: 0.6353\n",
      "Epoch 00012: val_accuracy did not improve from 0.49954\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.6611 - accuracy: 0.5898 - auc: 0.6353 - val_loss: 0.8563 - val_accuracy: 0.4655 - val_auc: 0.4822\n",
      "Epoch 13/400\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.6624 - accuracy: 0.5832 - auc: 0.6327\n",
      "Epoch 00013: val_accuracy did not improve from 0.49954\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.6616 - accuracy: 0.5876 - auc: 0.6361 - val_loss: 0.7721 - val_accuracy: 0.4434 - val_auc: 0.4288\n",
      "Epoch 14/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.6564 - accuracy: 0.6011 - auc: 0.6485\n",
      "Epoch 00014: val_accuracy did not improve from 0.49954\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.6564 - accuracy: 0.6011 - auc: 0.6485 - val_loss: 0.8386 - val_accuracy: 0.4361 - val_auc: 0.3735\n",
      "Epoch 15/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.6550 - accuracy: 0.6027 - auc: 0.6507\n",
      "Epoch 00015: val_accuracy did not improve from 0.49954\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.6550 - accuracy: 0.6027 - auc: 0.6507 - val_loss: 0.7491 - val_accuracy: 0.4526 - val_auc: 0.4362\n",
      "Epoch 16/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.6500 - accuracy: 0.6019 - auc: 0.6600\n",
      "Epoch 00016: val_accuracy did not improve from 0.49954\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.6500 - accuracy: 0.6019 - auc: 0.6600 - val_loss: 0.7866 - val_accuracy: 0.4637 - val_auc: 0.4174\n",
      "Epoch 17/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.6454 - accuracy: 0.6153 - auc: 0.6654\n",
      "Epoch 00017: val_accuracy did not improve from 0.49954\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.6454 - accuracy: 0.6153 - auc: 0.6654 - val_loss: 0.8387 - val_accuracy: 0.4609 - val_auc: 0.3817\n",
      "Epoch 18/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.6430 - accuracy: 0.6236 - auc: 0.6727\n",
      "Epoch 00018: val_accuracy did not improve from 0.49954\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.6430 - accuracy: 0.6236 - auc: 0.6727 - val_loss: 0.8292 - val_accuracy: 0.4609 - val_auc: 0.4292\n",
      "Epoch 19/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.6362 - accuracy: 0.6322 - auc: 0.6863\n",
      "Epoch 00019: val_accuracy did not improve from 0.49954\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.6362 - accuracy: 0.6322 - auc: 0.6863 - val_loss: 0.8413 - val_accuracy: 0.4664 - val_auc: 0.4624\n",
      "Epoch 20/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.6289 - accuracy: 0.6411 - auc: 0.6974\n",
      "Epoch 00020: val_accuracy did not improve from 0.49954\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.6289 - accuracy: 0.6411 - auc: 0.6974 - val_loss: 0.8346 - val_accuracy: 0.4701 - val_auc: 0.4217\n",
      "Epoch 21/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.6239 - accuracy: 0.6451 - auc: 0.7029\n",
      "Epoch 00021: val_accuracy did not improve from 0.49954\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.6216 - accuracy: 0.6497 - auc: 0.7075 - val_loss: 0.8299 - val_accuracy: 0.4756 - val_auc: 0.4167\n",
      "Epoch 22/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.6193 - accuracy: 0.6560 - auc: 0.7107\n",
      "Epoch 00022: val_accuracy did not improve from 0.49954\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.6184 - accuracy: 0.6581 - auc: 0.7125 - val_loss: 0.8889 - val_accuracy: 0.4784 - val_auc: 0.4314\n",
      "Epoch 23/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.6100 - accuracy: 0.6622 - auc: 0.7224\n",
      "Epoch 00023: val_accuracy did not improve from 0.49954\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.6100 - accuracy: 0.6622 - auc: 0.7224 - val_loss: 0.8567 - val_accuracy: 0.4839 - val_auc: 0.4330\n",
      "Epoch 24/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.6044 - accuracy: 0.6676 - auc: 0.7297\n",
      "Epoch 00024: val_accuracy did not improve from 0.49954\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.6044 - accuracy: 0.6676 - auc: 0.7297 - val_loss: 0.8711 - val_accuracy: 0.4821 - val_auc: 0.4360\n",
      "Epoch 25/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.6003 - accuracy: 0.6620 - auc: 0.7307\n",
      "Epoch 00025: val_accuracy did not improve from 0.49954\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.6003 - accuracy: 0.6620 - auc: 0.7307 - val_loss: 0.8808 - val_accuracy: 0.4986 - val_auc: 0.4836\n",
      "Epoch 26/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.5906 - accuracy: 0.6778 - auc: 0.7445\n",
      "Epoch 00026: val_accuracy improved from 0.49954 to 0.50690, saving model to ../my_stuff/EURUSD-H1_0.01-min_profit_0.2-lots_right-cur_side_9-30-60-cb-tk-tkp-sen-chi-ichi_cnn-lstm_classifier.hdf5\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.5906 - accuracy: 0.6778 - auc: 0.7445 - val_loss: 0.8815 - val_accuracy: 0.5069 - val_auc: 0.4876\n",
      "Epoch 27/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.5919 - accuracy: 0.6700 - auc: 0.7423\n",
      "Epoch 00027: val_accuracy did not improve from 0.50690\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.5933 - accuracy: 0.6688 - auc: 0.7409 - val_loss: 0.8101 - val_accuracy: 0.4867 - val_auc: 0.4848\n",
      "Epoch 28/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.5875 - accuracy: 0.6794 - auc: 0.7483\n",
      "Epoch 00028: val_accuracy did not improve from 0.50690\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.5875 - accuracy: 0.6794 - auc: 0.7483 - val_loss: 0.9327 - val_accuracy: 0.4995 - val_auc: 0.5166\n",
      "Epoch 29/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.5856 - accuracy: 0.6775 - auc: 0.7488\n",
      "Epoch 00029: val_accuracy did not improve from 0.50690\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.5856 - accuracy: 0.6775 - auc: 0.7488 - val_loss: 0.8564 - val_accuracy: 0.4995 - val_auc: 0.4893\n",
      "Epoch 30/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.5847 - accuracy: 0.6795 - auc: 0.7502\n",
      "Epoch 00030: val_accuracy did not improve from 0.50690\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.5847 - accuracy: 0.6795 - auc: 0.7502 - val_loss: 0.9548 - val_accuracy: 0.4637 - val_auc: 0.4669\n",
      "Epoch 31/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.5725 - accuracy: 0.6930 - auc: 0.7640\n",
      "Epoch 00031: val_accuracy did not improve from 0.50690\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.5725 - accuracy: 0.6930 - auc: 0.7640 - val_loss: 0.9073 - val_accuracy: 0.4940 - val_auc: 0.4934\n",
      "Epoch 32/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.5650 - accuracy: 0.7001 - auc: 0.7729\n",
      "Epoch 00032: val_accuracy did not improve from 0.50690\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.5650 - accuracy: 0.7001 - auc: 0.7729 - val_loss: 1.1020 - val_accuracy: 0.4683 - val_auc: 0.4585\n",
      "Epoch 33/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.5693 - accuracy: 0.6946 - auc: 0.7674\n",
      "Epoch 00033: val_accuracy did not improve from 0.50690\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.5693 - accuracy: 0.6946 - auc: 0.7674 - val_loss: 1.0044 - val_accuracy: 0.4821 - val_auc: 0.4685\n",
      "Epoch 34/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.5526 - accuracy: 0.7093 - auc: 0.7854\n",
      "Epoch 00034: val_accuracy did not improve from 0.50690\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.5526 - accuracy: 0.7093 - auc: 0.7854 - val_loss: 1.0673 - val_accuracy: 0.5041 - val_auc: 0.4406\n",
      "Epoch 35/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.5468 - accuracy: 0.7140 - auc: 0.7912\n",
      "Epoch 00035: val_accuracy did not improve from 0.50690\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.5468 - accuracy: 0.7140 - auc: 0.7912 - val_loss: 1.0725 - val_accuracy: 0.4894 - val_auc: 0.4733\n",
      "Epoch 36/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.5392 - accuracy: 0.7257 - auc: 0.7996\n",
      "Epoch 00036: val_accuracy did not improve from 0.50690\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.5392 - accuracy: 0.7257 - auc: 0.7996 - val_loss: 0.9669 - val_accuracy: 0.4839 - val_auc: 0.4968\n",
      "Epoch 37/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.5399 - accuracy: 0.7211 - auc: 0.7983\n",
      "Epoch 00037: val_accuracy did not improve from 0.50690\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.5399 - accuracy: 0.7211 - auc: 0.7983 - val_loss: 1.1052 - val_accuracy: 0.4995 - val_auc: 0.4679\n",
      "Epoch 38/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.5295 - accuracy: 0.7278 - auc: 0.8063\n",
      "Epoch 00038: val_accuracy did not improve from 0.50690\n",
      "7/7 [==============================] - 2s 239ms/step - loss: 0.5272 - accuracy: 0.7307 - auc: 0.8095 - val_loss: 1.0409 - val_accuracy: 0.5005 - val_auc: 0.4737\n",
      "Epoch 39/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.5161 - accuracy: 0.7377 - auc: 0.8200\n",
      "Epoch 00039: val_accuracy did not improve from 0.50690\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.5185 - accuracy: 0.7362 - auc: 0.8178 - val_loss: 1.0493 - val_accuracy: 0.4977 - val_auc: 0.4841\n",
      "Epoch 40/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.5249 - accuracy: 0.7329 - auc: 0.8123\n",
      "Epoch 00040: val_accuracy improved from 0.50690 to 0.55566, saving model to ../my_stuff/EURUSD-H1_0.01-min_profit_0.2-lots_right-cur_side_9-30-60-cb-tk-tkp-sen-chi-ichi_cnn-lstm_classifier.hdf5\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.5249 - accuracy: 0.7329 - auc: 0.8123 - val_loss: 1.0620 - val_accuracy: 0.5557 - val_auc: 0.4925\n",
      "Epoch 41/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.5109 - accuracy: 0.7439 - auc: 0.8258\n",
      "Epoch 00041: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.5109 - accuracy: 0.7439 - auc: 0.8258 - val_loss: 1.2008 - val_accuracy: 0.4867 - val_auc: 0.4851\n",
      "Epoch 42/400\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.5076 - accuracy: 0.7414 - auc: 0.8262\n",
      "Epoch 00042: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.5048 - accuracy: 0.7447 - auc: 0.8295 - val_loss: 1.1721 - val_accuracy: 0.4784 - val_auc: 0.4545\n",
      "Epoch 43/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.5026 - accuracy: 0.7458 - auc: 0.8313\n",
      "Epoch 00043: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.5013 - accuracy: 0.7462 - auc: 0.8317 - val_loss: 1.1500 - val_accuracy: 0.4867 - val_auc: 0.4917\n",
      "Epoch 44/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.4909 - accuracy: 0.7582 - auc: 0.8406\n",
      "Epoch 00044: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.4909 - accuracy: 0.7582 - auc: 0.8406 - val_loss: 1.1066 - val_accuracy: 0.5014 - val_auc: 0.4804\n",
      "Epoch 45/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.4863 - accuracy: 0.7575 - auc: 0.8427\n",
      "Epoch 00045: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.4892 - accuracy: 0.7570 - auc: 0.8414 - val_loss: 1.1731 - val_accuracy: 0.4986 - val_auc: 0.4999\n",
      "Epoch 46/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.4725 - accuracy: 0.7676 - auc: 0.8539\n",
      "Epoch 00046: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.4725 - accuracy: 0.7676 - auc: 0.8539 - val_loss: 1.1752 - val_accuracy: 0.5317 - val_auc: 0.4936\n",
      "Epoch 47/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.4749 - accuracy: 0.7720 - auc: 0.8514\n",
      "Epoch 00047: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.4749 - accuracy: 0.7720 - auc: 0.8514 - val_loss: 1.1630 - val_accuracy: 0.5345 - val_auc: 0.5014\n",
      "Epoch 48/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.4775 - accuracy: 0.7678 - auc: 0.8498\n",
      "Epoch 00048: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.4754 - accuracy: 0.7701 - auc: 0.8513 - val_loss: 1.1668 - val_accuracy: 0.5419 - val_auc: 0.5055\n",
      "Epoch 49/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.4597 - accuracy: 0.7807 - auc: 0.8629\n",
      "Epoch 00049: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.4597 - accuracy: 0.7807 - auc: 0.8629 - val_loss: 1.2223 - val_accuracy: 0.5225 - val_auc: 0.5031\n",
      "Epoch 50/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.4669 - accuracy: 0.7746 - auc: 0.8570\n",
      "Epoch 00050: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.4660 - accuracy: 0.7760 - auc: 0.8581 - val_loss: 1.2116 - val_accuracy: 0.5253 - val_auc: 0.4902\n",
      "Epoch 51/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.4647 - accuracy: 0.7759 - auc: 0.8596\n",
      "Epoch 00051: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.4647 - accuracy: 0.7759 - auc: 0.8596 - val_loss: 1.2884 - val_accuracy: 0.5170 - val_auc: 0.4841\n",
      "Epoch 52/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.4501 - accuracy: 0.7889 - auc: 0.8695\n",
      "Epoch 00052: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.4523 - accuracy: 0.7867 - auc: 0.8677 - val_loss: 1.2293 - val_accuracy: 0.4986 - val_auc: 0.4911\n",
      "Epoch 53/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.4484 - accuracy: 0.7881 - auc: 0.8708\n",
      "Epoch 00053: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.4484 - accuracy: 0.7881 - auc: 0.8708 - val_loss: 1.1950 - val_accuracy: 0.5244 - val_auc: 0.5088\n",
      "Epoch 54/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.4367 - accuracy: 0.7965 - auc: 0.8781\n",
      "Epoch 00054: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.4379 - accuracy: 0.7958 - auc: 0.8774 - val_loss: 1.2900 - val_accuracy: 0.5143 - val_auc: 0.4913\n",
      "Epoch 55/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.4213 - accuracy: 0.8085 - auc: 0.8876\n",
      "Epoch 00055: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.4219 - accuracy: 0.8057 - auc: 0.8875 - val_loss: 1.2495 - val_accuracy: 0.5014 - val_auc: 0.4944\n",
      "Epoch 56/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.4167 - accuracy: 0.8083 - auc: 0.8892\n",
      "Epoch 00056: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.4167 - accuracy: 0.8083 - auc: 0.8892 - val_loss: 1.2228 - val_accuracy: 0.4968 - val_auc: 0.5013\n",
      "Epoch 57/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.4156 - accuracy: 0.8087 - auc: 0.8906\n",
      "Epoch 00057: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.4156 - accuracy: 0.8087 - auc: 0.8906 - val_loss: 1.3629 - val_accuracy: 0.4959 - val_auc: 0.4899\n",
      "Epoch 58/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.4125 - accuracy: 0.8101 - auc: 0.8923\n",
      "Epoch 00058: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.4137 - accuracy: 0.8082 - auc: 0.8916 - val_loss: 1.3556 - val_accuracy: 0.4949 - val_auc: 0.4762\n",
      "Epoch 59/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.4113 - accuracy: 0.8123 - auc: 0.8932\n",
      "Epoch 00059: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.4114 - accuracy: 0.8107 - auc: 0.8932 - val_loss: 1.3244 - val_accuracy: 0.5106 - val_auc: 0.4992\n",
      "Epoch 60/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.4024 - accuracy: 0.8151 - auc: 0.8978\n",
      "Epoch 00060: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.4024 - accuracy: 0.8151 - auc: 0.8978 - val_loss: 1.2828 - val_accuracy: 0.5179 - val_auc: 0.5081\n",
      "Epoch 61/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.3877 - accuracy: 0.8274 - auc: 0.9059\n",
      "Epoch 00061: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.3877 - accuracy: 0.8274 - auc: 0.9059 - val_loss: 1.2426 - val_accuracy: 0.5041 - val_auc: 0.5127\n",
      "Epoch 62/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.3838 - accuracy: 0.8292 - auc: 0.9072\n",
      "Epoch 00062: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.3846 - accuracy: 0.8284 - auc: 0.9066 - val_loss: 1.3341 - val_accuracy: 0.4683 - val_auc: 0.4816\n",
      "Epoch 63/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.3869 - accuracy: 0.8284 - auc: 0.9058\n",
      "Epoch 00063: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.3869 - accuracy: 0.8284 - auc: 0.9058 - val_loss: 1.3311 - val_accuracy: 0.5152 - val_auc: 0.5064\n",
      "Epoch 64/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.4060 - accuracy: 0.8139 - auc: 0.8960\n",
      "Epoch 00064: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.4060 - accuracy: 0.8139 - auc: 0.8960 - val_loss: 1.2650 - val_accuracy: 0.4775 - val_auc: 0.4916\n",
      "Epoch 65/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.3844 - accuracy: 0.8297 - auc: 0.9070\n",
      "Epoch 00065: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.3844 - accuracy: 0.8297 - auc: 0.9070 - val_loss: 1.4618 - val_accuracy: 0.4765 - val_auc: 0.4659\n",
      "Epoch 66/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.3776 - accuracy: 0.8315 - auc: 0.9110\n",
      "Epoch 00066: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.3776 - accuracy: 0.8315 - auc: 0.9110 - val_loss: 1.4063 - val_accuracy: 0.4793 - val_auc: 0.4634\n",
      "Epoch 67/400\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.3589 - accuracy: 0.8449 - auc: 0.9207\n",
      "Epoch 00067: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.3584 - accuracy: 0.8430 - auc: 0.9206 - val_loss: 1.4153 - val_accuracy: 0.5189 - val_auc: 0.5014\n",
      "Epoch 68/400\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.3618 - accuracy: 0.8416 - auc: 0.9184\n",
      "Epoch 00068: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.3608 - accuracy: 0.8419 - auc: 0.9189 - val_loss: 1.3061 - val_accuracy: 0.5051 - val_auc: 0.4988\n",
      "Epoch 69/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.3637 - accuracy: 0.8416 - auc: 0.9171\n",
      "Epoch 00069: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.3637 - accuracy: 0.8416 - auc: 0.9171 - val_loss: 1.1690 - val_accuracy: 0.5207 - val_auc: 0.5345\n",
      "Epoch 70/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.3592 - accuracy: 0.8446 - auc: 0.9192\n",
      "Epoch 00070: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.3623 - accuracy: 0.8432 - auc: 0.9179 - val_loss: 1.2555 - val_accuracy: 0.4986 - val_auc: 0.5134\n",
      "Epoch 71/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.3591 - accuracy: 0.8462 - auc: 0.9199\n",
      "Epoch 00071: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.3576 - accuracy: 0.8455 - auc: 0.9208 - val_loss: 1.3863 - val_accuracy: 0.5290 - val_auc: 0.5109\n",
      "Epoch 72/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.3455 - accuracy: 0.8519 - auc: 0.9261\n",
      "Epoch 00072: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.3455 - accuracy: 0.8519 - auc: 0.9261 - val_loss: 1.4397 - val_accuracy: 0.5078 - val_auc: 0.4940\n",
      "Epoch 73/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.3344 - accuracy: 0.8561 - auc: 0.9313\n",
      "Epoch 00073: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.3462 - accuracy: 0.8523 - auc: 0.9254 - val_loss: 1.4889 - val_accuracy: 0.5087 - val_auc: 0.4944\n",
      "Epoch 74/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.3484 - accuracy: 0.8501 - auc: 0.9245\n",
      "Epoch 00074: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.3484 - accuracy: 0.8501 - auc: 0.9245 - val_loss: 1.3566 - val_accuracy: 0.5244 - val_auc: 0.5159\n",
      "Epoch 75/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.3442 - accuracy: 0.8548 - auc: 0.9267\n",
      "Epoch 00075: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.3442 - accuracy: 0.8548 - auc: 0.9267 - val_loss: 1.3150 - val_accuracy: 0.5216 - val_auc: 0.5284\n",
      "Epoch 76/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.3390 - accuracy: 0.8526 - auc: 0.9290\n",
      "Epoch 00076: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.3348 - accuracy: 0.8554 - auc: 0.9307 - val_loss: 1.4301 - val_accuracy: 0.5308 - val_auc: 0.5238\n",
      "Epoch 77/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.3231 - accuracy: 0.8650 - auc: 0.9353\n",
      "Epoch 00077: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.3231 - accuracy: 0.8650 - auc: 0.9353 - val_loss: 1.3555 - val_accuracy: 0.5281 - val_auc: 0.5105\n",
      "Epoch 78/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.3124 - accuracy: 0.8716 - auc: 0.9401\n",
      "Epoch 00078: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.3124 - accuracy: 0.8716 - auc: 0.9401 - val_loss: 1.4714 - val_accuracy: 0.5299 - val_auc: 0.5131\n",
      "Epoch 79/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.3133 - accuracy: 0.8715 - auc: 0.9397\n",
      "Epoch 00079: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.3087 - accuracy: 0.8736 - auc: 0.9414 - val_loss: 1.4417 - val_accuracy: 0.4922 - val_auc: 0.5010\n",
      "Epoch 80/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.3076 - accuracy: 0.8750 - auc: 0.9412\n",
      "Epoch 00080: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.3076 - accuracy: 0.8750 - auc: 0.9412 - val_loss: 1.5376 - val_accuracy: 0.5051 - val_auc: 0.5136\n",
      "Epoch 81/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.3159 - accuracy: 0.8666 - auc: 0.9384\n",
      "Epoch 00081: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.3159 - accuracy: 0.8666 - auc: 0.9384 - val_loss: 1.4532 - val_accuracy: 0.5253 - val_auc: 0.5145\n",
      "Epoch 82/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.3073 - accuracy: 0.8726 - auc: 0.9416\n",
      "Epoch 00082: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.3073 - accuracy: 0.8726 - auc: 0.9416 - val_loss: 1.4134 - val_accuracy: 0.5069 - val_auc: 0.5074\n",
      "Epoch 83/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.3037 - accuracy: 0.8756 - auc: 0.9433\n",
      "Epoch 00083: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.3037 - accuracy: 0.8756 - auc: 0.9433 - val_loss: 1.4398 - val_accuracy: 0.5198 - val_auc: 0.5217\n",
      "Epoch 84/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.2944 - accuracy: 0.8847 - auc: 0.9469\n",
      "Epoch 00084: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.2980 - accuracy: 0.8815 - auc: 0.9452 - val_loss: 1.4274 - val_accuracy: 0.5345 - val_auc: 0.5240\n",
      "Epoch 85/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.2889 - accuracy: 0.8840 - auc: 0.9492\n",
      "Epoch 00085: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.2913 - accuracy: 0.8829 - auc: 0.9482 - val_loss: 1.4755 - val_accuracy: 0.5216 - val_auc: 0.5210\n",
      "Epoch 86/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2931 - accuracy: 0.8790 - auc: 0.9470\n",
      "Epoch 00086: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.2931 - accuracy: 0.8790 - auc: 0.9470 - val_loss: 1.5038 - val_accuracy: 0.5409 - val_auc: 0.5374\n",
      "Epoch 87/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2954 - accuracy: 0.8786 - auc: 0.9465\n",
      "Epoch 00087: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.2954 - accuracy: 0.8786 - auc: 0.9465 - val_loss: 1.4806 - val_accuracy: 0.5345 - val_auc: 0.5247\n",
      "Epoch 88/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.2984 - accuracy: 0.8818 - auc: 0.9449\n",
      "Epoch 00088: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.2980 - accuracy: 0.8812 - auc: 0.9452 - val_loss: 1.3969 - val_accuracy: 0.5317 - val_auc: 0.5394\n",
      "Epoch 89/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2988 - accuracy: 0.8774 - auc: 0.9446\n",
      "Epoch 00089: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.2988 - accuracy: 0.8774 - auc: 0.9446 - val_loss: 1.2592 - val_accuracy: 0.5133 - val_auc: 0.5432\n",
      "Epoch 90/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.2895 - accuracy: 0.8825 - auc: 0.9480\n",
      "Epoch 00090: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.2948 - accuracy: 0.8796 - auc: 0.9461 - val_loss: 1.3861 - val_accuracy: 0.5363 - val_auc: 0.5498\n",
      "Epoch 91/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.2820 - accuracy: 0.8834 - auc: 0.9510\n",
      "Epoch 00091: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.2817 - accuracy: 0.8834 - auc: 0.9511 - val_loss: 1.5468 - val_accuracy: 0.5152 - val_auc: 0.5183\n",
      "Epoch 92/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.2721 - accuracy: 0.8879 - auc: 0.9542\n",
      "Epoch 00092: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.2721 - accuracy: 0.8882 - auc: 0.9542 - val_loss: 1.5086 - val_accuracy: 0.5271 - val_auc: 0.5289\n",
      "Epoch 93/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2737 - accuracy: 0.8883 - auc: 0.9537\n",
      "Epoch 00093: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.2737 - accuracy: 0.8883 - auc: 0.9537 - val_loss: 1.4986 - val_accuracy: 0.5317 - val_auc: 0.5205\n",
      "Epoch 94/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.2705 - accuracy: 0.8908 - auc: 0.9550\n",
      "Epoch 00094: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.2693 - accuracy: 0.8934 - auc: 0.9552 - val_loss: 1.4421 - val_accuracy: 0.5299 - val_auc: 0.5430\n",
      "Epoch 95/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.2632 - accuracy: 0.8954 - auc: 0.9574\n",
      "Epoch 00095: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.2585 - accuracy: 0.8973 - auc: 0.9591 - val_loss: 1.5606 - val_accuracy: 0.5308 - val_auc: 0.5266\n",
      "Epoch 96/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2537 - accuracy: 0.9004 - auc: 0.9603\n",
      "Epoch 00096: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.2537 - accuracy: 0.9004 - auc: 0.9603 - val_loss: 1.4579 - val_accuracy: 0.5465 - val_auc: 0.5387\n",
      "Epoch 97/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2628 - accuracy: 0.8956 - auc: 0.9573\n",
      "Epoch 00097: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.2628 - accuracy: 0.8956 - auc: 0.9573 - val_loss: 1.5840 - val_accuracy: 0.5419 - val_auc: 0.5278\n",
      "Epoch 98/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2539 - accuracy: 0.8973 - auc: 0.9605\n",
      "Epoch 00098: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.2539 - accuracy: 0.8973 - auc: 0.9605 - val_loss: 1.5680 - val_accuracy: 0.5124 - val_auc: 0.5213\n",
      "Epoch 99/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2452 - accuracy: 0.9029 - auc: 0.9629\n",
      "Epoch 00099: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.2452 - accuracy: 0.9029 - auc: 0.9629 - val_loss: 1.5346 - val_accuracy: 0.5419 - val_auc: 0.5520\n",
      "Epoch 100/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2451 - accuracy: 0.9029 - auc: 0.9627\n",
      "Epoch 00100: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.2451 - accuracy: 0.9029 - auc: 0.9627 - val_loss: 1.5611 - val_accuracy: 0.5106 - val_auc: 0.5197\n",
      "Epoch 101/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2521 - accuracy: 0.8975 - auc: 0.9602\n",
      "Epoch 00101: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.2521 - accuracy: 0.8975 - auc: 0.9602 - val_loss: 1.5011 - val_accuracy: 0.5216 - val_auc: 0.5433\n",
      "Epoch 102/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2723 - accuracy: 0.8868 - auc: 0.9543\n",
      "Epoch 00102: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.2723 - accuracy: 0.8868 - auc: 0.9543 - val_loss: 1.4940 - val_accuracy: 0.5354 - val_auc: 0.5297\n",
      "Epoch 103/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2861 - accuracy: 0.8843 - auc: 0.9489\n",
      "Epoch 00103: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.2861 - accuracy: 0.8843 - auc: 0.9489 - val_loss: 1.6110 - val_accuracy: 0.5373 - val_auc: 0.5324\n",
      "Epoch 104/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.2690 - accuracy: 0.8903 - auc: 0.9552\n",
      "Epoch 00104: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 0.2750 - accuracy: 0.8878 - auc: 0.9530 - val_loss: 1.7353 - val_accuracy: 0.5069 - val_auc: 0.5034\n",
      "Epoch 105/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2594 - accuracy: 0.8960 - auc: 0.9584\n",
      "Epoch 00105: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.2594 - accuracy: 0.8960 - auc: 0.9584 - val_loss: 1.5876 - val_accuracy: 0.5317 - val_auc: 0.5209\n",
      "Epoch 106/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.2453 - accuracy: 0.9018 - auc: 0.9634\n",
      "Epoch 00106: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.2498 - accuracy: 0.8998 - auc: 0.9616 - val_loss: 1.5699 - val_accuracy: 0.5152 - val_auc: 0.5053\n",
      "Epoch 107/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2486 - accuracy: 0.9025 - auc: 0.9617\n",
      "Epoch 00107: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 0.2486 - accuracy: 0.9025 - auc: 0.9617 - val_loss: 1.7834 - val_accuracy: 0.4968 - val_auc: 0.5162\n",
      "Epoch 108/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2491 - accuracy: 0.9008 - auc: 0.9614\n",
      "Epoch 00108: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.2491 - accuracy: 0.9008 - auc: 0.9614 - val_loss: 1.4730 - val_accuracy: 0.5483 - val_auc: 0.5439\n",
      "Epoch 109/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2451 - accuracy: 0.8988 - auc: 0.9626\n",
      "Epoch 00109: val_accuracy did not improve from 0.55566\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.2451 - accuracy: 0.8988 - auc: 0.9626 - val_loss: 1.4740 - val_accuracy: 0.5501 - val_auc: 0.5557\n",
      "Epoch 110/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2548 - accuracy: 0.8945 - auc: 0.9597\n",
      "Epoch 00110: val_accuracy improved from 0.55566 to 0.58050, saving model to ../my_stuff/EURUSD-H1_0.01-min_profit_0.2-lots_right-cur_side_9-30-60-cb-tk-tkp-sen-chi-ichi_cnn-lstm_classifier.hdf5\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.2548 - accuracy: 0.8945 - auc: 0.9597 - val_loss: 1.5703 - val_accuracy: 0.5805 - val_auc: 0.5440\n",
      "Epoch 111/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2953 - accuracy: 0.8773 - auc: 0.9465\n",
      "Epoch 00111: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.2953 - accuracy: 0.8773 - auc: 0.9465 - val_loss: 1.7054 - val_accuracy: 0.5124 - val_auc: 0.5153\n",
      "Epoch 112/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2640 - accuracy: 0.8924 - auc: 0.9571\n",
      "Epoch 00112: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.2640 - accuracy: 0.8924 - auc: 0.9571 - val_loss: 1.4393 - val_accuracy: 0.5290 - val_auc: 0.5250\n",
      "Epoch 113/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2554 - accuracy: 0.8961 - auc: 0.9599\n",
      "Epoch 00113: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.2554 - accuracy: 0.8961 - auc: 0.9599 - val_loss: 1.5745 - val_accuracy: 0.5409 - val_auc: 0.5525\n",
      "Epoch 114/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2458 - accuracy: 0.9016 - auc: 0.9624\n",
      "Epoch 00114: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.2458 - accuracy: 0.9016 - auc: 0.9624 - val_loss: 1.6577 - val_accuracy: 0.5152 - val_auc: 0.5150\n",
      "Epoch 115/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.2341 - accuracy: 0.9088 - auc: 0.9665\n",
      "Epoch 00115: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.2333 - accuracy: 0.9091 - auc: 0.9666 - val_loss: 1.5966 - val_accuracy: 0.5455 - val_auc: 0.5451\n",
      "Epoch 116/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2319 - accuracy: 0.9082 - auc: 0.9664\n",
      "Epoch 00116: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.2319 - accuracy: 0.9082 - auc: 0.9664 - val_loss: 1.6934 - val_accuracy: 0.5308 - val_auc: 0.5189\n",
      "Epoch 117/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2227 - accuracy: 0.9131 - auc: 0.9690\n",
      "Epoch 00117: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.2227 - accuracy: 0.9131 - auc: 0.9690 - val_loss: 1.7217 - val_accuracy: 0.5299 - val_auc: 0.5244\n",
      "Epoch 118/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2182 - accuracy: 0.9154 - auc: 0.9704\n",
      "Epoch 00118: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.2182 - accuracy: 0.9154 - auc: 0.9704 - val_loss: 1.6700 - val_accuracy: 0.5465 - val_auc: 0.5384\n",
      "Epoch 119/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2160 - accuracy: 0.9162 - auc: 0.9710\n",
      "Epoch 00119: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.2160 - accuracy: 0.9162 - auc: 0.9710 - val_loss: 1.6421 - val_accuracy: 0.5244 - val_auc: 0.5282\n",
      "Epoch 120/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2413 - accuracy: 0.9042 - auc: 0.9636\n",
      "Epoch 00120: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.2413 - accuracy: 0.9042 - auc: 0.9636 - val_loss: 1.5521 - val_accuracy: 0.5097 - val_auc: 0.5190\n",
      "Epoch 121/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2479 - accuracy: 0.9006 - auc: 0.9620\n",
      "Epoch 00121: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.2479 - accuracy: 0.9006 - auc: 0.9620 - val_loss: 1.6732 - val_accuracy: 0.5317 - val_auc: 0.5224\n",
      "Epoch 122/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2357 - accuracy: 0.9078 - auc: 0.9655\n",
      "Epoch 00122: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.2357 - accuracy: 0.9078 - auc: 0.9655 - val_loss: 1.5574 - val_accuracy: 0.5501 - val_auc: 0.5414\n",
      "Epoch 123/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2231 - accuracy: 0.9135 - auc: 0.9689\n",
      "Epoch 00123: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.2231 - accuracy: 0.9135 - auc: 0.9689 - val_loss: 1.6433 - val_accuracy: 0.5354 - val_auc: 0.5197\n",
      "Epoch 124/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2130 - accuracy: 0.9172 - auc: 0.9717\n",
      "Epoch 00124: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.2130 - accuracy: 0.9172 - auc: 0.9717 - val_loss: 1.6373 - val_accuracy: 0.5409 - val_auc: 0.5421\n",
      "Epoch 125/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.2143 - accuracy: 0.9171 - auc: 0.9713\n",
      "Epoch 00125: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.2157 - accuracy: 0.9155 - auc: 0.9710 - val_loss: 1.7556 - val_accuracy: 0.5271 - val_auc: 0.5113\n",
      "Epoch 126/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2126 - accuracy: 0.9140 - auc: 0.9719\n",
      "Epoch 00126: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.2126 - accuracy: 0.9140 - auc: 0.9719 - val_loss: 1.6220 - val_accuracy: 0.5584 - val_auc: 0.5400\n",
      "Epoch 127/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2149 - accuracy: 0.9127 - auc: 0.9710\n",
      "Epoch 00127: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.2149 - accuracy: 0.9127 - auc: 0.9710 - val_loss: 1.5856 - val_accuracy: 0.5354 - val_auc: 0.5360\n",
      "Epoch 128/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2087 - accuracy: 0.9184 - auc: 0.9728\n",
      "Epoch 00128: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.2087 - accuracy: 0.9184 - auc: 0.9728 - val_loss: 1.6989 - val_accuracy: 0.5317 - val_auc: 0.5226\n",
      "Epoch 129/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2039 - accuracy: 0.9241 - auc: 0.9739\n",
      "Epoch 00129: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.2039 - accuracy: 0.9241 - auc: 0.9739 - val_loss: 1.8433 - val_accuracy: 0.5373 - val_auc: 0.5254\n",
      "Epoch 130/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2072 - accuracy: 0.9192 - auc: 0.9732\n",
      "Epoch 00130: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.2072 - accuracy: 0.9192 - auc: 0.9732 - val_loss: 1.5374 - val_accuracy: 0.5676 - val_auc: 0.5683\n",
      "Epoch 131/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2094 - accuracy: 0.9195 - auc: 0.9726\n",
      "Epoch 00131: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.2094 - accuracy: 0.9195 - auc: 0.9726 - val_loss: 1.7049 - val_accuracy: 0.5170 - val_auc: 0.5144\n",
      "Epoch 132/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1984 - accuracy: 0.9228 - auc: 0.9754\n",
      "Epoch 00132: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 0.1984 - accuracy: 0.9228 - auc: 0.9754 - val_loss: 1.6746 - val_accuracy: 0.5492 - val_auc: 0.5404\n",
      "Epoch 133/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1986 - accuracy: 0.9243 - auc: 0.9754\n",
      "Epoch 00133: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1986 - accuracy: 0.9243 - auc: 0.9754 - val_loss: 1.7011 - val_accuracy: 0.5501 - val_auc: 0.5419\n",
      "Epoch 134/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.2012 - accuracy: 0.9192 - auc: 0.9749\n",
      "Epoch 00134: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.2025 - accuracy: 0.9194 - auc: 0.9744 - val_loss: 1.7473 - val_accuracy: 0.5428 - val_auc: 0.5258\n",
      "Epoch 135/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2059 - accuracy: 0.9204 - auc: 0.9733\n",
      "Epoch 00135: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.2059 - accuracy: 0.9204 - auc: 0.9733 - val_loss: 1.9252 - val_accuracy: 0.4913 - val_auc: 0.4967\n",
      "Epoch 136/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.2050 - accuracy: 0.9198 - auc: 0.9738\n",
      "Epoch 00136: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.2096 - accuracy: 0.9186 - auc: 0.9726 - val_loss: 1.7577 - val_accuracy: 0.5520 - val_auc: 0.5264\n",
      "Epoch 137/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2035 - accuracy: 0.9239 - auc: 0.9742\n",
      "Epoch 00137: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.2035 - accuracy: 0.9239 - auc: 0.9742 - val_loss: 1.6680 - val_accuracy: 0.5437 - val_auc: 0.5519\n",
      "Epoch 138/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2059 - accuracy: 0.9195 - auc: 0.9737\n",
      "Epoch 00138: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.2059 - accuracy: 0.9195 - auc: 0.9737 - val_loss: 1.7831 - val_accuracy: 0.5354 - val_auc: 0.5322\n",
      "Epoch 139/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.2058 - accuracy: 0.9185 - auc: 0.9734\n",
      "Epoch 00139: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.2059 - accuracy: 0.9186 - auc: 0.9734 - val_loss: 1.6715 - val_accuracy: 0.5271 - val_auc: 0.5366\n",
      "Epoch 140/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2076 - accuracy: 0.9204 - auc: 0.9732\n",
      "Epoch 00140: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.2076 - accuracy: 0.9204 - auc: 0.9732 - val_loss: 1.7012 - val_accuracy: 0.5501 - val_auc: 0.5459\n",
      "Epoch 141/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1941 - accuracy: 0.9238 - auc: 0.9765\n",
      "Epoch 00141: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.1941 - accuracy: 0.9238 - auc: 0.9765 - val_loss: 1.7222 - val_accuracy: 0.5455 - val_auc: 0.5499\n",
      "Epoch 142/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1918 - accuracy: 0.9288 - auc: 0.9771\n",
      "Epoch 00142: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1918 - accuracy: 0.9288 - auc: 0.9771 - val_loss: 1.7836 - val_accuracy: 0.5400 - val_auc: 0.5397\n",
      "Epoch 143/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1915 - accuracy: 0.9255 - auc: 0.9771\n",
      "Epoch 00143: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1915 - accuracy: 0.9255 - auc: 0.9771 - val_loss: 1.7879 - val_accuracy: 0.5225 - val_auc: 0.5189\n",
      "Epoch 144/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1957 - accuracy: 0.9252 - auc: 0.9758\n",
      "Epoch 00144: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1957 - accuracy: 0.9252 - auc: 0.9758 - val_loss: 1.7612 - val_accuracy: 0.5336 - val_auc: 0.5362\n",
      "Epoch 145/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1937 - accuracy: 0.9257 - auc: 0.9764\n",
      "Epoch 00145: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.1937 - accuracy: 0.9257 - auc: 0.9764 - val_loss: 1.8223 - val_accuracy: 0.5290 - val_auc: 0.5127\n",
      "Epoch 146/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1869 - accuracy: 0.9279 - auc: 0.9782\n",
      "Epoch 00146: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1869 - accuracy: 0.9279 - auc: 0.9782 - val_loss: 1.7215 - val_accuracy: 0.5419 - val_auc: 0.5417\n",
      "Epoch 147/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1923 - accuracy: 0.9257 - auc: 0.9770\n",
      "Epoch 00147: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.1923 - accuracy: 0.9257 - auc: 0.9770 - val_loss: 1.8123 - val_accuracy: 0.5391 - val_auc: 0.5306\n",
      "Epoch 148/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1921 - accuracy: 0.9251 - auc: 0.9768\n",
      "Epoch 00148: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1921 - accuracy: 0.9251 - auc: 0.9768 - val_loss: 1.8389 - val_accuracy: 0.5593 - val_auc: 0.5473\n",
      "Epoch 149/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2022 - accuracy: 0.9212 - auc: 0.9744\n",
      "Epoch 00149: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.2022 - accuracy: 0.9212 - auc: 0.9744 - val_loss: 1.9351 - val_accuracy: 0.5345 - val_auc: 0.5291\n",
      "Epoch 150/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1974 - accuracy: 0.9247 - auc: 0.9756\n",
      "Epoch 00150: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1974 - accuracy: 0.9247 - auc: 0.9756 - val_loss: 1.7922 - val_accuracy: 0.5511 - val_auc: 0.5422\n",
      "Epoch 151/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1917 - accuracy: 0.9250 - auc: 0.9772\n",
      "Epoch 00151: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1917 - accuracy: 0.9250 - auc: 0.9772 - val_loss: 1.8608 - val_accuracy: 0.5271 - val_auc: 0.5062\n",
      "Epoch 152/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1836 - accuracy: 0.9279 - auc: 0.9790\n",
      "Epoch 00152: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.1836 - accuracy: 0.9279 - auc: 0.9790 - val_loss: 1.8166 - val_accuracy: 0.5419 - val_auc: 0.5422\n",
      "Epoch 153/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1773 - accuracy: 0.9327 - auc: 0.9804\n",
      "Epoch 00153: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.1773 - accuracy: 0.9327 - auc: 0.9804 - val_loss: 1.8220 - val_accuracy: 0.5133 - val_auc: 0.5085\n",
      "Epoch 154/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1846 - accuracy: 0.9281 - auc: 0.9786\n",
      "Epoch 00154: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1846 - accuracy: 0.9281 - auc: 0.9786 - val_loss: 1.8646 - val_accuracy: 0.5253 - val_auc: 0.5178\n",
      "Epoch 155/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1895 - accuracy: 0.9257 - auc: 0.9777\n",
      "Epoch 00155: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.1895 - accuracy: 0.9257 - auc: 0.9777 - val_loss: 1.9308 - val_accuracy: 0.5317 - val_auc: 0.5107\n",
      "Epoch 156/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1856 - accuracy: 0.9279 - auc: 0.9784\n",
      "Epoch 00156: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1856 - accuracy: 0.9279 - auc: 0.9784 - val_loss: 1.8009 - val_accuracy: 0.5207 - val_auc: 0.5274\n",
      "Epoch 157/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1854 - accuracy: 0.9280 - auc: 0.9785\n",
      "Epoch 00157: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1854 - accuracy: 0.9280 - auc: 0.9785 - val_loss: 1.9748 - val_accuracy: 0.5428 - val_auc: 0.5148\n",
      "Epoch 158/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1825 - accuracy: 0.9286 - auc: 0.9791\n",
      "Epoch 00158: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1825 - accuracy: 0.9286 - auc: 0.9791 - val_loss: 1.8638 - val_accuracy: 0.5455 - val_auc: 0.5369\n",
      "Epoch 159/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1735 - accuracy: 0.9350 - auc: 0.9811\n",
      "Epoch 00159: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1735 - accuracy: 0.9350 - auc: 0.9811 - val_loss: 1.9433 - val_accuracy: 0.5483 - val_auc: 0.5340\n",
      "Epoch 160/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.1710 - accuracy: 0.9333 - auc: 0.9818\n",
      "Epoch 00160: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.1713 - accuracy: 0.9338 - auc: 0.9818 - val_loss: 2.0228 - val_accuracy: 0.5290 - val_auc: 0.5245\n",
      "Epoch 161/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1672 - accuracy: 0.9340 - auc: 0.9826\n",
      "Epoch 00161: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1672 - accuracy: 0.9340 - auc: 0.9826 - val_loss: 1.9198 - val_accuracy: 0.5520 - val_auc: 0.5280\n",
      "Epoch 162/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1684 - accuracy: 0.9335 - auc: 0.9823\n",
      "Epoch 00162: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1684 - accuracy: 0.9335 - auc: 0.9823 - val_loss: 2.0939 - val_accuracy: 0.5308 - val_auc: 0.5088\n",
      "Epoch 163/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1629 - accuracy: 0.9374 - auc: 0.9835\n",
      "Epoch 00163: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1629 - accuracy: 0.9374 - auc: 0.9835 - val_loss: 2.0061 - val_accuracy: 0.5400 - val_auc: 0.5226\n",
      "Epoch 164/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1689 - accuracy: 0.9356 - auc: 0.9822\n",
      "Epoch 00164: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1689 - accuracy: 0.9356 - auc: 0.9822 - val_loss: 2.0505 - val_accuracy: 0.5032 - val_auc: 0.4954\n",
      "Epoch 165/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.1730 - accuracy: 0.9310 - auc: 0.9815\n",
      "Epoch 00165: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1761 - accuracy: 0.9293 - auc: 0.9808 - val_loss: 2.0265 - val_accuracy: 0.5373 - val_auc: 0.5221\n",
      "Epoch 166/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1812 - accuracy: 0.9272 - auc: 0.9798\n",
      "Epoch 00166: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1812 - accuracy: 0.9272 - auc: 0.9798 - val_loss: 2.0156 - val_accuracy: 0.5198 - val_auc: 0.5163\n",
      "Epoch 167/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1729 - accuracy: 0.9316 - auc: 0.9814\n",
      "Epoch 00167: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1729 - accuracy: 0.9316 - auc: 0.9814 - val_loss: 2.0280 - val_accuracy: 0.5069 - val_auc: 0.4969\n",
      "Epoch 168/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1762 - accuracy: 0.9296 - auc: 0.9807\n",
      "Epoch 00168: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.1762 - accuracy: 0.9296 - auc: 0.9807 - val_loss: 2.0845 - val_accuracy: 0.5225 - val_auc: 0.5121\n",
      "Epoch 169/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1689 - accuracy: 0.9316 - auc: 0.9823\n",
      "Epoch 00169: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.1689 - accuracy: 0.9316 - auc: 0.9823 - val_loss: 1.9684 - val_accuracy: 0.5345 - val_auc: 0.5257\n",
      "Epoch 170/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1714 - accuracy: 0.9336 - auc: 0.9817\n",
      "Epoch 00170: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1714 - accuracy: 0.9336 - auc: 0.9817 - val_loss: 1.9585 - val_accuracy: 0.5382 - val_auc: 0.5236\n",
      "Epoch 171/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.1802 - accuracy: 0.9291 - auc: 0.9798\n",
      "Epoch 00171: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.1861 - accuracy: 0.9265 - auc: 0.9785 - val_loss: 1.8457 - val_accuracy: 0.5584 - val_auc: 0.5472\n",
      "Epoch 172/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1948 - accuracy: 0.9230 - auc: 0.9765\n",
      "Epoch 00172: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.1948 - accuracy: 0.9230 - auc: 0.9765 - val_loss: 1.8532 - val_accuracy: 0.5327 - val_auc: 0.5089\n",
      "Epoch 173/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1939 - accuracy: 0.9255 - auc: 0.9765\n",
      "Epoch 00173: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.1939 - accuracy: 0.9255 - auc: 0.9765 - val_loss: 1.7346 - val_accuracy: 0.5400 - val_auc: 0.5356\n",
      "Epoch 174/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1982 - accuracy: 0.9234 - auc: 0.9756\n",
      "Epoch 00174: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1982 - accuracy: 0.9234 - auc: 0.9756 - val_loss: 1.7919 - val_accuracy: 0.5474 - val_auc: 0.5352\n",
      "Epoch 175/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2006 - accuracy: 0.9210 - auc: 0.9751\n",
      "Epoch 00175: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.2006 - accuracy: 0.9210 - auc: 0.9751 - val_loss: 1.9239 - val_accuracy: 0.5216 - val_auc: 0.5006\n",
      "Epoch 176/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.1812 - accuracy: 0.9275 - auc: 0.9798\n",
      "Epoch 00176: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.1809 - accuracy: 0.9273 - auc: 0.9799 - val_loss: 1.9207 - val_accuracy: 0.5225 - val_auc: 0.5166\n",
      "Epoch 177/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1700 - accuracy: 0.9329 - auc: 0.9820\n",
      "Epoch 00177: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.1700 - accuracy: 0.9329 - auc: 0.9820 - val_loss: 1.9322 - val_accuracy: 0.5078 - val_auc: 0.5094\n",
      "Epoch 178/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.1885 - accuracy: 0.9270 - auc: 0.9777\n",
      "Epoch 00178: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.1929 - accuracy: 0.9250 - auc: 0.9767 - val_loss: 1.9453 - val_accuracy: 0.5373 - val_auc: 0.5327\n",
      "Epoch 179/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.1753 - accuracy: 0.9298 - auc: 0.9812\n",
      "Epoch 00179: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.1751 - accuracy: 0.9306 - auc: 0.9811 - val_loss: 1.8447 - val_accuracy: 0.5511 - val_auc: 0.5468\n",
      "Epoch 180/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1677 - accuracy: 0.9349 - auc: 0.9826\n",
      "Epoch 00180: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1677 - accuracy: 0.9349 - auc: 0.9826 - val_loss: 1.8100 - val_accuracy: 0.5465 - val_auc: 0.5326\n",
      "Epoch 181/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1729 - accuracy: 0.9312 - auc: 0.9815\n",
      "Epoch 00181: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1729 - accuracy: 0.9312 - auc: 0.9815 - val_loss: 1.9065 - val_accuracy: 0.5235 - val_auc: 0.5158\n",
      "Epoch 182/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1624 - accuracy: 0.9339 - auc: 0.9838\n",
      "Epoch 00182: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1624 - accuracy: 0.9339 - auc: 0.9838 - val_loss: 1.9271 - val_accuracy: 0.5299 - val_auc: 0.5236\n",
      "Epoch 183/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1571 - accuracy: 0.9374 - auc: 0.9847\n",
      "Epoch 00183: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1571 - accuracy: 0.9374 - auc: 0.9847 - val_loss: 2.0055 - val_accuracy: 0.5115 - val_auc: 0.5115\n",
      "Epoch 184/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.1706 - accuracy: 0.9342 - auc: 0.9815\n",
      "Epoch 00184: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.1729 - accuracy: 0.9317 - auc: 0.9812 - val_loss: 1.9308 - val_accuracy: 0.5308 - val_auc: 0.5325\n",
      "Epoch 185/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.1699 - accuracy: 0.9340 - auc: 0.9816\n",
      "Epoch 00185: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1658 - accuracy: 0.9359 - auc: 0.9824 - val_loss: 1.8797 - val_accuracy: 0.5428 - val_auc: 0.5404\n",
      "Epoch 186/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1596 - accuracy: 0.9381 - auc: 0.9843\n",
      "Epoch 00186: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.1596 - accuracy: 0.9381 - auc: 0.9843 - val_loss: 1.9885 - val_accuracy: 0.5235 - val_auc: 0.5255\n",
      "Epoch 187/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1480 - accuracy: 0.9400 - auc: 0.9865\n",
      "Epoch 00187: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.1480 - accuracy: 0.9400 - auc: 0.9865 - val_loss: 2.0704 - val_accuracy: 0.5271 - val_auc: 0.5288\n",
      "Epoch 188/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1462 - accuracy: 0.9426 - auc: 0.9869\n",
      "Epoch 00188: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.1462 - accuracy: 0.9426 - auc: 0.9869 - val_loss: 1.9257 - val_accuracy: 0.5244 - val_auc: 0.5269\n",
      "Epoch 189/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1439 - accuracy: 0.9432 - auc: 0.9873\n",
      "Epoch 00189: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1439 - accuracy: 0.9432 - auc: 0.9873 - val_loss: 2.0733 - val_accuracy: 0.5400 - val_auc: 0.5313\n",
      "Epoch 190/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1431 - accuracy: 0.9427 - auc: 0.9872\n",
      "Epoch 00190: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1431 - accuracy: 0.9427 - auc: 0.9872 - val_loss: 2.1352 - val_accuracy: 0.5106 - val_auc: 0.4955\n",
      "Epoch 191/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1406 - accuracy: 0.9442 - auc: 0.9878\n",
      "Epoch 00191: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1406 - accuracy: 0.9442 - auc: 0.9878 - val_loss: 2.1325 - val_accuracy: 0.5207 - val_auc: 0.5210\n",
      "Epoch 192/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1377 - accuracy: 0.9456 - auc: 0.9883\n",
      "Epoch 00192: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1377 - accuracy: 0.9456 - auc: 0.9883 - val_loss: 2.1596 - val_accuracy: 0.5216 - val_auc: 0.5093\n",
      "Epoch 193/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1365 - accuracy: 0.9461 - auc: 0.9886\n",
      "Epoch 00193: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1365 - accuracy: 0.9461 - auc: 0.9886 - val_loss: 2.2452 - val_accuracy: 0.5097 - val_auc: 0.5035\n",
      "Epoch 194/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1434 - accuracy: 0.9405 - auc: 0.9873\n",
      "Epoch 00194: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.1434 - accuracy: 0.9405 - auc: 0.9873 - val_loss: 2.1358 - val_accuracy: 0.5207 - val_auc: 0.5146\n",
      "Epoch 195/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1509 - accuracy: 0.9373 - auc: 0.9860\n",
      "Epoch 00195: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1509 - accuracy: 0.9373 - auc: 0.9860 - val_loss: 2.1240 - val_accuracy: 0.5511 - val_auc: 0.5198\n",
      "Epoch 196/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1469 - accuracy: 0.9429 - auc: 0.9868\n",
      "Epoch 00196: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1469 - accuracy: 0.9429 - auc: 0.9868 - val_loss: 2.0501 - val_accuracy: 0.5271 - val_auc: 0.5341\n",
      "Epoch 197/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1434 - accuracy: 0.9438 - auc: 0.9873\n",
      "Epoch 00197: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1434 - accuracy: 0.9438 - auc: 0.9873 - val_loss: 2.0752 - val_accuracy: 0.5235 - val_auc: 0.5199\n",
      "Epoch 198/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1482 - accuracy: 0.9398 - auc: 0.9863\n",
      "Epoch 00198: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1482 - accuracy: 0.9398 - auc: 0.9863 - val_loss: 2.0027 - val_accuracy: 0.5391 - val_auc: 0.5323\n",
      "Epoch 199/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1483 - accuracy: 0.9424 - auc: 0.9865\n",
      "Epoch 00199: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1483 - accuracy: 0.9424 - auc: 0.9865 - val_loss: 2.2266 - val_accuracy: 0.5336 - val_auc: 0.5178\n",
      "Epoch 200/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1437 - accuracy: 0.9415 - auc: 0.9873\n",
      "Epoch 00200: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1437 - accuracy: 0.9415 - auc: 0.9873 - val_loss: 2.2308 - val_accuracy: 0.5363 - val_auc: 0.5116\n",
      "Epoch 201/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1379 - accuracy: 0.9428 - auc: 0.9883\n",
      "Epoch 00201: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1379 - accuracy: 0.9428 - auc: 0.9883 - val_loss: 2.2584 - val_accuracy: 0.5271 - val_auc: 0.5157\n",
      "Epoch 202/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1405 - accuracy: 0.9437 - auc: 0.9879\n",
      "Epoch 00202: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1405 - accuracy: 0.9437 - auc: 0.9879 - val_loss: 2.2462 - val_accuracy: 0.5336 - val_auc: 0.5171\n",
      "Epoch 203/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1424 - accuracy: 0.9412 - auc: 0.9875\n",
      "Epoch 00203: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1424 - accuracy: 0.9412 - auc: 0.9875 - val_loss: 2.1831 - val_accuracy: 0.5382 - val_auc: 0.5345\n",
      "Epoch 204/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1406 - accuracy: 0.9418 - auc: 0.9877\n",
      "Epoch 00204: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.1406 - accuracy: 0.9418 - auc: 0.9877 - val_loss: 2.1429 - val_accuracy: 0.5115 - val_auc: 0.5126\n",
      "Epoch 205/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1423 - accuracy: 0.9418 - auc: 0.9876\n",
      "Epoch 00205: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1423 - accuracy: 0.9418 - auc: 0.9876 - val_loss: 2.2317 - val_accuracy: 0.5446 - val_auc: 0.5356\n",
      "Epoch 206/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1388 - accuracy: 0.9416 - auc: 0.9881\n",
      "Epoch 00206: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1388 - accuracy: 0.9416 - auc: 0.9881 - val_loss: 2.3014 - val_accuracy: 0.5207 - val_auc: 0.4944\n",
      "Epoch 207/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1367 - accuracy: 0.9437 - auc: 0.9886\n",
      "Epoch 00207: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.1367 - accuracy: 0.9437 - auc: 0.9886 - val_loss: 2.2185 - val_accuracy: 0.5382 - val_auc: 0.5301\n",
      "Epoch 208/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1445 - accuracy: 0.9395 - auc: 0.9871\n",
      "Epoch 00208: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1445 - accuracy: 0.9395 - auc: 0.9871 - val_loss: 2.2950 - val_accuracy: 0.5198 - val_auc: 0.5148\n",
      "Epoch 209/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1491 - accuracy: 0.9410 - auc: 0.9864\n",
      "Epoch 00209: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1491 - accuracy: 0.9410 - auc: 0.9864 - val_loss: 2.1436 - val_accuracy: 0.5354 - val_auc: 0.5248\n",
      "Epoch 210/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1446 - accuracy: 0.9435 - auc: 0.9871\n",
      "Epoch 00210: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1446 - accuracy: 0.9435 - auc: 0.9871 - val_loss: 2.1885 - val_accuracy: 0.5465 - val_auc: 0.5330\n",
      "Epoch 211/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1400 - accuracy: 0.9420 - auc: 0.9878\n",
      "Epoch 00211: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.1400 - accuracy: 0.9420 - auc: 0.9878 - val_loss: 2.2756 - val_accuracy: 0.5446 - val_auc: 0.5239\n",
      "Epoch 212/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1415 - accuracy: 0.9409 - auc: 0.9875\n",
      "Epoch 00212: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.1415 - accuracy: 0.9409 - auc: 0.9875 - val_loss: 2.1692 - val_accuracy: 0.5354 - val_auc: 0.5049\n",
      "Epoch 213/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1339 - accuracy: 0.9439 - auc: 0.9890\n",
      "Epoch 00213: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1339 - accuracy: 0.9439 - auc: 0.9890 - val_loss: 2.2552 - val_accuracy: 0.5382 - val_auc: 0.5385\n",
      "Epoch 214/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1494 - accuracy: 0.9403 - auc: 0.9863\n",
      "Epoch 00214: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1494 - accuracy: 0.9403 - auc: 0.9863 - val_loss: 2.1610 - val_accuracy: 0.5501 - val_auc: 0.5416\n",
      "Epoch 215/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1704 - accuracy: 0.9313 - auc: 0.9821\n",
      "Epoch 00215: val_accuracy did not improve from 0.58050\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1704 - accuracy: 0.9313 - auc: 0.9821 - val_loss: 1.9841 - val_accuracy: 0.5244 - val_auc: 0.5343\n",
      "Epoch 216/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1684 - accuracy: 0.9330 - auc: 0.9824\n",
      "Epoch 00216: val_accuracy improved from 0.58050 to 0.58694, saving model to ../my_stuff/EURUSD-H1_0.01-min_profit_0.2-lots_right-cur_side_9-30-60-cb-tk-tkp-sen-chi-ichi_cnn-lstm_classifier.hdf5\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.1684 - accuracy: 0.9330 - auc: 0.9824 - val_loss: 2.0096 - val_accuracy: 0.5869 - val_auc: 0.5462\n",
      "Epoch 217/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1533 - accuracy: 0.9358 - auc: 0.9854\n",
      "Epoch 00217: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1533 - accuracy: 0.9358 - auc: 0.9854 - val_loss: 2.0580 - val_accuracy: 0.5621 - val_auc: 0.5410\n",
      "Epoch 218/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1481 - accuracy: 0.9386 - auc: 0.9866\n",
      "Epoch 00218: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1481 - accuracy: 0.9386 - auc: 0.9866 - val_loss: 2.2504 - val_accuracy: 0.5179 - val_auc: 0.5164\n",
      "Epoch 219/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.1419 - accuracy: 0.9441 - auc: 0.9876\n",
      "Epoch 00219: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.1435 - accuracy: 0.9427 - auc: 0.9874 - val_loss: 2.1773 - val_accuracy: 0.5281 - val_auc: 0.5295\n",
      "Epoch 220/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1392 - accuracy: 0.9417 - auc: 0.9883\n",
      "Epoch 00220: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.1392 - accuracy: 0.9417 - auc: 0.9883 - val_loss: 2.4340 - val_accuracy: 0.5225 - val_auc: 0.5138\n",
      "Epoch 221/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.1451 - accuracy: 0.9384 - auc: 0.9869\n",
      "Epoch 00221: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.1454 - accuracy: 0.9373 - auc: 0.9869 - val_loss: 2.1826 - val_accuracy: 0.5235 - val_auc: 0.5382\n",
      "Epoch 222/400\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1340 - accuracy: 0.9456 - auc: 0.9892\n",
      "Epoch 00222: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.1415 - accuracy: 0.9421 - auc: 0.9877 - val_loss: 2.2521 - val_accuracy: 0.5198 - val_auc: 0.5246\n",
      "Epoch 223/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1698 - accuracy: 0.9355 - auc: 0.9824\n",
      "Epoch 00223: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1698 - accuracy: 0.9355 - auc: 0.9824 - val_loss: 2.3268 - val_accuracy: 0.4747 - val_auc: 0.5212\n",
      "Epoch 224/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1973 - accuracy: 0.9216 - auc: 0.9767\n",
      "Epoch 00224: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1973 - accuracy: 0.9216 - auc: 0.9767 - val_loss: 2.1790 - val_accuracy: 0.5373 - val_auc: 0.5294\n",
      "Epoch 225/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1804 - accuracy: 0.9241 - auc: 0.9804\n",
      "Epoch 00225: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1804 - accuracy: 0.9241 - auc: 0.9804 - val_loss: 2.1205 - val_accuracy: 0.5198 - val_auc: 0.5151\n",
      "Epoch 226/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1659 - accuracy: 0.9334 - auc: 0.9830\n",
      "Epoch 00226: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1659 - accuracy: 0.9334 - auc: 0.9830 - val_loss: 2.0502 - val_accuracy: 0.4931 - val_auc: 0.5328\n",
      "Epoch 227/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1598 - accuracy: 0.9341 - auc: 0.9842\n",
      "Epoch 00227: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1598 - accuracy: 0.9341 - auc: 0.9842 - val_loss: 2.0719 - val_accuracy: 0.5382 - val_auc: 0.5432\n",
      "Epoch 228/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1471 - accuracy: 0.9389 - auc: 0.9869\n",
      "Epoch 00228: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1471 - accuracy: 0.9389 - auc: 0.9869 - val_loss: 2.1002 - val_accuracy: 0.5373 - val_auc: 0.5323\n",
      "Epoch 229/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1411 - accuracy: 0.9432 - auc: 0.9879\n",
      "Epoch 00229: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.1411 - accuracy: 0.9432 - auc: 0.9879 - val_loss: 2.1803 - val_accuracy: 0.5409 - val_auc: 0.5354\n",
      "Epoch 230/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1278 - accuracy: 0.9458 - auc: 0.9902\n",
      "Epoch 00230: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1278 - accuracy: 0.9458 - auc: 0.9902 - val_loss: 2.0419 - val_accuracy: 0.5428 - val_auc: 0.5491\n",
      "Epoch 231/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1283 - accuracy: 0.9469 - auc: 0.9900\n",
      "Epoch 00231: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1283 - accuracy: 0.9469 - auc: 0.9900 - val_loss: 2.1163 - val_accuracy: 0.5465 - val_auc: 0.5488\n",
      "Epoch 232/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1231 - accuracy: 0.9475 - auc: 0.9909\n",
      "Epoch 00232: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1231 - accuracy: 0.9475 - auc: 0.9909 - val_loss: 2.1570 - val_accuracy: 0.5409 - val_auc: 0.5330\n",
      "Epoch 233/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1180 - accuracy: 0.9496 - auc: 0.9918\n",
      "Epoch 00233: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1180 - accuracy: 0.9496 - auc: 0.9918 - val_loss: 2.1803 - val_accuracy: 0.5409 - val_auc: 0.5518\n",
      "Epoch 234/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1195 - accuracy: 0.9502 - auc: 0.9913\n",
      "Epoch 00234: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1195 - accuracy: 0.9502 - auc: 0.9913 - val_loss: 2.1457 - val_accuracy: 0.5455 - val_auc: 0.5388\n",
      "Epoch 235/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.1211 - accuracy: 0.9509 - auc: 0.9911\n",
      "Epoch 00235: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1235 - accuracy: 0.9502 - auc: 0.9906 - val_loss: 2.3827 - val_accuracy: 0.5409 - val_auc: 0.5397\n",
      "Epoch 236/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1251 - accuracy: 0.9470 - auc: 0.9906\n",
      "Epoch 00236: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.1251 - accuracy: 0.9470 - auc: 0.9906 - val_loss: 2.3671 - val_accuracy: 0.5271 - val_auc: 0.5204\n",
      "Epoch 237/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1301 - accuracy: 0.9466 - auc: 0.9896\n",
      "Epoch 00237: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1301 - accuracy: 0.9466 - auc: 0.9896 - val_loss: 2.0825 - val_accuracy: 0.5391 - val_auc: 0.5514\n",
      "Epoch 238/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1211 - accuracy: 0.9490 - auc: 0.9911\n",
      "Epoch 00238: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.1211 - accuracy: 0.9490 - auc: 0.9911 - val_loss: 2.3215 - val_accuracy: 0.5455 - val_auc: 0.5381\n",
      "Epoch 239/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1200 - accuracy: 0.9497 - auc: 0.9912\n",
      "Epoch 00239: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.1200 - accuracy: 0.9497 - auc: 0.9912 - val_loss: 2.1928 - val_accuracy: 0.5437 - val_auc: 0.5429\n",
      "Epoch 240/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1213 - accuracy: 0.9485 - auc: 0.9910\n",
      "Epoch 00240: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1213 - accuracy: 0.9485 - auc: 0.9910 - val_loss: 2.2412 - val_accuracy: 0.5446 - val_auc: 0.5399\n",
      "Epoch 241/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1181 - accuracy: 0.9494 - auc: 0.9915\n",
      "Epoch 00241: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1181 - accuracy: 0.9494 - auc: 0.9915 - val_loss: 2.2884 - val_accuracy: 0.5409 - val_auc: 0.5290\n",
      "Epoch 242/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1225 - accuracy: 0.9492 - auc: 0.9909\n",
      "Epoch 00242: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1225 - accuracy: 0.9492 - auc: 0.9909 - val_loss: 2.3408 - val_accuracy: 0.5437 - val_auc: 0.5329\n",
      "Epoch 243/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1198 - accuracy: 0.9496 - auc: 0.9913\n",
      "Epoch 00243: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1198 - accuracy: 0.9496 - auc: 0.9913 - val_loss: 2.3584 - val_accuracy: 0.5419 - val_auc: 0.5254\n",
      "Epoch 244/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1280 - accuracy: 0.9475 - auc: 0.9898\n",
      "Epoch 00244: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1280 - accuracy: 0.9475 - auc: 0.9898 - val_loss: 2.1853 - val_accuracy: 0.5308 - val_auc: 0.5473\n",
      "Epoch 245/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1308 - accuracy: 0.9443 - auc: 0.9895\n",
      "Epoch 00245: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1308 - accuracy: 0.9443 - auc: 0.9895 - val_loss: 2.3554 - val_accuracy: 0.5465 - val_auc: 0.5261\n",
      "Epoch 246/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1275 - accuracy: 0.9458 - auc: 0.9900\n",
      "Epoch 00246: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1275 - accuracy: 0.9458 - auc: 0.9900 - val_loss: 2.3018 - val_accuracy: 0.5409 - val_auc: 0.5338\n",
      "Epoch 247/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1265 - accuracy: 0.9450 - auc: 0.9900\n",
      "Epoch 00247: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1265 - accuracy: 0.9450 - auc: 0.9900 - val_loss: 2.4125 - val_accuracy: 0.5363 - val_auc: 0.5241\n",
      "Epoch 248/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1194 - accuracy: 0.9507 - auc: 0.9912\n",
      "Epoch 00248: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1194 - accuracy: 0.9507 - auc: 0.9912 - val_loss: 2.2973 - val_accuracy: 0.5428 - val_auc: 0.5474\n",
      "Epoch 249/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1187 - accuracy: 0.9511 - auc: 0.9915\n",
      "Epoch 00249: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1187 - accuracy: 0.9511 - auc: 0.9915 - val_loss: 2.4182 - val_accuracy: 0.5290 - val_auc: 0.5336\n",
      "Epoch 250/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1203 - accuracy: 0.9491 - auc: 0.9912\n",
      "Epoch 00250: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1203 - accuracy: 0.9491 - auc: 0.9912 - val_loss: 2.4002 - val_accuracy: 0.5363 - val_auc: 0.5227\n",
      "Epoch 251/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1135 - accuracy: 0.9518 - auc: 0.9922\n",
      "Epoch 00251: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.1135 - accuracy: 0.9518 - auc: 0.9922 - val_loss: 2.3815 - val_accuracy: 0.5345 - val_auc: 0.5208\n",
      "Epoch 252/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1109 - accuracy: 0.9520 - auc: 0.9926\n",
      "Epoch 00252: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1109 - accuracy: 0.9520 - auc: 0.9926 - val_loss: 2.5150 - val_accuracy: 0.5308 - val_auc: 0.5163\n",
      "Epoch 253/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1092 - accuracy: 0.9541 - auc: 0.9928\n",
      "Epoch 00253: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1092 - accuracy: 0.9541 - auc: 0.9928 - val_loss: 2.4819 - val_accuracy: 0.5225 - val_auc: 0.5173\n",
      "Epoch 254/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1079 - accuracy: 0.9540 - auc: 0.9930\n",
      "Epoch 00254: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1079 - accuracy: 0.9540 - auc: 0.9930 - val_loss: 2.4776 - val_accuracy: 0.5262 - val_auc: 0.5219\n",
      "Epoch 255/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1101 - accuracy: 0.9536 - auc: 0.9927\n",
      "Epoch 00255: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.1101 - accuracy: 0.9536 - auc: 0.9927 - val_loss: 2.4789 - val_accuracy: 0.5244 - val_auc: 0.5206\n",
      "Epoch 256/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1169 - accuracy: 0.9497 - auc: 0.9917\n",
      "Epoch 00256: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1169 - accuracy: 0.9497 - auc: 0.9917 - val_loss: 2.5371 - val_accuracy: 0.5437 - val_auc: 0.5289\n",
      "Epoch 257/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1156 - accuracy: 0.9516 - auc: 0.9920\n",
      "Epoch 00257: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1156 - accuracy: 0.9516 - auc: 0.9920 - val_loss: 2.7125 - val_accuracy: 0.5170 - val_auc: 0.4981\n",
      "Epoch 258/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1209 - accuracy: 0.9480 - auc: 0.9911\n",
      "Epoch 00258: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1209 - accuracy: 0.9480 - auc: 0.9911 - val_loss: 2.5381 - val_accuracy: 0.5253 - val_auc: 0.5215\n",
      "Epoch 259/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1254 - accuracy: 0.9479 - auc: 0.9903\n",
      "Epoch 00259: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1254 - accuracy: 0.9479 - auc: 0.9903 - val_loss: 2.3594 - val_accuracy: 0.5428 - val_auc: 0.5491\n",
      "Epoch 260/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1277 - accuracy: 0.9475 - auc: 0.9898\n",
      "Epoch 00260: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.1277 - accuracy: 0.9475 - auc: 0.9898 - val_loss: 2.4293 - val_accuracy: 0.5262 - val_auc: 0.5363\n",
      "Epoch 261/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.1408 - accuracy: 0.9396 - auc: 0.9877\n",
      "Epoch 00261: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1368 - accuracy: 0.9411 - auc: 0.9884 - val_loss: 2.4883 - val_accuracy: 0.5161 - val_auc: 0.5285\n",
      "Epoch 262/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1539 - accuracy: 0.9370 - auc: 0.9853\n",
      "Epoch 00262: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.1539 - accuracy: 0.9370 - auc: 0.9853 - val_loss: 2.4996 - val_accuracy: 0.5133 - val_auc: 0.5198\n",
      "Epoch 263/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1787 - accuracy: 0.9309 - auc: 0.9802\n",
      "Epoch 00263: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1787 - accuracy: 0.9309 - auc: 0.9802 - val_loss: 2.2950 - val_accuracy: 0.5115 - val_auc: 0.5094\n",
      "Epoch 264/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1755 - accuracy: 0.9287 - auc: 0.9812\n",
      "Epoch 00264: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1755 - accuracy: 0.9287 - auc: 0.9812 - val_loss: 2.2266 - val_accuracy: 0.5097 - val_auc: 0.5202\n",
      "Epoch 265/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1765 - accuracy: 0.9291 - auc: 0.9807\n",
      "Epoch 00265: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.1765 - accuracy: 0.9291 - auc: 0.9807 - val_loss: 2.2588 - val_accuracy: 0.5189 - val_auc: 0.5029\n",
      "Epoch 266/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1503 - accuracy: 0.9418 - auc: 0.9859\n",
      "Epoch 00266: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1503 - accuracy: 0.9418 - auc: 0.9859 - val_loss: 2.3758 - val_accuracy: 0.5308 - val_auc: 0.5013\n",
      "Epoch 267/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1383 - accuracy: 0.9443 - auc: 0.9882\n",
      "Epoch 00267: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1383 - accuracy: 0.9443 - auc: 0.9882 - val_loss: 2.4355 - val_accuracy: 0.5281 - val_auc: 0.5104\n",
      "Epoch 268/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1297 - accuracy: 0.9458 - auc: 0.9896\n",
      "Epoch 00268: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1297 - accuracy: 0.9458 - auc: 0.9896 - val_loss: 2.4724 - val_accuracy: 0.5143 - val_auc: 0.5175\n",
      "Epoch 269/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1215 - accuracy: 0.9498 - auc: 0.9911\n",
      "Epoch 00269: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1215 - accuracy: 0.9498 - auc: 0.9911 - val_loss: 2.2942 - val_accuracy: 0.5419 - val_auc: 0.5206\n",
      "Epoch 270/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1166 - accuracy: 0.9497 - auc: 0.9920\n",
      "Epoch 00270: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1166 - accuracy: 0.9497 - auc: 0.9920 - val_loss: 2.4168 - val_accuracy: 0.5474 - val_auc: 0.5341\n",
      "Epoch 271/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.1058 - accuracy: 0.9576 - auc: 0.9934\n",
      "Epoch 00271: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1084 - accuracy: 0.9548 - auc: 0.9931 - val_loss: 2.4025 - val_accuracy: 0.5170 - val_auc: 0.5267\n",
      "Epoch 272/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1037 - accuracy: 0.9546 - auc: 0.9937\n",
      "Epoch 00272: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1037 - accuracy: 0.9546 - auc: 0.9937 - val_loss: 2.5278 - val_accuracy: 0.5041 - val_auc: 0.5157\n",
      "Epoch 273/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1025 - accuracy: 0.9556 - auc: 0.9938\n",
      "Epoch 00273: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1025 - accuracy: 0.9556 - auc: 0.9938 - val_loss: 2.5001 - val_accuracy: 0.5170 - val_auc: 0.5267\n",
      "Epoch 274/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1035 - accuracy: 0.9565 - auc: 0.9937\n",
      "Epoch 00274: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1035 - accuracy: 0.9565 - auc: 0.9937 - val_loss: 2.6089 - val_accuracy: 0.5308 - val_auc: 0.5215\n",
      "Epoch 275/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1055 - accuracy: 0.9561 - auc: 0.9932\n",
      "Epoch 00275: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1055 - accuracy: 0.9561 - auc: 0.9932 - val_loss: 2.5319 - val_accuracy: 0.5271 - val_auc: 0.5226\n",
      "Epoch 276/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1047 - accuracy: 0.9553 - auc: 0.9935\n",
      "Epoch 00276: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.1047 - accuracy: 0.9553 - auc: 0.9935 - val_loss: 2.4501 - val_accuracy: 0.5400 - val_auc: 0.5422\n",
      "Epoch 277/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1026 - accuracy: 0.9559 - auc: 0.9937\n",
      "Epoch 00277: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.1026 - accuracy: 0.9559 - auc: 0.9937 - val_loss: 2.4393 - val_accuracy: 0.5299 - val_auc: 0.5283\n",
      "Epoch 278/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0995 - accuracy: 0.9576 - auc: 0.9942\n",
      "Epoch 00278: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0995 - accuracy: 0.9576 - auc: 0.9942 - val_loss: 2.4936 - val_accuracy: 0.5465 - val_auc: 0.5272\n",
      "Epoch 279/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1011 - accuracy: 0.9552 - auc: 0.9938\n",
      "Epoch 00279: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.1011 - accuracy: 0.9552 - auc: 0.9938 - val_loss: 2.5618 - val_accuracy: 0.5244 - val_auc: 0.5271\n",
      "Epoch 280/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0984 - accuracy: 0.9577 - auc: 0.9942\n",
      "Epoch 00280: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0984 - accuracy: 0.9577 - auc: 0.9942 - val_loss: 2.6763 - val_accuracy: 0.5069 - val_auc: 0.5149\n",
      "Epoch 281/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0977 - accuracy: 0.9574 - auc: 0.9943\n",
      "Epoch 00281: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0977 - accuracy: 0.9574 - auc: 0.9943 - val_loss: 2.6875 - val_accuracy: 0.5327 - val_auc: 0.5235\n",
      "Epoch 282/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0971 - accuracy: 0.9584 - auc: 0.9943\n",
      "Epoch 00282: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0975 - accuracy: 0.9584 - auc: 0.9943 - val_loss: 2.6671 - val_accuracy: 0.5170 - val_auc: 0.5168\n",
      "Epoch 283/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1005 - accuracy: 0.9566 - auc: 0.9939\n",
      "Epoch 00283: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.1005 - accuracy: 0.9566 - auc: 0.9939 - val_loss: 2.7530 - val_accuracy: 0.5143 - val_auc: 0.5248\n",
      "Epoch 284/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1050 - accuracy: 0.9568 - auc: 0.9931\n",
      "Epoch 00284: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.1050 - accuracy: 0.9568 - auc: 0.9931 - val_loss: 2.7305 - val_accuracy: 0.5069 - val_auc: 0.5044\n",
      "Epoch 285/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1117 - accuracy: 0.9541 - auc: 0.9923\n",
      "Epoch 00285: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1117 - accuracy: 0.9541 - auc: 0.9923 - val_loss: 2.5428 - val_accuracy: 0.5290 - val_auc: 0.5281\n",
      "Epoch 286/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1126 - accuracy: 0.9518 - auc: 0.9921\n",
      "Epoch 00286: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1126 - accuracy: 0.9518 - auc: 0.9921 - val_loss: 2.5314 - val_accuracy: 0.5538 - val_auc: 0.5292\n",
      "Epoch 287/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1133 - accuracy: 0.9525 - auc: 0.9920\n",
      "Epoch 00287: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.1133 - accuracy: 0.9525 - auc: 0.9920 - val_loss: 2.5491 - val_accuracy: 0.5179 - val_auc: 0.5205\n",
      "Epoch 288/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1128 - accuracy: 0.9525 - auc: 0.9922\n",
      "Epoch 00288: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1128 - accuracy: 0.9525 - auc: 0.9922 - val_loss: 2.6425 - val_accuracy: 0.5253 - val_auc: 0.5192\n",
      "Epoch 289/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1047 - accuracy: 0.9557 - auc: 0.9934\n",
      "Epoch 00289: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1047 - accuracy: 0.9557 - auc: 0.9934 - val_loss: 2.6632 - val_accuracy: 0.5097 - val_auc: 0.5122\n",
      "Epoch 290/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1080 - accuracy: 0.9548 - auc: 0.9927\n",
      "Epoch 00290: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1080 - accuracy: 0.9548 - auc: 0.9927 - val_loss: 2.5524 - val_accuracy: 0.5547 - val_auc: 0.5306\n",
      "Epoch 291/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1041 - accuracy: 0.9555 - auc: 0.9935\n",
      "Epoch 00291: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.1041 - accuracy: 0.9555 - auc: 0.9935 - val_loss: 2.6353 - val_accuracy: 0.5566 - val_auc: 0.5221\n",
      "Epoch 292/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1119 - accuracy: 0.9525 - auc: 0.9924\n",
      "Epoch 00292: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1119 - accuracy: 0.9525 - auc: 0.9924 - val_loss: 2.6326 - val_accuracy: 0.5069 - val_auc: 0.5060\n",
      "Epoch 293/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1134 - accuracy: 0.9535 - auc: 0.9920\n",
      "Epoch 00293: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1134 - accuracy: 0.9535 - auc: 0.9920 - val_loss: 2.4973 - val_accuracy: 0.5336 - val_auc: 0.5411\n",
      "Epoch 294/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1281 - accuracy: 0.9465 - auc: 0.9897\n",
      "Epoch 00294: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1281 - accuracy: 0.9465 - auc: 0.9897 - val_loss: 2.6743 - val_accuracy: 0.5419 - val_auc: 0.5142\n",
      "Epoch 295/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1308 - accuracy: 0.9474 - auc: 0.9892\n",
      "Epoch 00295: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1308 - accuracy: 0.9474 - auc: 0.9892 - val_loss: 2.7227 - val_accuracy: 0.5170 - val_auc: 0.5128\n",
      "Epoch 296/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1240 - accuracy: 0.9481 - auc: 0.9905\n",
      "Epoch 00296: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.1240 - accuracy: 0.9481 - auc: 0.9905 - val_loss: 2.5341 - val_accuracy: 0.5032 - val_auc: 0.5102\n",
      "Epoch 297/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1354 - accuracy: 0.9474 - auc: 0.9884\n",
      "Epoch 00297: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1354 - accuracy: 0.9474 - auc: 0.9884 - val_loss: 2.6507 - val_accuracy: 0.5290 - val_auc: 0.5140\n",
      "Epoch 298/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1402 - accuracy: 0.9418 - auc: 0.9881\n",
      "Epoch 00298: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1402 - accuracy: 0.9418 - auc: 0.9881 - val_loss: 2.5241 - val_accuracy: 0.5400 - val_auc: 0.5252\n",
      "Epoch 299/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1420 - accuracy: 0.9414 - auc: 0.9875\n",
      "Epoch 00299: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1420 - accuracy: 0.9414 - auc: 0.9875 - val_loss: 2.5372 - val_accuracy: 0.5290 - val_auc: 0.5242\n",
      "Epoch 300/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1335 - accuracy: 0.9422 - auc: 0.9891\n",
      "Epoch 00300: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1335 - accuracy: 0.9422 - auc: 0.9891 - val_loss: 2.3404 - val_accuracy: 0.5520 - val_auc: 0.5579\n",
      "Epoch 301/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1263 - accuracy: 0.9470 - auc: 0.9903\n",
      "Epoch 00301: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1263 - accuracy: 0.9470 - auc: 0.9903 - val_loss: 2.3182 - val_accuracy: 0.5474 - val_auc: 0.5447\n",
      "Epoch 302/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1235 - accuracy: 0.9478 - auc: 0.9902\n",
      "Epoch 00302: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1235 - accuracy: 0.9478 - auc: 0.9902 - val_loss: 2.6420 - val_accuracy: 0.5363 - val_auc: 0.5230\n",
      "Epoch 303/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1100 - accuracy: 0.9523 - auc: 0.9927\n",
      "Epoch 00303: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1100 - accuracy: 0.9523 - auc: 0.9927 - val_loss: 2.6828 - val_accuracy: 0.5299 - val_auc: 0.5021\n",
      "Epoch 304/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1098 - accuracy: 0.9552 - auc: 0.9928\n",
      "Epoch 00304: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.1098 - accuracy: 0.9552 - auc: 0.9928 - val_loss: 2.4749 - val_accuracy: 0.5538 - val_auc: 0.5340\n",
      "Epoch 305/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1119 - accuracy: 0.9538 - auc: 0.9923\n",
      "Epoch 00305: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.1119 - accuracy: 0.9538 - auc: 0.9923 - val_loss: 2.5414 - val_accuracy: 0.5235 - val_auc: 0.5201\n",
      "Epoch 306/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1061 - accuracy: 0.9558 - auc: 0.9933\n",
      "Epoch 00306: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1061 - accuracy: 0.9558 - auc: 0.9933 - val_loss: 2.6658 - val_accuracy: 0.5639 - val_auc: 0.5327\n",
      "Epoch 307/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1042 - accuracy: 0.9555 - auc: 0.9935\n",
      "Epoch 00307: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1042 - accuracy: 0.9555 - auc: 0.9935 - val_loss: 2.4090 - val_accuracy: 0.5373 - val_auc: 0.5236\n",
      "Epoch 308/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1002 - accuracy: 0.9567 - auc: 0.9941\n",
      "Epoch 00308: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1002 - accuracy: 0.9567 - auc: 0.9941 - val_loss: 2.7040 - val_accuracy: 0.5446 - val_auc: 0.5288\n",
      "Epoch 309/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0936 - accuracy: 0.9598 - auc: 0.9948\n",
      "Epoch 00309: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.0933 - accuracy: 0.9605 - auc: 0.9949 - val_loss: 2.5697 - val_accuracy: 0.5262 - val_auc: 0.5331\n",
      "Epoch 310/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0921 - accuracy: 0.9599 - auc: 0.9950\n",
      "Epoch 00310: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0921 - accuracy: 0.9599 - auc: 0.9950 - val_loss: 2.8039 - val_accuracy: 0.5327 - val_auc: 0.5159\n",
      "Epoch 311/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0902 - accuracy: 0.9619 - auc: 0.9951\n",
      "Epoch 00311: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0902 - accuracy: 0.9619 - auc: 0.9951 - val_loss: 2.5813 - val_accuracy: 0.5492 - val_auc: 0.5373\n",
      "Epoch 312/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0848 - accuracy: 0.9632 - auc: 0.9958\n",
      "Epoch 00312: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.0848 - accuracy: 0.9632 - auc: 0.9958 - val_loss: 2.7506 - val_accuracy: 0.5244 - val_auc: 0.5037\n",
      "Epoch 313/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0900 - accuracy: 0.9597 - auc: 0.9952\n",
      "Epoch 00313: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.0900 - accuracy: 0.9597 - auc: 0.9952 - val_loss: 2.8074 - val_accuracy: 0.5400 - val_auc: 0.5269\n",
      "Epoch 314/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1078 - accuracy: 0.9538 - auc: 0.9929\n",
      "Epoch 00314: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.1078 - accuracy: 0.9538 - auc: 0.9929 - val_loss: 2.6857 - val_accuracy: 0.5345 - val_auc: 0.5134\n",
      "Epoch 315/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0992 - accuracy: 0.9570 - auc: 0.9940\n",
      "Epoch 00315: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0992 - accuracy: 0.9570 - auc: 0.9940 - val_loss: 2.5996 - val_accuracy: 0.5520 - val_auc: 0.5440\n",
      "Epoch 316/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1026 - accuracy: 0.9552 - auc: 0.9937\n",
      "Epoch 00316: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.1026 - accuracy: 0.9552 - auc: 0.9937 - val_loss: 2.5290 - val_accuracy: 0.5547 - val_auc: 0.5480\n",
      "Epoch 317/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.1055 - accuracy: 0.9562 - auc: 0.9931\n",
      "Epoch 00317: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.1048 - accuracy: 0.9562 - auc: 0.9932 - val_loss: 2.9206 - val_accuracy: 0.5446 - val_auc: 0.5279\n",
      "Epoch 318/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0996 - accuracy: 0.9566 - auc: 0.9939\n",
      "Epoch 00318: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.1021 - accuracy: 0.9557 - auc: 0.9935 - val_loss: 2.7139 - val_accuracy: 0.5382 - val_auc: 0.5225\n",
      "Epoch 319/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0939 - accuracy: 0.9617 - auc: 0.9945\n",
      "Epoch 00319: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0939 - accuracy: 0.9617 - auc: 0.9945 - val_loss: 2.6774 - val_accuracy: 0.5373 - val_auc: 0.5284\n",
      "Epoch 320/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0927 - accuracy: 0.9594 - auc: 0.9948\n",
      "Epoch 00320: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0927 - accuracy: 0.9594 - auc: 0.9948 - val_loss: 2.7830 - val_accuracy: 0.5584 - val_auc: 0.5285\n",
      "Epoch 321/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0923 - accuracy: 0.9606 - auc: 0.9948\n",
      "Epoch 00321: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0923 - accuracy: 0.9606 - auc: 0.9948 - val_loss: 2.8090 - val_accuracy: 0.5290 - val_auc: 0.5161\n",
      "Epoch 322/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0918 - accuracy: 0.9605 - auc: 0.9948\n",
      "Epoch 00322: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0918 - accuracy: 0.9605 - auc: 0.9948 - val_loss: 2.8175 - val_accuracy: 0.5685 - val_auc: 0.5386\n",
      "Epoch 323/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0958 - accuracy: 0.9591 - auc: 0.9942\n",
      "Epoch 00323: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.0958 - accuracy: 0.9591 - auc: 0.9942 - val_loss: 2.6579 - val_accuracy: 0.5336 - val_auc: 0.5285\n",
      "Epoch 324/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0922 - accuracy: 0.9604 - auc: 0.9949\n",
      "Epoch 00324: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.0918 - accuracy: 0.9606 - auc: 0.9949 - val_loss: 2.8836 - val_accuracy: 0.5391 - val_auc: 0.5243\n",
      "Epoch 325/400\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.0918 - accuracy: 0.9619 - auc: 0.9949\n",
      "Epoch 00325: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 0.0938 - accuracy: 0.9589 - auc: 0.9947 - val_loss: 2.6927 - val_accuracy: 0.5409 - val_auc: 0.5268\n",
      "Epoch 326/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0958 - accuracy: 0.9586 - auc: 0.9942\n",
      "Epoch 00326: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0958 - accuracy: 0.9586 - auc: 0.9942 - val_loss: 2.8169 - val_accuracy: 0.5446 - val_auc: 0.5204\n",
      "Epoch 327/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0988 - accuracy: 0.9570 - auc: 0.9940\n",
      "Epoch 00327: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.1001 - accuracy: 0.9558 - auc: 0.9939 - val_loss: 2.5681 - val_accuracy: 0.5501 - val_auc: 0.5390\n",
      "Epoch 328/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978 - accuracy: 0.9592 - auc: 0.9940\n",
      "Epoch 00328: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0978 - accuracy: 0.9592 - auc: 0.9940 - val_loss: 2.6203 - val_accuracy: 0.5198 - val_auc: 0.5154\n",
      "Epoch 329/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1314 - accuracy: 0.9445 - auc: 0.9893\n",
      "Epoch 00329: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.1314 - accuracy: 0.9445 - auc: 0.9893 - val_loss: 2.8750 - val_accuracy: 0.5271 - val_auc: 0.5157\n",
      "Epoch 330/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.1314 - accuracy: 0.9465 - auc: 0.9894\n",
      "Epoch 00330: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.1314 - accuracy: 0.9474 - auc: 0.9893 - val_loss: 2.7815 - val_accuracy: 0.5097 - val_auc: 0.5055\n",
      "Epoch 331/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1307 - accuracy: 0.9462 - auc: 0.9894\n",
      "Epoch 00331: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1307 - accuracy: 0.9462 - auc: 0.9894 - val_loss: 2.6565 - val_accuracy: 0.5327 - val_auc: 0.5254\n",
      "Epoch 332/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.1248 - accuracy: 0.9485 - auc: 0.9904\n",
      "Epoch 00332: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.1251 - accuracy: 0.9489 - auc: 0.9904 - val_loss: 2.4725 - val_accuracy: 0.5428 - val_auc: 0.5310\n",
      "Epoch 333/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1226 - accuracy: 0.9481 - auc: 0.9907\n",
      "Epoch 00333: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1226 - accuracy: 0.9481 - auc: 0.9907 - val_loss: 2.7926 - val_accuracy: 0.5336 - val_auc: 0.5209\n",
      "Epoch 334/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1138 - accuracy: 0.9516 - auc: 0.9920\n",
      "Epoch 00334: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.1138 - accuracy: 0.9516 - auc: 0.9920 - val_loss: 2.6846 - val_accuracy: 0.5253 - val_auc: 0.5264\n",
      "Epoch 335/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1054 - accuracy: 0.9546 - auc: 0.9932\n",
      "Epoch 00335: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.1054 - accuracy: 0.9546 - auc: 0.9932 - val_loss: 2.6611 - val_accuracy: 0.5382 - val_auc: 0.5263\n",
      "Epoch 336/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0936 - accuracy: 0.9594 - auc: 0.9948\n",
      "Epoch 00336: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0936 - accuracy: 0.9594 - auc: 0.9948 - val_loss: 2.7187 - val_accuracy: 0.5409 - val_auc: 0.5107\n",
      "Epoch 337/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0967 - accuracy: 0.9577 - auc: 0.9945\n",
      "Epoch 00337: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0968 - accuracy: 0.9576 - auc: 0.9945 - val_loss: 2.5529 - val_accuracy: 0.5658 - val_auc: 0.5509\n",
      "Epoch 338/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0944 - accuracy: 0.9609 - auc: 0.9947\n",
      "Epoch 00338: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.0933 - accuracy: 0.9609 - auc: 0.9948 - val_loss: 2.6793 - val_accuracy: 0.5382 - val_auc: 0.5213\n",
      "Epoch 339/400\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.0893 - accuracy: 0.9625 - auc: 0.9954\n",
      "Epoch 00339: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.0878 - accuracy: 0.9630 - auc: 0.9956 - val_loss: 2.7239 - val_accuracy: 0.5511 - val_auc: 0.5390\n",
      "Epoch 340/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0885 - accuracy: 0.9628 - auc: 0.9954\n",
      "Epoch 00340: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0885 - accuracy: 0.9628 - auc: 0.9954 - val_loss: 2.8214 - val_accuracy: 0.5161 - val_auc: 0.5092\n",
      "Epoch 341/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0886 - accuracy: 0.9622 - auc: 0.9954\n",
      "Epoch 00341: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0886 - accuracy: 0.9622 - auc: 0.9954 - val_loss: 2.8024 - val_accuracy: 0.5308 - val_auc: 0.5233\n",
      "Epoch 342/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0947 - accuracy: 0.9588 - auc: 0.9946\n",
      "Epoch 00342: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0947 - accuracy: 0.9588 - auc: 0.9946 - val_loss: 2.6881 - val_accuracy: 0.5419 - val_auc: 0.5328\n",
      "Epoch 343/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981 - accuracy: 0.9608 - auc: 0.9940\n",
      "Epoch 00343: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0981 - accuracy: 0.9608 - auc: 0.9940 - val_loss: 2.9117 - val_accuracy: 0.5290 - val_auc: 0.5094\n",
      "Epoch 344/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0983 - accuracy: 0.9590 - auc: 0.9941\n",
      "Epoch 00344: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0983 - accuracy: 0.9590 - auc: 0.9941 - val_loss: 2.9517 - val_accuracy: 0.5207 - val_auc: 0.5054\n",
      "Epoch 345/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1002 - accuracy: 0.9565 - auc: 0.9940\n",
      "Epoch 00345: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1002 - accuracy: 0.9565 - auc: 0.9940 - val_loss: 2.6991 - val_accuracy: 0.5520 - val_auc: 0.5357\n",
      "Epoch 346/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1027 - accuracy: 0.9580 - auc: 0.9932\n",
      "Epoch 00346: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1027 - accuracy: 0.9580 - auc: 0.9932 - val_loss: 2.6343 - val_accuracy: 0.5428 - val_auc: 0.5271\n",
      "Epoch 347/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0988 - accuracy: 0.9564 - auc: 0.9938\n",
      "Epoch 00347: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0988 - accuracy: 0.9564 - auc: 0.9938 - val_loss: 2.7412 - val_accuracy: 0.5446 - val_auc: 0.5398\n",
      "Epoch 348/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967 - accuracy: 0.9603 - auc: 0.9944\n",
      "Epoch 00348: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0967 - accuracy: 0.9603 - auc: 0.9944 - val_loss: 2.6152 - val_accuracy: 0.5658 - val_auc: 0.5375\n",
      "Epoch 349/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0899 - accuracy: 0.9611 - auc: 0.9952\n",
      "Epoch 00349: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0899 - accuracy: 0.9611 - auc: 0.9952 - val_loss: 2.8197 - val_accuracy: 0.5198 - val_auc: 0.5209\n",
      "Epoch 350/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0888 - accuracy: 0.9619 - auc: 0.9954\n",
      "Epoch 00350: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0888 - accuracy: 0.9619 - auc: 0.9954 - val_loss: 2.7876 - val_accuracy: 0.5327 - val_auc: 0.5301\n",
      "Epoch 351/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0918 - accuracy: 0.9603 - auc: 0.9949\n",
      "Epoch 00351: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.0933 - accuracy: 0.9598 - auc: 0.9947 - val_loss: 2.8125 - val_accuracy: 0.5327 - val_auc: 0.5269\n",
      "Epoch 352/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0876 - accuracy: 0.9607 - auc: 0.9954\n",
      "Epoch 00352: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0876 - accuracy: 0.9607 - auc: 0.9954 - val_loss: 2.7777 - val_accuracy: 0.5189 - val_auc: 0.5207\n",
      "Epoch 353/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971 - accuracy: 0.9591 - auc: 0.9941\n",
      "Epoch 00353: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0971 - accuracy: 0.9591 - auc: 0.9941 - val_loss: 2.9964 - val_accuracy: 0.5106 - val_auc: 0.5086\n",
      "Epoch 354/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1014 - accuracy: 0.9556 - auc: 0.9937\n",
      "Epoch 00354: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1014 - accuracy: 0.9556 - auc: 0.9937 - val_loss: 2.7086 - val_accuracy: 0.5501 - val_auc: 0.5438\n",
      "Epoch 355/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1086 - accuracy: 0.9545 - auc: 0.9925\n",
      "Epoch 00355: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1086 - accuracy: 0.9545 - auc: 0.9925 - val_loss: 3.0628 - val_accuracy: 0.5345 - val_auc: 0.5094\n",
      "Epoch 356/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1130 - accuracy: 0.9527 - auc: 0.9920\n",
      "Epoch 00356: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1130 - accuracy: 0.9527 - auc: 0.9920 - val_loss: 2.7893 - val_accuracy: 0.5198 - val_auc: 0.5296\n",
      "Epoch 357/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1152 - accuracy: 0.9515 - auc: 0.9917\n",
      "Epoch 00357: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1152 - accuracy: 0.9515 - auc: 0.9917 - val_loss: 2.6848 - val_accuracy: 0.5428 - val_auc: 0.5448\n",
      "Epoch 358/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1117 - accuracy: 0.9526 - auc: 0.9923\n",
      "Epoch 00358: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1117 - accuracy: 0.9526 - auc: 0.9923 - val_loss: 2.6387 - val_accuracy: 0.5529 - val_auc: 0.5508\n",
      "Epoch 359/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1283 - accuracy: 0.9450 - auc: 0.9900\n",
      "Epoch 00359: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1283 - accuracy: 0.9450 - auc: 0.9900 - val_loss: 2.7405 - val_accuracy: 0.5317 - val_auc: 0.5204\n",
      "Epoch 360/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1236 - accuracy: 0.9490 - auc: 0.9907\n",
      "Epoch 00360: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1236 - accuracy: 0.9490 - auc: 0.9907 - val_loss: 2.8835 - val_accuracy: 0.5262 - val_auc: 0.5105\n",
      "Epoch 361/400\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.1156 - accuracy: 0.9525 - auc: 0.9917\n",
      "Epoch 00361: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.1144 - accuracy: 0.9526 - auc: 0.9919 - val_loss: 2.8576 - val_accuracy: 0.5152 - val_auc: 0.5040\n",
      "Epoch 362/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.1155 - accuracy: 0.9499 - auc: 0.9919\n",
      "Epoch 00362: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1149 - accuracy: 0.9501 - auc: 0.9919 - val_loss: 2.9312 - val_accuracy: 0.5373 - val_auc: 0.5074\n",
      "Epoch 363/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1126 - accuracy: 0.9537 - auc: 0.9920\n",
      "Epoch 00363: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.1126 - accuracy: 0.9537 - auc: 0.9920 - val_loss: 2.4271 - val_accuracy: 0.5363 - val_auc: 0.5420\n",
      "Epoch 364/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.1123 - accuracy: 0.9540 - auc: 0.9922\n",
      "Epoch 00364: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1136 - accuracy: 0.9526 - auc: 0.9921 - val_loss: 2.6929 - val_accuracy: 0.5501 - val_auc: 0.5273\n",
      "Epoch 365/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1057 - accuracy: 0.9566 - auc: 0.9931\n",
      "Epoch 00365: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1057 - accuracy: 0.9566 - auc: 0.9931 - val_loss: 2.6246 - val_accuracy: 0.5363 - val_auc: 0.5256\n",
      "Epoch 366/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1047 - accuracy: 0.9564 - auc: 0.9932\n",
      "Epoch 00366: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1047 - accuracy: 0.9564 - auc: 0.9932 - val_loss: 2.7209 - val_accuracy: 0.5455 - val_auc: 0.5191\n",
      "Epoch 367/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980 - accuracy: 0.9595 - auc: 0.9941\n",
      "Epoch 00367: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0980 - accuracy: 0.9595 - auc: 0.9941 - val_loss: 2.6562 - val_accuracy: 0.5290 - val_auc: 0.5187\n",
      "Epoch 368/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0923 - accuracy: 0.9601 - auc: 0.9948\n",
      "Epoch 00368: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0923 - accuracy: 0.9601 - auc: 0.9948 - val_loss: 2.9355 - val_accuracy: 0.5179 - val_auc: 0.5013\n",
      "Epoch 369/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0878 - accuracy: 0.9628 - auc: 0.9954\n",
      "Epoch 00369: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0878 - accuracy: 0.9628 - auc: 0.9954 - val_loss: 2.9577 - val_accuracy: 0.5317 - val_auc: 0.5105\n",
      "Epoch 370/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0822 - accuracy: 0.9643 - auc: 0.9961\n",
      "Epoch 00370: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0826 - accuracy: 0.9635 - auc: 0.9961 - val_loss: 2.7454 - val_accuracy: 0.5391 - val_auc: 0.5200\n",
      "Epoch 371/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0828 - accuracy: 0.9648 - auc: 0.9959\n",
      "Epoch 00371: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0828 - accuracy: 0.9648 - auc: 0.9959 - val_loss: 2.8962 - val_accuracy: 0.5382 - val_auc: 0.5259\n",
      "Epoch 372/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0828 - accuracy: 0.9653 - auc: 0.9959\n",
      "Epoch 00372: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0828 - accuracy: 0.9653 - auc: 0.9959 - val_loss: 2.8889 - val_accuracy: 0.5235 - val_auc: 0.5126\n",
      "Epoch 373/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0798 - accuracy: 0.9661 - auc: 0.9963\n",
      "Epoch 00373: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0798 - accuracy: 0.9661 - auc: 0.9963 - val_loss: 2.7760 - val_accuracy: 0.5575 - val_auc: 0.5306\n",
      "Epoch 374/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0763 - accuracy: 0.9666 - auc: 0.9966\n",
      "Epoch 00374: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0763 - accuracy: 0.9666 - auc: 0.9966 - val_loss: 3.0112 - val_accuracy: 0.5382 - val_auc: 0.5169\n",
      "Epoch 375/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0740 - accuracy: 0.9694 - auc: 0.9969\n",
      "Epoch 00375: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0740 - accuracy: 0.9694 - auc: 0.9969 - val_loss: 2.9437 - val_accuracy: 0.5161 - val_auc: 0.5104\n",
      "Epoch 376/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0743 - accuracy: 0.9681 - auc: 0.9968\n",
      "Epoch 00376: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0743 - accuracy: 0.9681 - auc: 0.9968 - val_loss: 2.9500 - val_accuracy: 0.5363 - val_auc: 0.5254\n",
      "Epoch 377/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0776 - accuracy: 0.9665 - auc: 0.9965\n",
      "Epoch 00377: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0776 - accuracy: 0.9665 - auc: 0.9965 - val_loss: 3.0664 - val_accuracy: 0.5244 - val_auc: 0.5061\n",
      "Epoch 378/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0744 - accuracy: 0.9678 - auc: 0.9968\n",
      "Epoch 00378: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0744 - accuracy: 0.9678 - auc: 0.9968 - val_loss: 3.0226 - val_accuracy: 0.5465 - val_auc: 0.5203\n",
      "Epoch 379/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0726 - accuracy: 0.9682 - auc: 0.9970\n",
      "Epoch 00379: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0726 - accuracy: 0.9682 - auc: 0.9970 - val_loss: 2.9130 - val_accuracy: 0.5262 - val_auc: 0.5220\n",
      "Epoch 380/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0758 - accuracy: 0.9677 - auc: 0.9966\n",
      "Epoch 00380: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0758 - accuracy: 0.9677 - auc: 0.9966 - val_loss: 3.1131 - val_accuracy: 0.5437 - val_auc: 0.5191\n",
      "Epoch 381/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0865 - accuracy: 0.9641 - auc: 0.9954\n",
      "Epoch 00381: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0883 - accuracy: 0.9629 - auc: 0.9952 - val_loss: 3.1664 - val_accuracy: 0.5244 - val_auc: 0.5113\n",
      "Epoch 382/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0939 - accuracy: 0.9585 - auc: 0.9944\n",
      "Epoch 00382: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.0939 - accuracy: 0.9585 - auc: 0.9944 - val_loss: 3.0409 - val_accuracy: 0.5271 - val_auc: 0.5109\n",
      "Epoch 383/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0885 - accuracy: 0.9622 - auc: 0.9951\n",
      "Epoch 00383: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0885 - accuracy: 0.9622 - auc: 0.9951 - val_loss: 3.0814 - val_accuracy: 0.5299 - val_auc: 0.5188\n",
      "Epoch 384/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0877 - accuracy: 0.9629 - auc: 0.9952\n",
      "Epoch 00384: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0877 - accuracy: 0.9629 - auc: 0.9952 - val_loss: 2.8835 - val_accuracy: 0.5253 - val_auc: 0.5195\n",
      "Epoch 385/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0918 - accuracy: 0.9600 - auc: 0.9948\n",
      "Epoch 00385: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0917 - accuracy: 0.9602 - auc: 0.9948 - val_loss: 2.9201 - val_accuracy: 0.5483 - val_auc: 0.5227\n",
      "Epoch 386/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0981 - accuracy: 0.9593 - auc: 0.9940\n",
      "Epoch 00386: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0963 - accuracy: 0.9597 - auc: 0.9943 - val_loss: 2.9806 - val_accuracy: 0.5373 - val_auc: 0.5073\n",
      "Epoch 387/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967 - accuracy: 0.9591 - auc: 0.9942\n",
      "Epoch 00387: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0967 - accuracy: 0.9591 - auc: 0.9942 - val_loss: 3.0808 - val_accuracy: 0.5281 - val_auc: 0.4980\n",
      "Epoch 388/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0877 - accuracy: 0.9622 - auc: 0.9953\n",
      "Epoch 00388: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0877 - accuracy: 0.9622 - auc: 0.9953 - val_loss: 2.9240 - val_accuracy: 0.5189 - val_auc: 0.5134\n",
      "Epoch 389/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0970 - accuracy: 0.9577 - auc: 0.9940\n",
      "Epoch 00389: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.0980 - accuracy: 0.9569 - auc: 0.9939 - val_loss: 3.0373 - val_accuracy: 0.5078 - val_auc: 0.5048\n",
      "Epoch 390/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0943 - accuracy: 0.9595 - auc: 0.9945\n",
      "Epoch 00390: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0970 - accuracy: 0.9587 - auc: 0.9942 - val_loss: 2.7894 - val_accuracy: 0.5317 - val_auc: 0.5361\n",
      "Epoch 391/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0973 - accuracy: 0.9581 - auc: 0.9942\n",
      "Epoch 00391: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.1004 - accuracy: 0.9565 - auc: 0.9938 - val_loss: 2.8091 - val_accuracy: 0.5409 - val_auc: 0.5409\n",
      "Epoch 392/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.1006 - accuracy: 0.9564 - auc: 0.9938\n",
      "Epoch 00392: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.0991 - accuracy: 0.9573 - auc: 0.9940 - val_loss: 2.8724 - val_accuracy: 0.5409 - val_auc: 0.5176\n",
      "Epoch 393/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1112 - accuracy: 0.9527 - auc: 0.9919\n",
      "Epoch 00393: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.1112 - accuracy: 0.9527 - auc: 0.9919 - val_loss: 2.8667 - val_accuracy: 0.5584 - val_auc: 0.5353\n",
      "Epoch 394/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1026 - accuracy: 0.9595 - auc: 0.9929\n",
      "Epoch 00394: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.1026 - accuracy: 0.9595 - auc: 0.9929 - val_loss: 2.8701 - val_accuracy: 0.5373 - val_auc: 0.5197\n",
      "Epoch 395/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0991 - accuracy: 0.9584 - auc: 0.9939\n",
      "Epoch 00395: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0990 - accuracy: 0.9583 - auc: 0.9939 - val_loss: 3.0480 - val_accuracy: 0.5216 - val_auc: 0.5006\n",
      "Epoch 396/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0937 - accuracy: 0.9597 - auc: 0.9947\n",
      "Epoch 00396: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0937 - accuracy: 0.9597 - auc: 0.9947 - val_loss: 3.1134 - val_accuracy: 0.5244 - val_auc: 0.5095\n",
      "Epoch 397/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0897 - accuracy: 0.9607 - auc: 0.9953\n",
      "Epoch 00397: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0897 - accuracy: 0.9607 - auc: 0.9953 - val_loss: 2.7154 - val_accuracy: 0.5557 - val_auc: 0.5386\n",
      "Epoch 398/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0794 - accuracy: 0.9659 - auc: 0.9964\n",
      "Epoch 00398: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0794 - accuracy: 0.9659 - auc: 0.9964 - val_loss: 2.8630 - val_accuracy: 0.5639 - val_auc: 0.5388\n",
      "Epoch 399/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0756 - accuracy: 0.9676 - auc: 0.9967\n",
      "Epoch 00399: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0756 - accuracy: 0.9676 - auc: 0.9967 - val_loss: 2.9250 - val_accuracy: 0.5391 - val_auc: 0.5346\n",
      "Epoch 400/400\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0740 - accuracy: 0.9687 - auc: 0.9968\n",
      "Epoch 00400: val_accuracy did not improve from 0.58694\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0740 - accuracy: 0.9687 - auc: 0.9968 - val_loss: 2.9826 - val_accuracy: 0.5474 - val_auc: 0.5285\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2.4866 - accuracy: 0.4738 - auc_9: 0.4971\n",
      "{'loss': 2.486553192138672, 'accuracy': 0.4738371968269348, 'auc_9': 0.49712100625038147}\n"
     ]
    }
   ],
   "source": [
    "data_dict = research.get_split_lstm_data(train_data, ma_window=7, seq_len=128, split_percents=split_percents, fully_divisible_batch_sizes=True,\n",
    "                                          normalization_groups=normalization_groups, pc_cols=pc_cols, ma_cols=ma_cols, min_batch_size=1000, max_batch_size=2000,\n",
    "                                          buy_sell_labels_df=train_data_labels['first_decision'], apply_pct_change=True)\n",
    "\n",
    "x_train, y_train = data_dict['train_data_np']\n",
    "x_val, y_val = data_dict['val_data_np']\n",
    "x_test, y_test = data_dict['test_data_np']\n",
    "\n",
    "binary_model = create_model_binary(seq_len=x_train.shape[1], num_features=x_train.shape[2])\n",
    "    \n",
    "filepath = f'../my_stuff/{cur_pair}-{timeframe}_{min_profit_percent}-min_profit_{lots_per_trade}-lots_{currency_side}-cur_side' \\\n",
    "            f'_{tenkan_period}-{kijun_period}-{senkou_b_period}-{sigs_for_filename}-ichi_cnn-lstm_classifier.hdf5'\n",
    "callback = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "                                    \n",
    "binary_model.fit(convert_to_tensor(x_train), convert_to_tensor(y_train),\n",
    "                  batch_size=data_dict['eval_batch_size'],\n",
    "                  callbacks=[callback],\n",
    "                  epochs=num_epochs,\n",
    "                  validation_data=(convert_to_tensor(x_val), convert_to_tensor(y_val)))\n",
    "\n",
    "binary_model = tf.keras.models.load_model(filepath)\n",
    "\n",
    "eval_results = binary_model.evaluate(convert_to_tensor(x_test), convert_to_tensor(y_test), return_dict=True)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### try using model for close price forcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fast_ma_window = fast_ma_data['ma_window']\n",
    "filepath = f'../my_stuff/{cur_pair}-{timeframe}_Bi-LSTM_{fast_ma_window}-ma_{tenkan_period}-{kijun_period}-{senkou_b_period}-ichi.hdf5'\n",
    "callback = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "\n",
    "fast_ma_model = create_model(seq_len=x_train_fast_ma.shape[1], num_features=x_train_fast_ma.shape[2])\n",
    "# print(fast_ma_model.summary())\n",
    "\n",
    "start_t = time.time()\n",
    "\n",
    "fast_ma_model.fit(convert_to_tensor(x_train_fast_ma), convert_to_tensor(y_train_fast_ma),\n",
    "                  batch_size=fast_ma_data['eval_batch_size'],\n",
    "                  callbacks=[callback],\n",
    "                  epochs=num_epochs,\n",
    "                  validation_data=(convert_to_tensor(x_val_fast_ma), convert_to_tensor(y_val_fast_ma)))\n",
    "\n",
    "print(f'training time = {(time.time()-start_t)/60} min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slow_ma_window = slow_ma_data['ma_window']\n",
    "filepath = f'../my_stuff/{cur_pair}-{timeframe}_Bi-LSTM_{slow_ma_window}-ma_{tenkan_period}-{kijun_period}-{senkou_b_period}-ichi.hdf5'\n",
    "callback = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "\n",
    "slow_ma_model = create_model(seq_len=x_train_slow_ma.shape[1], num_features=x_train_slow_ma.shape[2])\n",
    "\n",
    "start_t = time.time()\n",
    "\n",
    "slow_ma_model.fit(convert_to_tensor(x_train_slow_ma), convert_to_tensor(y_train_slow_ma),\n",
    "                  batch_size=slow_ma_data['eval_batch_size'],\n",
    "                  callbacks=[callback],\n",
    "                  epochs=num_epochs,\n",
    "                  #shuffle=True,\n",
    "                  validation_data=(convert_to_tensor(x_val_slow_ma), convert_to_tensor(y_val_slow_ma)))\n",
    "\n",
    "print(f'training time = {(time.time()-start_t)/60} min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_ma_model = tf.keras.models.load_model('../my_stuff/final_Bi-LSTM_fast_5_ma.hdf5')#('../my_stuff/Bi-LSTM_mov_avg_5.hdf5')\n",
    "\n",
    "print('done loading fast ma model')\n",
    "\n",
    "# #Calculate predication for training, validation and test data\n",
    "# train_pred_fast_ma = fast_ma_model.predict(convert_to_tensor(x_train_fast_ma))\n",
    "# val_pred_fast_ma = fast_ma_model.predict(convert_to_tensor(x_val_fast_ma))\n",
    "# test_pred_fast_ma = fast_ma_model.predict(convert_to_tensor(x_test_fast_ma))\n",
    "\n",
    "#Print evaluation metrics for all datasets\n",
    "train_eval_fast_ma = fast_ma_model.evaluate(convert_to_tensor(x_train_fast_ma), convert_to_tensor(y_train_fast_ma), verbose=0)\n",
    "val_eval_fast_ma = fast_ma_model.evaluate(convert_to_tensor(x_val_fast_ma), convert_to_tensor(y_val_fast_ma), verbose=0)\n",
    "test_eval_fast_ma = fast_ma_model.evaluate(convert_to_tensor(x_test_fast_ma), convert_to_tensor(y_test_fast_ma), verbose=0)\n",
    "\n",
    "print('Evaluation metrics')\n",
    "print('Training Data - Loss: {:.4f}, MAE: {:.4f}, MAPE: {:.4f}'.format(train_eval_fast_ma[0], train_eval_fast_ma[1], train_eval_fast_ma[2]))\n",
    "print('Validation Data - Loss: {:.4f}, MAE: {:.4f}, MAPE: {:.4f}'.format(val_eval_fast_ma[0], val_eval_fast_ma[1], val_eval_fast_ma[2]))\n",
    "print('Test Data - Loss: {:.4f}, MAE: {:.4f}, MAPE: {:.4f}'.format(test_eval_fast_ma[0], test_eval_fast_ma[1], test_eval_fast_ma[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slow_ma_model = tf.keras.models.load_model('../my_stuff/final_Bi-LSTM_slow_13_ma.hdf5')#('../my_stuff/Bi-LSTM_mov_avg_13.hdf5')\n",
    "\n",
    "print('done loading slow ma model')\n",
    "\n",
    "#Calculate predication for training, validation and test data\n",
    "train_pred_slow_ma = slow_ma_model.predict(convert_to_tensor(x_train_slow_ma))\n",
    "val_pred_slow_ma = slow_ma_model.predict(convert_to_tensor(x_val_slow_ma))\n",
    "test_pred_slow_ma = slow_ma_model.predict(convert_to_tensor(x_test_slow_ma))\n",
    "\n",
    "#Print evaluation metrics for all datasets\n",
    "train_eval_slow_ma = slow_ma_model.evaluate(convert_to_tensor(x_train_slow_ma), convert_to_tensor(y_train_slow_ma), verbose=0)\n",
    "val_eval_slow_ma = slow_ma_model.evaluate(convert_to_tensor(x_val_slow_ma), convert_to_tensor(y_val_slow_ma), verbose=0)\n",
    "test_eval_slow_ma = slow_ma_model.evaluate(convert_to_tensor(x_test_slow_ma), convert_to_tensor(y_test_slow_ma), verbose=0)\n",
    "\n",
    "print('Evaluation metrics')\n",
    "print('Training Data - Loss: {:.4f}, MAE: {:.4f}, MAPE: {:.4f}'.format(train_eval_slow_ma[0], train_eval_slow_ma[1], train_eval_slow_ma[2]))\n",
    "print('Validation Data - Loss: {:.4f}, MAE: {:.4f}, MAPE: {:.4f}'.format(val_eval_slow_ma[0], val_eval_slow_ma[1], val_eval_slow_ma[2]))\n",
    "print('Test Data - Loss: {:.4f}, MAE: {:.4f}, MAPE: {:.4f}'.format(test_eval_slow_ma[0], test_eval_slow_ma[1], test_eval_slow_ma[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_fast_ma = fast_ma_data['test_data_df'].to_numpy()\n",
    "\n",
    "test_data_slow_ma = slow_ma_data['test_data_df'].to_numpy()\n",
    "\n",
    "fig = plt.figure(figsize=(15,15))\n",
    "st = fig.suptitle(\"CNN + Bi-LSTM Model\", fontsize=22)\n",
    "st.set_y(1.02)\n",
    "\n",
    "# #Plot training data results\n",
    "# ax11 = fig.add_subplot(311)\n",
    "# ax11.plot(train_data[seq_len:, 3], label='EURUSD Closing Returns')\n",
    "# ax11.plot(train_pred, color='yellow', linewidth=3, label='Predicted EURUSD Closing Returns')\n",
    "# ax11.set_title(\"Training Data\", fontsize=18)\n",
    "# ax11.set_xlabel('Date')\n",
    "# ax11.set_ylabel('EURUSD Closing Returns')\n",
    "\n",
    "# #Plot validation data results\n",
    "# ax21 = fig.add_subplot(312)\n",
    "# ax21.plot(val_data[seq_len:, 3], label='EURUSD Closing Returns')\n",
    "# ax21.plot(val_pred, color='yellow', linewidth=3, label='Predicted EURUSD Closing Returns')\n",
    "# ax21.set_title(\"Validation Data\", fontsize=18)\n",
    "# ax21.set_xlabel('Date')\n",
    "# ax21.set_ylabel('EURUSD Closing Returns')\n",
    "\n",
    "#Plot test data results\n",
    "ax31 = fig.add_subplot(111)\n",
    "ax31.plot(test_data_fast_ma[seq_len:, 3], label='EURUSD closing mov avg 5')\n",
    "ax31.plot(test_pred_fast_ma, linewidth=3, label='Predicted EURUSD closing mov avg 5')\n",
    "ax31.plot(test_data_slow_ma[seq_len:, 3], label='EURUSD closing mov avg 13')\n",
    "ax31.plot(test_pred_slow_ma, linewidth=3, label='Predicted EURUSD closing mov avg 13')\n",
    "ax31.plot(test_data_orig[:, 3], label='Original EURUSD Closing Returns')\n",
    "ax31.set_title(\"Test Data\", fontsize=18)\n",
    "ax31.set_xlabel('Date')\n",
    "ax31.set_ylabel('EURUSD Closing Returns')\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "plt.tight_layout()\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train models for backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fast_ma_window = fast_ma_data['ma_window']\n",
    "filepath = f'../my_stuff/final_{cur_pair}-{timeframe}_Bi-LSTM_{fast_ma_window}-ma_{tenkan_period}-{kijun_period}-{senkou_b_period}-ichi.hdf5'\n",
    "callback = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='loss', save_best_only=True, verbose=1)\n",
    "\n",
    "all_x_train_fast_ma, all_y_train_fast_ma = fast_ma_data['all_train_data_np']\n",
    "fast_ma_model = create_model(seq_len=all_x_train_fast_ma.shape[1], num_features=all_x_train_fast_ma.shape[2])\n",
    "\n",
    "start_t = time.time()\n",
    "\n",
    "fast_ma_model.fit(conc8vert_to_tensor(all_x_train_fast_ma), convert_to_tensor(all_y_train_fast_ma),\n",
    "                  batch_size=fast_ma_data['final_batch_size'],\n",
    "                  callbacks=[callback],\n",
    "                  epochs=num_epochs)\n",
    "\n",
    "print(f'training time = {(time.time()-start_t)/60} min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "slow_ma_window = slow_ma_data['ma_window']\n",
    "filepath = f'../my_stuff/final_{cur_pair}-{timeframe}_Bi-LSTM_{slow_ma_window}-ma_{tenkan_period}-{kijun_period}-{senkou_b_period}-ichi.hdf5'\n",
    "callback = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='loss', save_best_only=True, verbose=1)\n",
    "\n",
    "all_x_train_slow_ma, all_y_train_slow_ma = slow_ma_data['all_train_data_np']\n",
    "slow_ma_model = create_model(seq_len=all_x_train_slow_ma.shape[1], num_features=all_x_train_slow_ma.shape[2])\n",
    "\n",
    "start_t = time.time()\n",
    "\n",
    "slow_ma_model.fit(convert_to_tensor(all_x_train_slow_ma), convert_to_tensor(all_y_train_slow_ma),\n",
    "                  batch_size=slow_ma_data['final_batch_size'],\n",
    "                  callbacks=[callback],\n",
    "                  epochs=num_epochs)\n",
    "\n",
    "print(f'training time = {(time.time()-start_t)/60} min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# backtest models (xgboost for opening and CNN+Bi-LSTM for closing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### global hyperparameters for backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all parameters SHOULD match what the models were trained on for best results (so far this assumption is consistant)\n",
    "\n",
    "# independant params\n",
    "min_profit_percent, profit_noise_percent = 0.0016, 0.0016\n",
    "contract_size = 100_000   # size of 1 lot is typically 100,000 (100 for gold, becuase 1 lot = 100 oz of gold)\n",
    "lots_per_trade = 0.2  \n",
    "starting_balance = 1000\n",
    "leverage = 500    # 1:leverage\n",
    "max_concurrent_trades = np.inf # 5\n",
    "currency_side = 'right'\n",
    "in_quote_currency = True if currency_side == 'right' else False\n",
    "pip_resolution = 0.0001\n",
    "stop_out_pct = 0.2  # explaination: https://www.tradersway.com/new_to_the_market/forex_and_cfd_basics#margin\n",
    "fast_ma_diff_thresh = 0.01  #.01\n",
    "# slow_ma_diff_thresh = 0.05   #0.02  \n",
    "decision_prob_diff_thresh = 0.5   # 0.5 accepts all probabilities\n",
    "tenkan_period = 9\n",
    "kijun_period = 30\n",
    "senkou_b_period = 60\n",
    "label_non_signals=False\n",
    "hedged_margin = 50_000\n",
    "tradersway_commodity = False\n",
    "cur_pair = 'EURUSD'\n",
    "timeframe ='H1'\n",
    "\n",
    "# dependant params (don't edit)\n",
    "pip_value = contract_size * lots_per_trade * pip_resolution   # in quote currency (right side currency of currency pair)\n",
    "min_profit = min_profit_percent * lots_per_trade * contract_size   # in base currecy because thats what models were traied on\n",
    "profit_noise = profit_noise_percent * lots_per_trade * contract_size   # in base currecy because thats what models were traied on\n",
    "\n",
    "indicators_info = {\n",
    "    'ichimoku': {\n",
    "        'tenkan_period': tenkan_period,\n",
    "        'kijun_period': kijun_period,\n",
    "        'chikou_period': kijun_period,\n",
    "        'senkou_b_period': senkou_b_period\n",
    "    },\n",
    "    'rsi': {\n",
    "        'periods': 14\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prepare CNN+Bi-LSTM models and preprocessing vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 60306 rows of tick data from C:\\GitHub Repos\\ForexMachine\\ForexMachine\\PackageData\\TicksData\\mt5_EURUSD_h1_ticks_2011-01-01T00;00UTC_to_2020-10-01T00;00UTC.csv\n",
      "------------------------------------------------------\n",
      "data w/ moving average window of 7 info:\n",
      "\n",
      "batch size for evaluation: 0\n",
      "training data size reduction for evaulation: 0 -> 0\n",
      "batch size for final training: 1174\n",
      "training data size reduction for final training: 59876 -> 59874\n",
      "\n",
      "training data shape: x=(0,), y=(0,)\n",
      "validation data shape: x=(0,), y=(0,)\n",
      "test data shape: x=(59876, 128, 30), y=(59876,)\n",
      "all train data shape: x=(59874, 128, 30), y=(59874,)\n",
      "------------------------------------------------------\n",
      "fast MA window: 7\n",
      "sequence length for LSTMs: 128\n"
     ]
    }
   ],
   "source": [
    "reload(research)\n",
    "\n",
    "fast_ma_model = tf.keras.models.load_model('../my_stuff/final_EURUSD-H1_Bi-LSTM_7-ma_9-30-60-ichi.hdf5')\n",
    "# slow_ma_model = tf.keras.models.load_model('../my_stuff/final_EURUSD-H1_Bi-LSTM_7-ma_9-30-60-ichi.hdf5')\n",
    "\n",
    "fast_ma_window = 7\n",
    "# slow_ma_window = 7\n",
    "lstm_seq_len = 128\n",
    "\n",
    "# lstm_decision_predictor = tf.keras.models.load_model('../my_stuff/EURUSD-H1_0.01-min_profit_0.2-lots_right-cur_side'\n",
    "#                                                      '_9-30-60-cb-tk-tkp-sen-chi-ichi_cnn-lstm_classifier.hdf5')\n",
    "\n",
    "\n",
    "tick_data_filepath = research.download_mt5_data(cur_pair, timeframe, global_train_data_range_start, global_train_data_range_end)\n",
    "data_with_indicators = research.add_indicators_to_raw(filepath=tick_data_filepath,\n",
    "                                                      indicators_info=indicators_info, \n",
    "                                                      datetime_col='datetime')\n",
    "train_data = research.add_ichimoku_features(data_with_indicators)\n",
    "\n",
    "# train_data_labels = generate_ichimoku_labels(train_data, label_non_signals=label_non_signals, min_profit_percent=min_profit_percent, \n",
    "#                                              profit_noise_percent=profit_noise_percent, signals_to_consider=signals_to_consider, \n",
    "#                                              contract_size=contract_size, lots_per_trade=lots_per_trade,\n",
    "#                                              in_quote_currency=in_quote_currency,pip_resolution=pip_resolution)\n",
    "\n",
    "start_idx, end_idx = research.no_missing_data_idx_range(train_data, early_ending_cols=['chikou_span_visual'])\n",
    "train_data = train_data.iloc[start_idx:end_idx+1]\n",
    "train_data = research.dummy_and_remove_features(train_data)\n",
    "# train_data_labels = train_data_labels.iloc[start_idx:end_idx+1]\n",
    "\n",
    "ma_cols = ['Open','High','Low','Close','Volume']\n",
    "pc_cols = ['Open','High','Low','Close','Volume',\n",
    "           'trend_ichimoku_base','trend_ichimoku_conv',\n",
    "           'trend_ichimoku_a', 'trend_ichimoku_b']\n",
    "normalization_groups = [['Open','High','Low','Close'],  # prices\n",
    "                        ['trend_ichimoku_base','trend_ichimoku_conv'],  # ichi conv & base lines\n",
    "                        ['trend_ichimoku_a', 'trend_ichimoku_b'], # ichi cloud lines\n",
    "                        ['tk_cross_bull_strength','tk_cross_bear_strength',   # tk cross strength\n",
    "                        'tk_price_cross_bull_strength','tk_price_cross_bear_strength',   # tk price cross strength\n",
    "                        'senkou_cross_bull_strength','senkou_cross_bear_strength',   # semkou cross strength\n",
    "                        'chikou_cross_bull_strength','chikou_cross_bear_strength']]   # chikou cross strength\n",
    "\n",
    "fast_ma_data = research.get_split_lstm_data(train_data, ma_window=fast_ma_window, seq_len=lstm_seq_len, split_percents=(0,0), fully_divisible_batch_sizes=True,\n",
    "                                 normalization_groups=normalization_groups, pc_cols=pc_cols, ma_cols=ma_cols, min_batch_size=1000, max_batch_size=2000)\n",
    "fast_ma_norm_terms = fast_ma_data['all_train_normalization_terms']\n",
    "\n",
    "# lstm_data = research.get_split_lstm_data(train_data, ma_window=None, seq_len=lstm_seq_len, split_percents=(0,0), fully_divisible_batch_sizes=True,\n",
    "#                               normalization_groups=normalization_groups, pc_cols=pc_cols, ma_cols=ma_cols, min_batch_size=1000, max_batch_size=2000,\n",
    "#                               buy_sell_labels_df=train_data_labels['first_decision'], apply_pct_change=True)\n",
    "# lstm_norm_terms = lstm_data['sub_train_normalization_terms']\n",
    "\n",
    "print(f'fast MA window: {fast_ma_window}')\n",
    "# print(f'slow MA window: {slow_ma_window}')\n",
    "print(f'sequence length for LSTMs: {lstm_seq_len}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prepare xgboost models and preprocessing vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels dict for XGB classifier:\n",
      "\t{1: 'buy', 0: 'sell'}\n",
      "signals to consider for opening trades:\n",
      "\tcloud_breakout_bull\n",
      "\tcloud_breakout_bear\n",
      "\ttk_cross_bull_strength\n",
      "\ttk_cross_bear_strength\n",
      "\ttk_price_cross_bull_strength\n",
      "\ttk_price_cross_bear_strength\n",
      "\tsenkou_cross_bull_strength\n",
      "\tsenkou_cross_bear_strength\n",
      "\tchikou_cross_bull_strength\n",
      "\tchikou_cross_bear_strength\n"
     ]
    }
   ],
   "source": [
    "xgb_decision_predictor = xgb.Booster()\n",
    "xgb_decision_predictor.load_model('../my_stuff/EURUSD-H1_0.01-min_profit_0.2-lots_right-cur_side_9-30-60-cb-tk-tkp-sen-chi-ichi_xgb_classifier.json')\n",
    "xgb_labels_dict = {1: 'buy', 0: 'sell'}\n",
    "open_trade_sigs = ['cloud_breakout_bull','cloud_breakout_bear',                       # cloud breakout\n",
    "                   'tk_cross_bull_strength', 'tk_cross_bear_strength',                # Tenkan Sen / Kijun Sen Cross\n",
    "                   'tk_price_cross_bull_strength', 'tk_price_cross_bear_strength',    # price crossing both the Tenkan Sen / Kijun Sen\n",
    "                   'senkou_cross_bull_strength', 'senkou_cross_bear_strength',        # Senkou Span Cross\n",
    "                   'chikou_cross_bull_strength', 'chikou_cross_bear_strength']        # Chikou Span Cross\n",
    "\n",
    "print(f'labels dict for XGB classifier:\\n\\t{xgb_labels_dict}')\n",
    "print('signals to consider for opening trades:')\n",
    "for sig in open_trade_sigs:\n",
    "    print(f'\\t{sig}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prepare data for backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 1547 rows of tick data from C:\\GitHub Repos\\ForexMachine\\ForexMachine\\PackageData\\TicksData\\mt5_EURUSD_h1_ticks_2020-10-02T00;00UTC_to_2021-01-05T00;00UTC.csv\n"
     ]
    }
   ],
   "source": [
    "tick_data_filepath = research.download_mt5_data(\"EURUSD\", 'H1', '2020-10-02', '2021-01-05')\n",
    "# tick_data_filepath = research.download_mt5_data(\"EURUSD\", 'H1', '2020-11-02', '2021-01-05')\n",
    "data_with_indicators = research.add_indicators_to_raw(filepath=tick_data_filepath, \n",
    "                                                      indicators_info=indicators_info, \n",
    "                                                      datetime_col='datetime')\n",
    "\n",
    "test_data_with_ichi_sigs = research.add_ichimoku_features(data_with_indicators)\n",
    "model_data = research.dummy_and_remove_features(test_data_with_ichi_sigs)\n",
    "\n",
    "start, stop = research.no_missing_data_idx_range(model_data, early_ending_cols=['chikou_span_visual'])\n",
    "\n",
    "model_data = model_data.iloc[start:stop+1]\n",
    "model_data_np = model_data.to_numpy()\n",
    "\n",
    "test_data_with_ichi_sigs = test_data_with_ichi_sigs.iloc[start:stop+1]\n",
    "test_data_np = test_data_with_ichi_sigs.to_numpy()\n",
    "\n",
    "ma_cols_set = set([model_data.columns.get_loc(col_name) for col_name in ma_cols])\n",
    "pc_cols_set = set([model_data.columns.get_loc(col_name) for col_name in pc_cols])\n",
    "\n",
    "feature_indices = {test_data_with_ichi_sigs.columns[i]: i for i in range(len(test_data_with_ichi_sigs.columns))}\n",
    "\n",
    "test_data_labels = research.generate_ichimoku_labels(test_data_with_ichi_sigs, label_non_signals=label_non_signals, min_profit_percent=min_profit_percent, \n",
    "                                                    profit_noise_percent=profit_noise_percent, signals_to_consider=open_trade_sigs, \n",
    "                                                    contract_size=contract_size, lots_per_trade=lots_per_trade,\n",
    "                                                    in_quote_currency=in_quote_currency, pip_resolution=pip_resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Open', 'High', 'Low', 'Close', 'Volume', 'trend_ichimoku_conv',\n",
      "       'trend_ichimoku_base', 'trend_ichimoku_a', 'trend_ichimoku_b',\n",
      "       'is_price_above_cb_lines', 'is_price_above_cloud',\n",
      "       'is_price_inside_cloud', 'is_price_below_cloud', 'cloud_breakout_bull',\n",
      "       'cloud_breakout_bear', 'tk_cross_bull_strength',\n",
      "       'tk_cross_bear_strength', 'tk_price_cross_bull_strength',\n",
      "       'tk_price_cross_bear_strength', 'senkou_cross_bull_strength',\n",
      "       'senkou_cross_bear_strength', 'chikou_cross_bull_strength',\n",
      "       'chikou_cross_bear_strength', 'quarter_2', 'quarter_3', 'quarter_4',\n",
      "       'day_of_week_1', 'day_of_week_2', 'day_of_week_3', 'day_of_week_4'],\n",
      "      dtype='object') 30\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>trend_ichimoku_conv</th>\n",
       "      <th>trend_ichimoku_base</th>\n",
       "      <th>trend_ichimoku_a</th>\n",
       "      <th>trend_ichimoku_b</th>\n",
       "      <th>is_price_above_cb_lines</th>\n",
       "      <th>...</th>\n",
       "      <th>senkou_cross_bear_strength</th>\n",
       "      <th>chikou_cross_bull_strength</th>\n",
       "      <th>chikou_cross_bear_strength</th>\n",
       "      <th>quarter_2</th>\n",
       "      <th>quarter_3</th>\n",
       "      <th>quarter_4</th>\n",
       "      <th>day_of_week_1</th>\n",
       "      <th>day_of_week_2</th>\n",
       "      <th>day_of_week_3</th>\n",
       "      <th>day_of_week_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>1.17545</td>\n",
       "      <td>1.17598</td>\n",
       "      <td>1.17515</td>\n",
       "      <td>1.17583</td>\n",
       "      <td>2075</td>\n",
       "      <td>1.175085</td>\n",
       "      <td>1.175730</td>\n",
       "      <td>1.175407</td>\n",
       "      <td>1.176635</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>1.17583</td>\n",
       "      <td>1.17595</td>\n",
       "      <td>1.17546</td>\n",
       "      <td>1.17575</td>\n",
       "      <td>1440</td>\n",
       "      <td>1.175085</td>\n",
       "      <td>1.175720</td>\n",
       "      <td>1.175403</td>\n",
       "      <td>1.176635</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>1.17575</td>\n",
       "      <td>1.17605</td>\n",
       "      <td>1.17537</td>\n",
       "      <td>1.17591</td>\n",
       "      <td>1298</td>\n",
       "      <td>1.175085</td>\n",
       "      <td>1.175720</td>\n",
       "      <td>1.175403</td>\n",
       "      <td>1.176635</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>1.17591</td>\n",
       "      <td>1.17615</td>\n",
       "      <td>1.17566</td>\n",
       "      <td>1.17582</td>\n",
       "      <td>1131</td>\n",
       "      <td>1.175085</td>\n",
       "      <td>1.175720</td>\n",
       "      <td>1.175403</td>\n",
       "      <td>1.176635</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>1.17582</td>\n",
       "      <td>1.17602</td>\n",
       "      <td>1.17517</td>\n",
       "      <td>1.17571</td>\n",
       "      <td>938</td>\n",
       "      <td>1.174715</td>\n",
       "      <td>1.175720</td>\n",
       "      <td>1.175218</td>\n",
       "      <td>1.176635</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1542</th>\n",
       "      <td>1.22529</td>\n",
       "      <td>1.22565</td>\n",
       "      <td>1.22425</td>\n",
       "      <td>1.22509</td>\n",
       "      <td>3526</td>\n",
       "      <td>1.227575</td>\n",
       "      <td>1.225905</td>\n",
       "      <td>1.226740</td>\n",
       "      <td>1.225945</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543</th>\n",
       "      <td>1.22509</td>\n",
       "      <td>1.22523</td>\n",
       "      <td>1.22439</td>\n",
       "      <td>1.22521</td>\n",
       "      <td>2837</td>\n",
       "      <td>1.227575</td>\n",
       "      <td>1.225905</td>\n",
       "      <td>1.226740</td>\n",
       "      <td>1.225945</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544</th>\n",
       "      <td>1.22521</td>\n",
       "      <td>1.22521</td>\n",
       "      <td>1.22414</td>\n",
       "      <td>1.22492</td>\n",
       "      <td>2972</td>\n",
       "      <td>1.227520</td>\n",
       "      <td>1.225905</td>\n",
       "      <td>1.226713</td>\n",
       "      <td>1.225945</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1545</th>\n",
       "      <td>1.22492</td>\n",
       "      <td>1.22523</td>\n",
       "      <td>1.22417</td>\n",
       "      <td>1.22452</td>\n",
       "      <td>1774</td>\n",
       "      <td>1.227520</td>\n",
       "      <td>1.225905</td>\n",
       "      <td>1.226713</td>\n",
       "      <td>1.225945</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546</th>\n",
       "      <td>1.22452</td>\n",
       "      <td>1.22512</td>\n",
       "      <td>1.22428</td>\n",
       "      <td>1.22501</td>\n",
       "      <td>405</td>\n",
       "      <td>1.227520</td>\n",
       "      <td>1.225905</td>\n",
       "      <td>1.226713</td>\n",
       "      <td>1.225945</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1432 rows  30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Open     High      Low    Close  Volume  trend_ichimoku_conv  \\\n",
       "115   1.17545  1.17598  1.17515  1.17583    2075             1.175085   \n",
       "116   1.17583  1.17595  1.17546  1.17575    1440             1.175085   \n",
       "117   1.17575  1.17605  1.17537  1.17591    1298             1.175085   \n",
       "118   1.17591  1.17615  1.17566  1.17582    1131             1.175085   \n",
       "119   1.17582  1.17602  1.17517  1.17571     938             1.174715   \n",
       "...       ...      ...      ...      ...     ...                  ...   \n",
       "1542  1.22529  1.22565  1.22425  1.22509    3526             1.227575   \n",
       "1543  1.22509  1.22523  1.22439  1.22521    2837             1.227575   \n",
       "1544  1.22521  1.22521  1.22414  1.22492    2972             1.227520   \n",
       "1545  1.22492  1.22523  1.22417  1.22452    1774             1.227520   \n",
       "1546  1.22452  1.22512  1.22428  1.22501     405             1.227520   \n",
       "\n",
       "      trend_ichimoku_base  trend_ichimoku_a  trend_ichimoku_b  \\\n",
       "115              1.175730          1.175407          1.176635   \n",
       "116              1.175720          1.175403          1.176635   \n",
       "117              1.175720          1.175403          1.176635   \n",
       "118              1.175720          1.175403          1.176635   \n",
       "119              1.175720          1.175218          1.176635   \n",
       "...                   ...               ...               ...   \n",
       "1542             1.225905          1.226740          1.225945   \n",
       "1543             1.225905          1.226740          1.225945   \n",
       "1544             1.225905          1.226713          1.225945   \n",
       "1545             1.225905          1.226713          1.225945   \n",
       "1546             1.225905          1.226713          1.225945   \n",
       "\n",
       "     is_price_above_cb_lines  ... senkou_cross_bear_strength  \\\n",
       "115                     True  ...                        0.0   \n",
       "116                     True  ...                        0.0   \n",
       "117                     True  ...                        0.0   \n",
       "118                     True  ...                        0.0   \n",
       "119                    False  ...                        0.0   \n",
       "...                      ...  ...                        ...   \n",
       "1542                   False  ...                        0.0   \n",
       "1543                   False  ...                        0.0   \n",
       "1544                   False  ...                        0.0   \n",
       "1545                   False  ...                        0.0   \n",
       "1546                   False  ...                        0.0   \n",
       "\n",
       "     chikou_cross_bull_strength chikou_cross_bear_strength  quarter_2  \\\n",
       "115                         0.0                        0.0        0.0   \n",
       "116                         0.0                        0.0        0.0   \n",
       "117                         0.0                        0.0        0.0   \n",
       "118                         0.0                        0.0        0.0   \n",
       "119                         0.0                        0.0        0.0   \n",
       "...                         ...                        ...        ...   \n",
       "1542                        0.0                        0.0        0.0   \n",
       "1543                        0.0                        0.0        0.0   \n",
       "1544                        0.0                        0.0        0.0   \n",
       "1545                        0.0                        0.0        0.0   \n",
       "1546                        1.0                        0.0        0.0   \n",
       "\n",
       "      quarter_3  quarter_4  day_of_week_1  day_of_week_2  day_of_week_3  \\\n",
       "115         0.0        1.0            0.0            0.0            1.0   \n",
       "116         0.0        1.0            0.0            0.0            1.0   \n",
       "117         0.0        1.0            0.0            0.0            1.0   \n",
       "118         0.0        1.0            0.0            0.0            1.0   \n",
       "119         0.0        1.0            0.0            0.0            0.0   \n",
       "...         ...        ...            ...            ...            ...   \n",
       "1542        0.0        0.0            0.0            0.0            0.0   \n",
       "1543        0.0        0.0            0.0            0.0            0.0   \n",
       "1544        0.0        0.0            0.0            0.0            0.0   \n",
       "1545        0.0        0.0            0.0            0.0            0.0   \n",
       "1546        0.0        0.0            1.0            0.0            0.0   \n",
       "\n",
       "      day_of_week_4  \n",
       "115             0.0  \n",
       "116             0.0  \n",
       "117             0.0  \n",
       "118             0.0  \n",
       "119             1.0  \n",
       "...             ...  \n",
       "1542            0.0  \n",
       "1543            0.0  \n",
       "1544            0.0  \n",
       "1545            0.0  \n",
       "1546            0.0  \n",
       "\n",
       "[1432 rows x 30 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model_data.columns, len(model_data.columns))\n",
    "model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### analyze test data to develop trading strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot move avg cnn+lstm preds vs price data\n",
    "\n",
    "fast_ma_preds = fast_ma_model.predict(convert_to_tensor(fast_ma_data['all_train_data_np'][0]))\n",
    "fast_ma_preds = np.reshape(fast_ma_preds,(fast_ma_preds.shape[0],))\n",
    "fast_ma_preds = fast_ma_preds.tolist()\n",
    "fill = [None]*(len(test_data_with_ichi_sigs) - len(fast_ma_preds))\n",
    "fill.extend(fast_ma_preds)\n",
    "fast_ma_preds = fill\n",
    "\n",
    "# slow_ma_preds = slow_ma_model.predict(convert_to_tensor(slow_ma_data['all_train_data_np'][0]))\n",
    "# slow_ma_preds = np.reshape(slow_ma_preds,(slow_ma_preds.shape[0],))\n",
    "# slow_ma_preds = slow_ma_preds.tolist()\n",
    "# fill = [None]*(len(test_data_with_ichi_sigs) - len(slow_ma_preds))\n",
    "# fill.extend(slow_ma_preds)\n",
    "# slow_ma_preds = fill\n",
    "\n",
    "lstm_preds = pd.DataFrame({\n",
    "    'fast_ma':fast_ma_preds, \n",
    "#     'slow_ma':slow_ma_preds\n",
    "})\n",
    "\n",
    "# import random\n",
    "# test_data = [random.random() for i in range(len(test_data_with_ichi_sigs))]\n",
    "# test_data2 = [random.random() for i in range(len(test_data_with_ichi_sigs))]\n",
    "# test_data = {'testing1': test_data,\n",
    "#              'testing2': test_data2}\n",
    "# test_data=pd.DataFrame(test_data)\n",
    "# show_data_from_range(test_data_with_ichi_sigs, '2020-10-12', '2020-10-16', \n",
    "#                      main_indicator='ichimoku', sub_indicators=[test_data,'rsi'], visualize_crosses=True,\n",
    "#                      visualize_labels=True, labels_df=test_data_labels)\n",
    "\n",
    "labels = ['first_decision','ticks_till_best_profit_first_decision', 'best_profit_first_decision', 'profit_peak_first_decision',\n",
    "          'second_decision','ticks_till_best_profit_second_decision', 'best_profit_second_decision', 'profit_peak_second_decision']\n",
    "show_data_from_range(test_data_with_ichi_sigs, '2020-11-12', '2020-12-17', \n",
    "                     main_indicator='ichimoku', sub_indicators=[lstm_preds], visualize_crosses=True,\n",
    "                     visualize_labels=True, labels_df=test_data_labels, labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### backtest strat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model buffers full, beginning trade sim...\n",
      "backtest percentage done: 10%\n",
      "backtest percentage done: 20%\n",
      "backtest percentage done: 30%\n",
      "backtest percentage done: 40%\n",
      "backtest percentage done: 50%\n",
      "backtest percentage done: 60%\n",
      "backtest percentage done: 70%\n",
      "backtest percentage done: 80%\n",
      "backtest percentage done: 90%\n",
      "backtest percentage done: 100%\n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "BACKTEST RESULTS:\n",
      "ticks data duration: 80 days\n",
      "starting balance: 1000\n",
      "ending balance: 4767.800000000004\n",
      "number of trades won: 154\n",
      "number of trades lost: 118\n",
      "number of buys: 233 (134 won, 99 lost)\n",
      "number of sells: 39 (20 won, 19 lost)\n",
      "balance range: [731.6000000000001, 4767.800000000004]\n",
      "equity range: [852.6000000000022, 4766.6000000000195]\n",
      "free margin range: [735.4819333333355, 4692.996066666686]\n",
      "margins range: [23.4434, 414.4994088888889]\n",
      "margin levels range: [403.5431048341795, 18983.078524627046]\n",
      "concurrently open trades range: [0, 9]\n",
      "concurrently losing trades range: [0, 6]\n",
      "backtest runtime: 0.840446138381958 min\n",
      "\n",
      "WON TRADES RESULTS:\n",
      "fast_ma_diff_at_sig that aggreed: count=69, min=0.00012356042861938477, max=0.0504816472530365, mean=0.012701357714831829, median=0.011491477489471436\n",
      "fast_ma_diff_at_sig that opposed: count=85, min=0.00038570165634155273, max=0.07036197185516357, mean=0.014942696318030357, median=0.0122891366481781\n",
      "fast_ma_diff_at_close: count=154, min=0.01000368595123291, max=0.043580591678619385, mean=0.019648874178528786, median=0.016134142875671387\n",
      "slow_ma_diff_at_close: count=154, min=0, max=0, mean=0.0, median=0.0\n",
      "fast_ma_diff_at_best_sign_to_close: count=127, min=4.1544437408447266e-05, max=0.03649979829788208, mean=0.011282931081950665, median=0.009528934955596924\n",
      "\n",
      "LOST TRADES RESULTS:\n",
      "fast_ma_diff_at_sig that aggreed: count=67, min=0.0011864900588989258, max=0.04998880624771118, mean=0.013081256300210953, median=0.010770022869110107\n",
      "fast_ma_diff_at_sig that opposed: count=51, min=0.00040662288665771484, max=0.04537758231163025, mean=0.013018745929002762, median=0.009776443243026733\n",
      "fast_ma_diff_at_close: count=118, min=0.010013997554779053, max=0.07036197185516357, mean=0.021359773352742195, median=0.01878562569618225\n",
      "slow_ma_diff_at_close: count=118, min=0, max=0, mean=0.0, median=0.0\n",
      "fast_ma_diff_at_best_sign_to_close: count=82, min=0.00040662288665771484, max=0.07036197185516357, mean=0.012242540717124939, median=0.009389117360115051\n",
      "\n",
      "MODELS STATS:\n",
      "average pred time of fast MA CNN+LSTM models: 38.2662780112221 ms\n",
      "average pred time of XGB model: 2.179379010722585 ms\n"
     ]
    }
   ],
   "source": [
    "trades = {}\n",
    "backtest_trades = {}   # closed trades results\n",
    "pending_order = None\n",
    "pending_close = None\n",
    "decisions_so_far = []\n",
    "fast_ma_seq_buf = deque()\n",
    "slow_ma_seq_buf = deque()\n",
    "fast_ma_window_buf = deque()\n",
    "slow_ma_window_buf = deque()\n",
    "fast_ma_avgs = []\n",
    "slow_ma_avgs = []\n",
    "fast_ma_perc_chngs = []\n",
    "slow_ma_perc_chngs = []\n",
    "xgb_model_perc_chngs = []\n",
    "fast_ma_preds = []\n",
    "slow_ma_preds = []\n",
    "cnn_lstm_pred_times = []\n",
    "xgb_pred_times = []\n",
    "free_margins = []\n",
    "margins = []\n",
    "margin_levels = []\n",
    "equities = []\n",
    "balances = []\n",
    "open_trades_counts = []\n",
    "losing_trades_counts = []\n",
    "pct_done = 0\n",
    "buffers_rdy_idx = None\n",
    "balance = starting_balance\n",
    "equity = starting_balance\n",
    "free_margin = starting_balance\n",
    "losing_trades = 0\n",
    "margin_level = None\n",
    "margin = None\n",
    "final_dt = None\n",
    "stop = False\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(len(test_data_np)):\n",
    "    \"\"\"\n",
    "    fill data buffers for models\n",
    "    \"\"\"\n",
    "\n",
    "    # for xgb model\n",
    "\n",
    "    if i > 0:\n",
    "        row = research.apply_perc_change_list(model_data_np[i-1], model_data_np[i], cols_set=pc_cols_set)\n",
    "        xgb_model_perc_chngs.append(row)\n",
    "\n",
    "    # for fast MA model\n",
    "\n",
    "    fast_ma_window_buf.append(model_data_np[i])\n",
    "    if len(fast_ma_window_buf) > fast_ma_window:\n",
    "        fast_ma_window_buf.popleft()\n",
    "\n",
    "    if len(fast_ma_window_buf) == fast_ma_window:\n",
    "        row = research.apply_moving_avg_q(fast_ma_window_buf, ma_cols_set)\n",
    "        fast_ma_avgs.append(row)\n",
    "\n",
    "    if len(fast_ma_avgs) >= 2:\n",
    "        row = research.apply_perc_change_list(fast_ma_avgs[-2], fast_ma_avgs[-1], pc_cols_set)\n",
    "        row = research.normalize_data_list(row, fast_ma_norm_terms)\n",
    "        fast_ma_perc_chngs.append(row) \n",
    "\n",
    "    if len(fast_ma_perc_chngs) > 0:\n",
    "        fast_ma_seq_buf.append(fast_ma_perc_chngs[-1])\n",
    "\n",
    "    if len(fast_ma_seq_buf) > lstm_seq_len:\n",
    "        fast_ma_seq_buf.popleft()\n",
    "\n",
    "    # for slow MA model\n",
    "\n",
    "#     slow_ma_window_buf.append(model_data_np[i])\n",
    "#     if len(slow_ma_window_buf) > slow_ma_window:\n",
    "#         slow_ma_window_buf.popleft()\n",
    "\n",
    "#     if len(slow_ma_window_buf) == slow_ma_window:\n",
    "#         row = apply_moving_avg_q(slow_ma_window_buf, ma_cols_set)\n",
    "#         slow_ma_avgs.append(row)\n",
    "\n",
    "#     if len(slow_ma_avgs) >= 2:\n",
    "#         row = apply_perc_change_list(slow_ma_avgs[-2], slow_ma_avgs[-1], pc_cols_set)\n",
    "#         row = normalize_data_list(row, slow_ma_norm_terms)\n",
    "#         slow_ma_perc_chngs.append(row)  \n",
    "\n",
    "#     if len(slow_ma_perc_chngs) > 0:\n",
    "#         slow_ma_seq_buf.append(slow_ma_perc_chngs[-1])\n",
    "\n",
    "#     if len(slow_ma_seq_buf) > lstm_seq_len:\n",
    "#         slow_ma_seq_buf.popleft()\n",
    "\n",
    "    # now check if LSTMs have enough data to being trade simulation\n",
    "\n",
    "#     if len(fast_ma_seq_buf) == lstm_seq_len and len(slow_ma_seq_buf) == lstm_seq_len:\n",
    "    if len(fast_ma_seq_buf) == lstm_seq_len:\n",
    "        \"\"\"\n",
    "        simulate trading\n",
    "        \"\"\"\n",
    "\n",
    "        if buffers_rdy_idx is None:\n",
    "            buffers_rdy_idx = i\n",
    "            print('model buffers full, beginning trade sim...')\n",
    "\n",
    "        # look for ichiomku signals\n",
    "        causes = []\n",
    "        for sig in open_trade_sigs:\n",
    "            sig_i = feature_indices[sig]\n",
    "            if test_data_np[i][sig_i] != 0:\n",
    "                causes.append(sig)\n",
    "\n",
    "        start = time.time()\n",
    "        fast_ma_pred = fast_ma_model.predict(np.array([fast_ma_seq_buf]))\n",
    "        slow_ma_pred = [[0]] #slow_ma_model.predict(np.array([slow_ma_seq_buf]))\n",
    "        duration = time.time() - start\n",
    "        cnn_lstm_pred_times.append(duration)\n",
    "\n",
    "        fast_ma_preds.append(fast_ma_pred[0][0])\n",
    "        slow_ma_preds.append(slow_ma_pred[0][0])\n",
    "\n",
    "        if len(fast_ma_preds) > 1:\n",
    "            fast_ma_diff = fast_ma_preds[-1] - fast_ma_preds[-2]    # remember this is the diff in the pct_change of the mov avg\n",
    "            slow_ma_diff = slow_ma_preds[-1] - slow_ma_preds[-2]\n",
    "        else:\n",
    "            fast_ma_diff = 0\n",
    "            slow_ma_diff = 0\n",
    "\n",
    "        if pending_order is not None:\n",
    "            pending_order_i, decision_label, decision_prob, order_causes, sig_fast_ma_diff, sig_slow_ma_diff = pending_order\n",
    "            open_price = test_data_np[i][feature_indices['Open']]\n",
    "            decision_prob_diff = abs(decision_label-decision_prob)\n",
    "\n",
    "            trades[pending_order_i] = {\n",
    "                'decision_label': decision_label,\n",
    "                'decision_prob': decision_prob,\n",
    "                'causes': order_causes,\n",
    "                'open_price': open_price,\n",
    "                'trade_open_tick_i': i,\n",
    "                'profit': None,\n",
    "                'best_profit': None,\n",
    "                'ticks_till_close': None,\n",
    "                'close_idx': None,\n",
    "                'lots': lots_per_trade,\n",
    "                'look_to_close': False,\n",
    "                'forced_close': False,\n",
    "                'fast_ma_diff_at_sig': sig_fast_ma_diff,\n",
    "                'slow_ma_diff_at_sig': sig_slow_ma_diff,\n",
    "                'fast_ma_diff_at_close': None,\n",
    "                'slow_ma_diff_at_close': None,\n",
    "                'fast_ma_diff_at_best_sign_to_close': None,\n",
    "                'slow_ma_diff_at_best_sign_to_close': None\n",
    "            }\n",
    "\n",
    "            required_margin = research.get_margin(trades, buy_label=1, sell_label=0, contract_size=contract_size, leverage=leverage, \n",
    "                                                  tradersway_commodity=tradersway_commodity, in_quote_currency=in_quote_currency, hedged_margin=hedged_margin)\n",
    "\n",
    "            # reference on opening trades and margin level https://www.luckscout.com/leverage-margin-balance-equity-free-margin-and-margin-level-in-forex-trading/\n",
    "            if required_margin > free_margin or (margin_level is not None and margin_level <= 100) \\\n",
    "                    or len(trades) > max_concurrent_trades or decision_prob_diff > decision_prob_diff_thresh:\n",
    "                del trades[pending_order_i]\n",
    "            else:\n",
    "                margin = required_margin\n",
    "\n",
    "            pending_order = None\n",
    "\n",
    "        # update equity and free margin based on currently opened trades\n",
    "        for trade_i in trades:\n",
    "            trade = trades[trade_i]\n",
    "            close_price = test_data_np[i][feature_indices['Close']]\n",
    "            trade_decision = xgb_labels_dict[trade['decision_label']]\n",
    "\n",
    "            profit = research.get_profit(close_price, trade['open_price'], pip_value=pip_value, \n",
    "                                         pip_resolution=pip_resolution, in_quote_currency=in_quote_currency)\n",
    "            if trade_decision == 'sell':\n",
    "                profit *= - 1\n",
    "\n",
    "            if trade['profit'] is None:\n",
    "                profit_delta = profit\n",
    "            else:\n",
    "                profit_delta = profit - trade['profit']\n",
    "            trade['profit'] = profit\n",
    "\n",
    "            if trade['best_profit'] is None or profit > trade['best_profit']:\n",
    "                trade['best_profit'] = profit\n",
    "                if (fast_ma_diff < 0 and trade_decision == 'buy') or (fast_ma_diff > 0 and trade_decision == 'sell'):\n",
    "                    trade['fast_ma_diff_at_best_sign_to_close'] = fast_ma_diff\n",
    "                if (slow_ma_diff < 0 and trade_decision == 'buy') or (slow_ma_diff > 0 and trade_decision == 'sell'):\n",
    "                    trade['slow_ma_diff_at_best_sign_to_close'] = slow_ma_diff\n",
    "\n",
    "            equity += profit_delta\n",
    "            free_margin = equity - margin \n",
    "            margin_level = equity / margin * 100\n",
    "\n",
    "            scaled_profit_noise = profit_noise if not in_quote_currency else profit_noise / close_price\n",
    "            if abs(profit) >= scaled_profit_noise:\n",
    "                trade['look_to_close'] = True\n",
    "\n",
    "        # check if equity is <= 0, and if so end the sim\n",
    "        if equity <= 0:\n",
    "            stop = True\n",
    "            print(f'strat failed (i={i}, dt={test_data_np[i][feature_indices[\"datetime\"]]}): no more equity')\n",
    "\n",
    "        # check if trades should be closed due to stop-out starting with biggest loss if so\n",
    "        if margin_level is not None and margin_level <= stop_out_pct:\n",
    "            sorted_keys = sorted(trades, key=lambda trade_i: trades[trade_i]['profit'])\n",
    "            for j, trade_i in enumerate(sorted_keys):\n",
    "                balance += trades[trade_i]['profit']\n",
    "\n",
    "                open_tick_i = trades[trade_i]['trade_open_tick_i']\n",
    "                trades[trade_i]['ticks_till_close'] = i - open_tick_i\n",
    "                trades[trade_i]['close_idx'] = i\n",
    "                trades[trade_i]['forced_close'] = True\n",
    "                trades[trade_i]['fast_ma_diff_at_close'] = fast_ma_diff\n",
    "                trades[trade_i]['slow_ma_diff_at_close'] = slow_ma_diff\n",
    "                backtest_trades[trade_i] = trades[trade_i]\n",
    "\n",
    "                del trades[trade_i]\n",
    "\n",
    "                if j != len(sorted_keys) - 1:\n",
    "                    margin = research.get_margin(trades, buy_label=1, sell_label=0, contract_size=contract_size, leverage=leverage, \n",
    "                                                 tradersway_commodity=tradersway_commodity, in_quote_currency=in_quote_currency, hedged_margin=hedged_margin)\n",
    "                    free_margin = equity - margin\n",
    "                    margin_level = equity / margin * 100                    \n",
    "                    if margin_level > stop_out_pct:\n",
    "                        break   \n",
    "\n",
    "        # find trades to close based on CNN-LSTM preds\n",
    "        closed_trades = []\n",
    "        losing_trades = 0\n",
    "        for trade_i in trades: \n",
    "            trade = trades[trade_i]\n",
    "            trade_decision = xgb_labels_dict[trade['decision_label']]\n",
    "            \n",
    "            if trade['look_to_close']:\n",
    "                if abs(fast_ma_diff) >= fast_ma_diff_thresh:\n",
    "                    # (MA pct_change is decreasing on a long trade) or (MA pct_change is increasing on a short trade)\n",
    "                    if (fast_ma_diff < 0 and trade_decision == 'buy') or (fast_ma_diff > 0 and trade_decision == 'sell'):  \n",
    "                        closed_trades.append(trade_i)\n",
    "                        continue    # continue to not count this trade in losing trades if it will be closed anyways\n",
    "            \n",
    "            if trade['profit'] < 0:\n",
    "                losing_trades += 1\n",
    "\n",
    "        for trade_i in closed_trades:\n",
    "            balance += trades[trade_i]['profit']\n",
    "\n",
    "            open_tick_i = trades[trade_i]['trade_open_tick_i']\n",
    "            trades[trade_i]['ticks_till_close'] = i - open_tick_i\n",
    "            trades[trade_i]['close_idx'] = i\n",
    "            trades[trade_i]['fast_ma_diff_at_close'] = fast_ma_diff\n",
    "            trades[trade_i]['slow_ma_diff_at_close'] = slow_ma_diff\n",
    "            backtest_trades[trade_i] = trades[trade_i]\n",
    "\n",
    "            del trades[trade_i]\n",
    "\n",
    "        if len(trades) == 0:\n",
    "            margin = None\n",
    "            margin_level = None\n",
    "\n",
    "        # generate decision w/ XGB classifier and create pending order\n",
    "        if len(causes) > 0 and not stop:\n",
    "            start = time.time()\n",
    "            model_input = pd.DataFrame([xgb_model_perc_chngs[-1]], columns=model_data.columns)\n",
    "            model_input = xgb.DMatrix(model_input)\n",
    "            decision_prob = xgb_decision_predictor.predict(model_input)[0]\n",
    "            duration = time.time() - start # inlucde converting input in pred time\n",
    "            xgb_pred_times.append(duration)\n",
    "\n",
    "            decision_label = np.around(decision_prob)\n",
    "\n",
    "#             if (decision_label == 1 and fast_ma_diff > 0) or (decision_label == 0 and fast_ma_diff < 0):\n",
    "#                 pending_order = (i, decision_label, decision_prob, causes, fast_ma_diff, slow_ma_diff)\n",
    "            pending_order = (i, decision_label, decision_prob, causes, fast_ma_diff, slow_ma_diff)\n",
    "\n",
    "        cur_pct_done = int((i-buffers_rdy_idx+1) / (len(test_data_np)-buffers_rdy_idx) * 100)\n",
    "        if cur_pct_done != pct_done and cur_pct_done % 10 == 0:\n",
    "            pct_done = cur_pct_done\n",
    "            print(f'backtest percentage done: {cur_pct_done}%')\n",
    "\n",
    "    free_margins.append(free_margin)\n",
    "    equities.append(equity)\n",
    "    balances.append(balance)\n",
    "    margins.append(margin)\n",
    "    margin_levels.append(margin_level)\n",
    "    open_trades_counts.append(len(trades))\n",
    "    losing_trades_counts.append(losing_trades)\n",
    "\n",
    "    final_dt = test_data_np[i][feature_indices[\"datetime\"]]\n",
    "    if stop:\n",
    "        break\n",
    "\n",
    "# print backtest results\n",
    "\n",
    "backtest_runtime = time.time() - start_time\n",
    "start_dt = test_data_np[buffers_rdy_idx][feature_indices['datetime']]\n",
    "end_dt = final_dt\n",
    "\n",
    "margin_levels_no_none = [ml for ml in margin_levels if ml is not None]\n",
    "max_margin_level = None if len(margin_levels_no_none) == 0 else max(margin_levels_no_none)\n",
    "min_margin_level = None if len(margin_levels_no_none) == 0 else min(margin_levels_no_none)\n",
    "margins_no_none = [m for m in margins if m is not None]\n",
    "max_margin = None if len(margins_no_none) == 0 else max(margins_no_none)\n",
    "min_margin =  None if len(margins_no_none) == 0 else min(margins_no_none)\n",
    "\n",
    "num_won = 0\n",
    "num_lost = 0\n",
    "num_won_sells = 0\n",
    "num_won_buys = 0\n",
    "num_lost_sells = 0\n",
    "num_lost_buys = 0\n",
    "ma_diff_stat_names = ['fast_ma_diff_at_sig', 'slow_ma_diff_at_sig', 'fast_ma_diff_at_close', 'slow_ma_diff_at_close',\n",
    "                      'fast_ma_diff_at_best_sign_to_close', 'slow_ma_diff_at_best_sign_to_close']\n",
    "losses_ma_diff_stats = {name: {'list': [], 'agree_list':[], 'oppose_list':[]} for name in ma_diff_stat_names}\n",
    "wins_ma_diff_stats = {name: {'list': [], 'agree_list':[], 'oppose_list':[]} for name in ma_diff_stat_names}\n",
    "for trade_i in backtest_trades:\n",
    "    trade = backtest_trades[trade_i]\n",
    "    if trade['profit'] > 0:\n",
    "        if trade['decision_label'] == 1:\n",
    "            num_won_buys += 1\n",
    "        else:\n",
    "            num_won_sells += 1\n",
    "        num_won += 1\n",
    "\n",
    "        if (trade['decision_label'] == 1 and trade['fast_ma_diff_at_sig'] > 0) or (trade['decision_label'] == 0 and trade['fast_ma_diff_at_sig'] < 0):\n",
    "            wins_ma_diff_stats['fast_ma_diff_at_sig']['agree_list'].append(abs(trade['fast_ma_diff_at_sig']))\n",
    "        elif (trade['decision_label'] == 1 and trade['fast_ma_diff_at_sig'] < 0) or (trade['decision_label'] == 0 and trade['fast_ma_diff_at_sig'] > 0):\n",
    "            wins_ma_diff_stats['fast_ma_diff_at_sig']['oppose_list'].append(abs(trade['fast_ma_diff_at_sig']))\n",
    "        if (trade['decision_label'] == 1 and trade['slow_ma_diff_at_sig'] > 0) or (trade['decision_label'] == 0 and trade['slow_ma_diff_at_sig'] < 0):\n",
    "            wins_ma_diff_stats['slow_ma_diff_at_sig']['agree_list'].append(abs(trade['slow_ma_diff_at_sig']))\n",
    "        elif (trade['decision_label'] == 1 and trade['slow_ma_diff_at_sig'] < 0) or (trade['decision_label'] == 0 and trade['slow_ma_diff_at_sig'] > 0):\n",
    "            wins_ma_diff_stats['slow_ma_diff_at_sig']['oppose_list'].append(abs(trade['slow_ma_diff_at_sig']))\n",
    "\n",
    "        wins_ma_diff_stats['fast_ma_diff_at_close']['list'].append(abs(trade['fast_ma_diff_at_close']))\n",
    "        wins_ma_diff_stats['slow_ma_diff_at_close']['list'].append(abs(trade['slow_ma_diff_at_close']))\n",
    "\n",
    "        if trade['fast_ma_diff_at_best_sign_to_close'] is not None:\n",
    "            wins_ma_diff_stats['fast_ma_diff_at_best_sign_to_close']['list'].append(abs(trade['fast_ma_diff_at_best_sign_to_close']))\n",
    "        if trade['slow_ma_diff_at_best_sign_to_close'] is not None:\n",
    "            wins_ma_diff_stats['slow_ma_diff_at_best_sign_to_close']['list'].append(abs(trade['slow_ma_diff_at_best_sign_to_close']))\n",
    "    else:\n",
    "        if trade['decision_label'] == 1:\n",
    "            num_lost_buys += 1\n",
    "        else:\n",
    "            num_lost_sells += 1\n",
    "        num_lost += 1\n",
    "\n",
    "        if (trade['decision_label'] == 1 and trade['fast_ma_diff_at_sig'] > 0) or (trade['decision_label'] == 0 and trade['fast_ma_diff_at_sig'] < 0):\n",
    "            losses_ma_diff_stats['fast_ma_diff_at_sig']['agree_list'].append(abs(trade['fast_ma_diff_at_sig']))\n",
    "        elif (trade['decision_label'] == 1 and trade['fast_ma_diff_at_sig'] < 0) or (trade['decision_label'] == 0 and trade['fast_ma_diff_at_sig'] > 0):\n",
    "            losses_ma_diff_stats['fast_ma_diff_at_sig']['oppose_list'].append(abs(trade['fast_ma_diff_at_sig']))\n",
    "        if (trade['decision_label'] == 1 and trade['slow_ma_diff_at_sig'] > 0) or (trade['decision_label'] == 0 and trade['slow_ma_diff_at_sig'] < 0):\n",
    "            losses_ma_diff_stats['slow_ma_diff_at_sig']['agree_list'].append(abs(trade['slow_ma_diff_at_sig']))\n",
    "        elif (trade['decision_label'] == 1 and trade['slow_ma_diff_at_sig'] < 0) or (trade['decision_label'] == 0 and trade['slow_ma_diff_at_sig'] > 0):\n",
    "            losses_ma_diff_stats['slow_ma_diff_at_sig']['oppose_list'].append(abs(trade['slow_ma_diff_at_sig']))\n",
    "\n",
    "        losses_ma_diff_stats['fast_ma_diff_at_close']['list'].append(abs(trade['fast_ma_diff_at_close']))\n",
    "        losses_ma_diff_stats['slow_ma_diff_at_close']['list'].append(abs(trade['slow_ma_diff_at_close']))\n",
    "\n",
    "        if trade['fast_ma_diff_at_best_sign_to_close'] is not None:\n",
    "            losses_ma_diff_stats['fast_ma_diff_at_best_sign_to_close']['list'].append(abs(trade['fast_ma_diff_at_best_sign_to_close']))\n",
    "        if trade['slow_ma_diff_at_best_sign_to_close'] is not None:\n",
    "            losses_ma_diff_stats['slow_ma_diff_at_best_sign_to_close']['list'].append(abs(trade['slow_ma_diff_at_best_sign_to_close']))\n",
    "losses_ma_diff_stats = {name: {'arr': np.array(losses_ma_diff_stats[name]['list']), \n",
    "                               'agree_arr': np.array(losses_ma_diff_stats[name]['agree_list']), \n",
    "                               'oppose_arr': np.array(losses_ma_diff_stats[name]['oppose_list'])} for name in losses_ma_diff_stats}\n",
    "wins_ma_diff_stats = {name: {'arr': np.array(wins_ma_diff_stats[name]['list']), \n",
    "                             'agree_arr': np.array(wins_ma_diff_stats[name]['agree_list']), \n",
    "                             'oppose_arr': np.array(wins_ma_diff_stats[name]['oppose_list'])} for name in wins_ma_diff_stats}\n",
    "\n",
    "print('\\n--------------------------------------------------------------------\\n')\n",
    "print('BACKTEST RESULTS:')\n",
    "print(f'ticks data duration: {(end_dt-start_dt).days} days')\n",
    "print(f'starting balance: {starting_balance}')\n",
    "print(f'ending balance: {balance}')\n",
    "print(f'number of trades won: {num_won}')\n",
    "print(f'number of trades lost: {num_lost}')\n",
    "print(f'number of buys: {num_won_buys+num_lost_buys} ({num_won_buys} won, {num_lost_buys} lost)')\n",
    "print(f'number of sells: {num_won_sells+num_lost_sells} ({num_won_sells} won, {num_lost_sells} lost)')\n",
    "print(f'balance range: [{min(balances)}, {max(balances)}]')\n",
    "print(f'equity range: [{min(equities)}, {max(equities)}]')\n",
    "print(f'free margin range: [{min(free_margins)}, {max(free_margins)}]')\n",
    "print(f'margins range: [{min_margin}, {max_margin}]')\n",
    "print(f'margin levels range: [{min_margin_level}, {max_margin_level}]')\n",
    "print(f'concurrently open trades range: [{min(open_trades_counts)}, {max(open_trades_counts)}]')\n",
    "print(f'concurrently losing trades range: [{min(losing_trades_counts)}, {max(losing_trades_counts)}]')\n",
    "print(f'backtest runtime: {backtest_runtime/60} min')\n",
    "\n",
    "print('\\nWON TRADES RESULTS:')\n",
    "for stat in wins_ma_diff_stats:\n",
    "    stat_arr = wins_ma_diff_stats[stat]['arr']\n",
    "    stat_agree_arr = wins_ma_diff_stats[stat]['agree_arr']\n",
    "    stat_oppose_arr = wins_ma_diff_stats[stat]['oppose_arr']\n",
    "    if len(stat_arr) > 0:\n",
    "        print(f'{stat}: count={len(stat_arr)}, min={np.amin(stat_arr)}, max={np.amax(stat_arr)},'\n",
    "              f' mean={np.mean(stat_arr)}, median={np.median(stat_arr)}')\n",
    "    if len(stat_agree_arr) > 0:\n",
    "        print(f'{stat} that aggreed: count={len(stat_agree_arr)}, min={np.amin(stat_agree_arr)}, max={np.amax(stat_agree_arr)},'\n",
    "              f' mean={np.mean(stat_agree_arr)}, median={np.median(stat_agree_arr)}')\n",
    "    if len(stat_oppose_arr) > 0:\n",
    "        print(f'{stat} that opposed: count={len(stat_oppose_arr)}, min={np.amin(stat_oppose_arr)}, max={np.amax(stat_oppose_arr)},'\n",
    "              f' mean={np.mean(stat_oppose_arr)}, median={np.median(stat_oppose_arr)}')\n",
    "\n",
    "print('\\nLOST TRADES RESULTS:')\n",
    "for stat in losses_ma_diff_stats:\n",
    "    stat_arr = losses_ma_diff_stats[stat]['arr']\n",
    "    stat_agree_arr = losses_ma_diff_stats[stat]['agree_arr']\n",
    "    stat_oppose_arr = losses_ma_diff_stats[stat]['oppose_arr']\n",
    "    if len(stat_arr) > 0:\n",
    "        print(f'{stat}: count={len(stat_arr)}, min={np.amin(stat_arr)}, max={np.amax(stat_arr)},'\n",
    "              f' mean={np.mean(stat_arr)}, median={np.median(stat_arr)}')\n",
    "    if len(stat_agree_arr) > 0:\n",
    "        print(f'{stat} that aggreed: count={len(stat_agree_arr)}, min={np.amin(stat_agree_arr)}, max={np.amax(stat_agree_arr)},'\n",
    "              f' mean={np.mean(stat_agree_arr)}, median={np.median(stat_agree_arr)}')\n",
    "    if len(stat_oppose_arr) > 0:\n",
    "        print(f'{stat} that opposed: count={len(stat_oppose_arr)}, min={np.amin(stat_oppose_arr)}, max={np.amax(stat_oppose_arr)},'\n",
    "              f' mean={np.mean(stat_oppose_arr)}, median={np.median(stat_oppose_arr)}')\n",
    "\n",
    "print('\\nMODELS STATS:')\n",
    "#         print(f'average pred time of fast & slow MA CNN+LSTM models: {sum(cnn_lstm_pred_times)/len(cnn_lstm_pred_times)*1000} ms')\n",
    "print(f'average pred time of fast MA CNN+LSTM models: {sum(cnn_lstm_pred_times)/len(cnn_lstm_pred_times)*1000} ms')\n",
    "print(f'average pred time of XGB model: {sum(xgb_pred_times)/len(xgb_pred_times)*1000} ms')\n",
    "\n",
    "# plot strategy over time vs. price data\n",
    "\n",
    "backtest_labels_col_names = ['decision_pred','ticks_till_best_profit_decision_pred', 'best_profit_decision_pred', 'profit_peak_decision_pred']\n",
    "backtest_labels = []\n",
    "for i in range(len(test_data_np)):\n",
    "    if i in backtest_trades:\n",
    "        trade = backtest_trades[i]\n",
    "        trade_decision = xgb_labels_dict[trade['decision_label']]\n",
    "        backtest_labels.append([trade_decision, trade['ticks_till_close'], trade['profit'], trade['close_idx']])\n",
    "    else:\n",
    "        backtest_labels.append([None]*len(backtest_labels_col_names))\n",
    "backtest_labels = pd.DataFrame(backtest_labels, columns=backtest_labels_col_names)\n",
    "backtest_labels = pd.concat((test_data_labels, backtest_labels.reset_index(drop=True)), axis=1)    \n",
    "\n",
    "fill = [None] * buffers_rdy_idx\n",
    "fill.extend(fast_ma_preds)\n",
    "fast_ma_preds = fill \n",
    "fast_ma_preds.extend([None]*(len(test_data_np) - len(fast_ma_preds)))\n",
    "\n",
    "fill = [None] * buffers_rdy_idx\n",
    "fill.extend(slow_ma_preds)\n",
    "slow_ma_preds = fill \n",
    "slow_ma_preds.extend([None]*(len(test_data_np) - len(slow_ma_preds)))\n",
    "\n",
    "lstm_preds = pd.DataFrame({\n",
    "    'fast_ma': fast_ma_preds, \n",
    "#     'slow_ma': slow_ma_preds\n",
    "})\n",
    "\n",
    "balances.extend([None]*(len(test_data_np) - len(balances)))\n",
    "equities.extend([None]*(len(test_data_np) - len(equities)))\n",
    "free_margins.extend([None]*(len(test_data_np) - len(free_margins)))\n",
    "open_trades_counts.extend([None]*(len(test_data_np) - len(open_trades_counts)))\n",
    "losing_trades_counts.extend([None]*(len(test_data_np) - len(losing_trades_counts)))\n",
    "\n",
    "strat_data_df = pd.DataFrame({\n",
    "    'balance': balances,\n",
    "    'equity': equities,\n",
    "    'free margin': free_margins\n",
    "})\n",
    "\n",
    "open_trades_counts_df = pd.DataFrame({\n",
    "    'open trades': open_trades_counts,\n",
    "    'losing trades': losing_trades_counts\n",
    "})\n",
    "\n",
    "labels = [#'first_decision','ticks_till_best_profit_first_decision', 'best_profit_first_decision', 'profit_peak_first_decision',\n",
    "          'decision_pred','ticks_till_best_profit_decision_pred', 'best_profit_decision_pred', 'profit_peak_decision_pred']\n",
    "show_data_from_range(test_data_with_ichi_sigs, start_dt.isoformat(), end_dt.isoformat(), main_indicator='ichimoku', \n",
    "                     sub_indicators=[lstm_preds, strat_data_df, open_trades_counts_df], visualize_crosses=True, visualize_labels=True, \n",
    "                     labels_df=backtest_labels, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for debugging\n",
    "\n",
    "# fast_ma_perc_chngs = pd.DataFrame(fast_ma_perc_chngs,columns=model_data.columns)\n",
    "# print(fast_ma_perc_chngs.shape)\n",
    "\n",
    "# slow_ma_perc_chngs = pd.DataFrame(slow_ma_perc_chngs,columns=model_data.columns)\n",
    "# print(slow_ma_perc_chngs.shape)  \n",
    "\n",
    "# x = apply_moving_avg(model_data, ma_cols, fast_ma_window)\n",
    "# x.dropna(how='any', axis=0, inplace=True)\n",
    "# x = apply_perc_change(x, pc_cols)\n",
    "# x.dropna(how='any', axis=0, inplace=True)\n",
    "# x = normalize_data(x, train_data=False, normalization_terms=fast_ma_norm_terms)[0]\n",
    "# x_vals = x.to_numpy().astype(np.float32)\n",
    "\n",
    "# print(x.shape)\n",
    "# res = np.isclose(x_vals, fast_ma_perc_chngs.to_numpy().astype(np.float32))\n",
    "# print(res)\n",
    "# print(np.all(res))\n",
    "\n",
    "# print()\n",
    "\n",
    "# x = apply_moving_avg(model_data, ma_cols, slow_ma_window)\n",
    "# x.dropna(how='any', axis=0, inplace=True)\n",
    "# x = apply_perc_change(x, pc_cols)\n",
    "# x.dropna(how='any', axis=0, inplace=True)\n",
    "# x = normalize_data(x, train_data=False, normalization_terms=slow_ma_norm_terms)[0]\n",
    "# x_vals = x.to_numpy().astype(np.float32)\n",
    "\n",
    "# print(x.shape)\n",
    "# res = np.isclose(x_vals, slow_ma_perc_chngs.to_numpy().astype(np.float32))\n",
    "# print(res)\n",
    "# print(np.all(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tune strat hyperparams with grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of parameters to test in grid search: 1\n"
     ]
    }
   ],
   "source": [
    "fast_ma_diff_thresholds = list(np.linspace(0,0.01,num=15))\n",
    "fast_ma_diff_thresholds.append(0.011)\n",
    "decision_prob_diff_thresholds = list(np.arange(0.35,0.5,step=0.05))\n",
    "profit_noise_percents = list(np.linspace(0,0.002,num=15))\n",
    "\n",
    "param_grid = {\n",
    "    'ma_models_settings': [\n",
    "        {\n",
    "            'fast_ma_model_path': '../my_stuff/final_EURUSD-H1_Bi-LSTM_7-ma_9-30-60-ichi.hdf5',\n",
    "#             'slow_ma_model_path': '../my_stuff/final_EURUSD-H1_Bi-LSTM_13-ma_8-22-44-ichi.hdf5',\n",
    "            'fast_ma_window': 7,\n",
    "#             'slow_ma_window': 13\n",
    "        },\n",
    "    ],\n",
    "    'xgb_model_settings': [\n",
    "        {\n",
    "            'model_filepath': '../my_stuff/EURUSD-H1_0.01-min_profit_0.2-lots_right-cur_side_9-30-60-cb-tk-tkp-sen-chi-ichi_xgb_classifier.json',\n",
    "            'lots_per_trade': 0.2,\n",
    "            'ichi_settings': (9, 30, 60),\n",
    "            'currency_side': 'right'\n",
    "        },\n",
    "    ],\n",
    "    'strat_params': [{\n",
    "        'starting_balance': [1000],\n",
    "        'leverage': [500],\n",
    "        'max_concurrent_trades': [np.inf, 10, 5],\n",
    "        'fast_ma_diff_threshold': fast_ma_diff_thresholds, \n",
    "        'decision_prob_diff_thresh': decision_prob_diff_thresholds,\n",
    "        'profit_noise_percent': profit_noise_percents\n",
    "    }]\n",
    "}\n",
    "\n",
    "param_grid = {\n",
    "    'ma_models_settings': [\n",
    "        {\n",
    "            'fast_ma_model_path': '../my_stuff/final_EURUSD-H1_Bi-LSTM_7-ma_9-30-60-ichi.hdf5',\n",
    "#             'slow_ma_model_path': '../my_stuff/final_EURUSD-H1_Bi-LSTM_13-ma_8-22-44-ichi.hdf5',\n",
    "            'fast_ma_window': 7,\n",
    "#             'slow_ma_window': 13\n",
    "        },\n",
    "    ],\n",
    "    'xgb_model_settings': [\n",
    "        {\n",
    "            'model_filepath': '../my_stuff/EURUSD-H1_0.01-min_profit_0.2-lots_right-cur_side_9-30-60-cb-tk-tkp-sen-chi-ichi_xgb_classifier.json',\n",
    "            'lots_per_trade': 0.2,\n",
    "            'ichi_settings': (9, 30, 60),\n",
    "            'currency_side': 'right'\n",
    "        },\n",
    "    ],\n",
    "    'strat_params': [{\n",
    "        'starting_balance': [1000],\n",
    "        'leverage': [500],\n",
    "        'max_concurrent_trades': [np.inf],\n",
    "        'fast_ma_diff_threshold': [0.01], \n",
    "        'decision_prob_diff_thresh': [0.5],\n",
    "        'profit_noise_percent': [0.0016]\n",
    "    }]\n",
    "}\n",
    "\n",
    "lstm_seq_len = 128\n",
    "xgb_labels_dict = {1: 'buy', 0: 'sell'}\n",
    "contract_size = 100_000   # size of 1 lot is typically 100,000 (100 for gold, becuase 1 lot = 100 oz of gold)\n",
    "pip_resolution = 0.0001\n",
    "stop_out_pct = 0.2  # explaination: https://www.tradersway.com/new_to_the_market/forex_and_cfd_basics#margin\n",
    "label_non_signals=False\n",
    "cur_pair = 'EURUSD'\n",
    "timeframe = 'H1'\n",
    "hedged_margin = 50_000\n",
    "tradersway_commodity = False\n",
    "\n",
    "open_trade_sigs = ['cloud_breakout_bull','cloud_breakout_bear',                       # cloud breakout\n",
    "                   'tk_cross_bull_strength', 'tk_cross_bear_strength',                # Tenkan Sen / Kijun Sen Cross\n",
    "                   'tk_price_cross_bull_strength', 'tk_price_cross_bear_strength',    # price crossing both the Tenkan Sen / Kijun Sen\n",
    "                   'senkou_cross_bull_strength', 'senkou_cross_bear_strength',        # Senkou Span Cross\n",
    "                   'chikou_cross_bull_strength', 'chikou_cross_bear_strength']        # Chikou Span Cross\n",
    "ma_cols = ['Open','High','Low','Close','Volume']\n",
    "pc_cols = ['Open','High','Low','Close','Volume',\n",
    "           'trend_ichimoku_base','trend_ichimoku_conv',\n",
    "           'trend_ichimoku_a', 'trend_ichimoku_b']\n",
    "normalization_groups = [['Open','High','Low','Close'],  # prices\n",
    "                        ['trend_ichimoku_base','trend_ichimoku_conv'],  # ichi conv & base lines\n",
    "                        ['trend_ichimoku_a', 'trend_ichimoku_b'], # ichi cloud lines\n",
    "                        ['tk_cross_bull_strength','tk_cross_bear_strength',   # tk cross strength\n",
    "                        'tk_price_cross_bull_strength','tk_price_cross_bear_strength',   # tk price cross strength\n",
    "                        'senkou_cross_bull_strength','senkou_cross_bear_strength',   # semkou cross strength\n",
    "                        'chikou_cross_bull_strength','chikou_cross_bear_strength']]   # chikou cross strength\n",
    "\n",
    "\n",
    "param_grid = ParameterGrid(param_grid)\n",
    "param_grid = random.sample(list(param_grid), len(param_grid))\n",
    "\n",
    "strat_params_len = len(ParameterGrid(param_grid[0]['strat_params']))\n",
    "total_params = len(param_grid) * strat_params_len\n",
    "print(f'total number of parameters to test in grid search: {total_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 1048 rows of tick data from C:\\GitHub Repos\\ForexMachine\\ForexMachine\\PackageData\\TicksData\\mt5_EURUSD_h1_ticks_2020-11-02T00;00UTC_to_2021-01-05T00;00UTC.csv\n",
      "model buffers full, beginning trade sim...\n",
      "backtest percentage done: 10%\n",
      "backtest percentage done: 20%\n",
      "backtest percentage done: 30%\n",
      "backtest percentage done: 40%\n",
      "backtest percentage done: 50%\n",
      "backtest percentage done: 60%\n",
      "backtest percentage done: 70%\n",
      "backtest percentage done: 80%\n",
      "backtest percentage done: 90%\n",
      "backtest percentage done: 100%\n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "BACKTEST RESULTS:\n",
      "ticks data duration: 47 days\n",
      "starting balance: 1000\n",
      "ending balance: 3214.3999999999983\n",
      "number of trades won: 107\n",
      "number of trades lost: 73\n",
      "number of buys: 157 (95 won, 62 lost)\n",
      "number of sells: 23 (12 won, 11 lost)\n",
      "balance range: [507.20000000001335, 3214.3999999999983]\n",
      "equity range: [507.20000000001335, 3213.1999999999994]\n",
      "free margin range: [349.6160380952551, 3139.596066666666]\n",
      "margins range: [24.227700000000002, 414.4994088888889]\n",
      "margin levels range: [343.37238133084355, 12641.50558265804]\n",
      "concurrently open trades range: [0, 9]\n",
      "concurrently losing trades range: [0, 6]\n",
      "backtest runtime: 0.512619427839915 min\n",
      "\n",
      "WON TRADES RESULTS:\n",
      "fast_ma_diff_at_sig that aggreed: count=44, min=0.00012356042861938477, max=0.0504816472530365, mean=0.01414154376834631, median=0.012051984667778015\n",
      "fast_ma_diff_at_sig that opposed: count=63, min=0.0004711747169494629, max=0.07036197185516357, mean=0.015826143324375153, median=0.013386398553848267\n",
      "fast_ma_diff_at_close: count=107, min=0.01000368595123291, max=0.0411984920501709, mean=0.020036082714796066, median=0.01799321174621582\n",
      "slow_ma_diff_at_close: count=107, min=0, max=0, mean=0.0, median=0.0\n",
      "fast_ma_diff_at_best_sign_to_close: count=92, min=4.1544437408447266e-05, max=0.03649979829788208, mean=0.012016554363071918, median=0.010157406330108643\n",
      "\n",
      "LOST TRADES RESULTS:\n",
      "fast_ma_diff_at_sig that aggreed: count=42, min=0.0011864900588989258, max=0.04998880624771118, mean=0.013804851099848747, median=0.011185497045516968\n",
      "fast_ma_diff_at_sig that opposed: count=31, min=0.0005922913551330566, max=0.03842133283615112, mean=0.013697518967092037, median=0.009898185729980469\n",
      "fast_ma_diff_at_close: count=73, min=0.010165005922317505, max=0.07036197185516357, mean=0.023902688175439835, median=0.019818663597106934\n",
      "slow_ma_diff_at_close: count=73, min=0, max=0, mean=0.0, median=0.0\n",
      "fast_ma_diff_at_best_sign_to_close: count=50, min=0.000624924898147583, max=0.07036197185516357, mean=0.013964162208139896, median=0.009898185729980469\n",
      "\n",
      "MODELS STATS:\n",
      "average pred time of fast MA CNN+LSTM models: 40.45412530860747 ms\n",
      "average pred time of XGB model: 2.4521023362547485 ms\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1/1 strat params tested, runtime of last params: 0.5134837746620178 min\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "last backtest results:\n",
      "{'tenkan_period': 9, 'kijun_period': 30, 'chikou_period': 30, 'senkou_b_period': 60, 'fast_ma_model_path': '../my_stuff/final_EURUSD-H1_Bi-LSTM_7-ma_9-30-60-ichi.hdf5', 'fast_ma_window': 7, 'xgb_model_path': '../my_stuff/EURUSD-H1_0.01-min_profit_0.2-lots_right-cur_side_9-30-60-cb-tk-tkp-sen-chi-ichi_xgb_classifier.json', 'lots_per_trade': 0.2, 'profit_noise_percent': 0.0016, 'stop_out_pct': 0.2, 'starting_balance': 1000, 'leverage': 500, 'max_concurrent_trades': inf, 'currency_side': 'right', 'fast_ma_diff_thresh': 0.01, 'decision_prob_diff_thresh': 0.5, 'ending_balance': 3214.3999999999983, 'max_balance': 3214.3999999999983, 'min_balance': 507.20000000001335, 'max_equity': 3213.1999999999994, 'min_equity': 507.20000000001335, 'max_free_margin': 3139.596066666666, 'min_free_margin': 349.6160380952551, 'max_margin': 414.4994088888889, 'min_margin': 24.227700000000002, 'max_margin_level': 12641.50558265804, 'min_margin_level': 343.37238133084355, 'max_concurrently_open_trades': 9, 'min_concurrently_open_trades': 0, 'num_won_trades': 107, 'num_lost_trades': 73, 'num_buys': 157, 'num_won_buys': 95, 'num_lost_buys': 62, 'num_sells': 23, 'num_won_sells': 12, 'num_lost_sells': 11}\n",
      "\n",
      "best backtest results:\n",
      "{'tenkan_period': 9, 'kijun_period': 30, 'chikou_period': 30, 'senkou_b_period': 60, 'fast_ma_model_path': '../my_stuff/final_EURUSD-H1_Bi-LSTM_7-ma_9-30-60-ichi.hdf5', 'fast_ma_window': 7, 'xgb_model_path': '../my_stuff/EURUSD-H1_0.01-min_profit_0.2-lots_right-cur_side_9-30-60-cb-tk-tkp-sen-chi-ichi_xgb_classifier.json', 'lots_per_trade': 0.2, 'profit_noise_percent': 0.0016, 'stop_out_pct': 0.2, 'starting_balance': 1000, 'leverage': 500, 'max_concurrent_trades': inf, 'currency_side': 'right', 'fast_ma_diff_thresh': 0.01, 'decision_prob_diff_thresh': 0.5, 'ending_balance': 3214.3999999999983, 'max_balance': 3214.3999999999983, 'min_balance': 507.20000000001335, 'max_equity': 3213.1999999999994, 'min_equity': 507.20000000001335, 'max_free_margin': 3139.596066666666, 'min_free_margin': 349.6160380952551, 'max_margin': 414.4994088888889, 'min_margin': 24.227700000000002, 'max_margin_level': 12641.50558265804, 'min_margin_level': 343.37238133084355, 'max_concurrently_open_trades': 9, 'min_concurrently_open_trades': 0, 'num_won_trades': 107, 'num_lost_trades': 73, 'num_buys': 157, 'num_won_buys': 95, 'num_lost_buys': 62, 'num_sells': 23, 'num_won_sells': 12, 'num_lost_sells': 11}\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1/1 model combos tested, runtime of last combo: 0.5838252107302347 min\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "grid search runtime: 0.5839333772659302 min\n"
     ]
    }
   ],
   "source": [
    "# start_dt_str, end_dt_str = '2020-10-02', '2021-01-05'\n",
    "start_dt_str, end_dt_str = '2020-11-02', '2021-01-05'\n",
    "grid_search_save_path = f'../my_stuff/{cur_pair}-{timeframe}_{start_dt_str}-to-{end_dt_str}_backtest_grid_search_results.csv'\n",
    "tick_data_filepath = research.download_mt5_data(cur_pair, timeframe, start_dt_str, end_dt_str) # (cur_pair, timeframe, '2020-10-02', '2021-01-05')\n",
    "best_strat_results = None\n",
    "best_strat_score = None\n",
    "backtest_results = []\n",
    "\n",
    "grid_search_start_time = time.time()\n",
    "for params_i, params in enumerate(param_grid):\n",
    "    s1 = time.time()\n",
    "    ma_models_settings = params['ma_models_settings']\n",
    "    xgb_model_settings = params['xgb_model_settings']\n",
    "    \n",
    "    strat_params = params['strat_params']\n",
    "    strat_params = ParameterGrid(strat_params)\n",
    "    strat_params = random.sample(list(strat_params), len(strat_params))\n",
    "    \n",
    "    fast_ma_model_path = ma_models_settings['fast_ma_model_path']\n",
    "#     slow_ma_model_path = ma_models_settings['slow_ma_model_path']\n",
    "    fast_ma_window = ma_models_settings['fast_ma_window']\n",
    "#     slow_ma_window = ma_models_settings['slow_ma_window']\n",
    "    \n",
    "    fast_ma_model = tf.keras.models.load_model(fast_ma_model_path)\n",
    "#     slow_ma_model = tf.keras.models.load_model(slow_ma_model_path)\n",
    "    \n",
    "    xgb_model_path = xgb_model_settings['model_filepath']\n",
    "    lots_per_trade = xgb_model_settings['lots_per_trade']\n",
    "    tenkan_period, kijun_period, senkou_b_period = xgb_model_settings['ichi_settings']\n",
    "    currency_side = xgb_model_settings['currency_side']\n",
    "    in_quote_currency = True if currency_side == 'right' else False\n",
    "    \n",
    "    xgb_decision_predictor = xgb.Booster()\n",
    "    xgb_decision_predictor.load_model(xgb_model_path)\n",
    "    \n",
    "    indicators_info = {\n",
    "        'ichimoku': {\n",
    "            'tenkan_period': tenkan_period,\n",
    "            'kijun_period': kijun_period,\n",
    "            'chikou_period': kijun_period,\n",
    "            'senkou_b_period': senkou_b_period\n",
    "        },\n",
    "        'rsi': {\n",
    "            'periods': 14\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    data_with_indicators = research.add_indicators_to_raw(filepath=tick_data_filepath, \n",
    "                                                          indicators_info=indicators_info, \n",
    "                                                          datetime_col='datetime')\n",
    "    test_data_with_ichi_sigs = research.add_ichimoku_features(data_with_indicators)\n",
    "    model_data = research.dummy_and_remove_features(test_data_with_ichi_sigs)\n",
    "    \n",
    "    start, stop = research.no_missing_data_idx_range(model_data)\n",
    "\n",
    "    model_data = model_data.iloc[start:stop+1]\n",
    "    model_data_np = model_data.to_numpy()\n",
    "\n",
    "    test_data_with_ichi_sigs = test_data_with_ichi_sigs.iloc[start:stop+1]\n",
    "    test_data_np = test_data_with_ichi_sigs.to_numpy()\n",
    "\n",
    "    ma_cols_set = set([model_data.columns.get_loc(col_name) for col_name in ma_cols])\n",
    "    pc_cols_set = set([model_data.columns.get_loc(col_name) for col_name in pc_cols])\n",
    "\n",
    "    feature_indices = {test_data_with_ichi_sigs.columns[i]: i for i in range(len(test_data_with_ichi_sigs.columns))}\n",
    "    \n",
    "    for params_i_2, params_2 in enumerate(strat_params):\n",
    "        s2 = time.time()\n",
    "        starting_balance = params_2['starting_balance']\n",
    "        leverage = params_2['leverage']    # 1:leverage\n",
    "        max_concurrent_trades = params_2['max_concurrent_trades']\n",
    "        fast_ma_diff_thresh = params_2['fast_ma_diff_threshold']\n",
    "        decision_prob_diff_thresh = params_2['decision_prob_diff_thresh']   # 0.5 accepts all probabilities\n",
    "        profit_noise_percent = params_2['profit_noise_percent']\n",
    "        \n",
    "        pip_value = contract_size * lots_per_trade * pip_resolution   # in quote currency (right side currency of currency pair)\n",
    "        profit_noise = profit_noise_percent * lots_per_trade * contract_size   # in base currecy because thats what models were traied on\n",
    "        \n",
    "        fast_ma_data = research.get_split_lstm_data(model_data, ma_window=fast_ma_window, seq_len=lstm_seq_len, split_percents=(0,0), \n",
    "                                                    normalization_groups=normalization_groups, pc_cols=pc_cols, ma_cols=ma_cols, min_batch_size=1000, \n",
    "                                                    max_batch_size=2000, just_train=True, print_info=False)\n",
    "#         slow_ma_data = research.get_split_lstm_data(model_data, ma_window=slow_ma_window, seq_len=lstm_seq_len, split_percents=(0,0), \n",
    "#                                           normalization_groups=normalization_groups, pc_cols=pc_cols, ma_cols=ma_cols, min_batch_size=1000, \n",
    "#                                           max_batch_size=2000, just_train=True, print_info=False)\n",
    "        \n",
    "        trades = {}\n",
    "        backtest_trades = {}   # closed trades results\n",
    "        pending_order = None\n",
    "        pending_close = None\n",
    "        decisions_so_far = []\n",
    "        fast_ma_seq_buf = deque()\n",
    "        slow_ma_seq_buf = deque()\n",
    "        fast_ma_window_buf = deque()\n",
    "        slow_ma_window_buf = deque()\n",
    "        fast_ma_avgs = []\n",
    "        slow_ma_avgs = []\n",
    "        fast_ma_perc_chngs = []\n",
    "        slow_ma_perc_chngs = []\n",
    "        xgb_model_perc_chngs = []\n",
    "        fast_ma_preds = []\n",
    "        slow_ma_preds = []\n",
    "        cnn_lstm_pred_times = []\n",
    "        xgb_pred_times = []\n",
    "        free_margins = []\n",
    "        margins = []\n",
    "        margin_levels = []\n",
    "        equities = []\n",
    "        balances = []\n",
    "        open_trades_counts = []\n",
    "        losing_trades_counts = []\n",
    "        pct_done = 0\n",
    "        buffers_rdy_idx = None\n",
    "        balance = starting_balance\n",
    "        equity = starting_balance\n",
    "        free_margin = starting_balance\n",
    "        losing_trades = 0\n",
    "        margin_level = None\n",
    "        margin = None\n",
    "        final_dt = None\n",
    "        stop = False\n",
    "\n",
    "        start_time = time.time()\n",
    "        for i in range(len(test_data_np)):\n",
    "            \"\"\"\n",
    "            fill data buffers for models\n",
    "            \"\"\"\n",
    "\n",
    "            # for xgb model\n",
    "\n",
    "            if i > 0:\n",
    "                row = research.apply_perc_change_list(model_data_np[i-1], model_data_np[i], cols_set=pc_cols_set)\n",
    "                xgb_model_perc_chngs.append(row)\n",
    "\n",
    "            # for fast MA model\n",
    "\n",
    "            fast_ma_window_buf.append(model_data_np[i])\n",
    "            if len(fast_ma_window_buf) > fast_ma_window:\n",
    "                fast_ma_window_buf.popleft()\n",
    "\n",
    "            if len(fast_ma_window_buf) == fast_ma_window:\n",
    "                row = research.apply_moving_avg_q(fast_ma_window_buf, ma_cols_set)\n",
    "                fast_ma_avgs.append(row)\n",
    "\n",
    "            if len(fast_ma_avgs) >= 2:\n",
    "                row = research.apply_perc_change_list(fast_ma_avgs[-2], fast_ma_avgs[-1], pc_cols_set)\n",
    "                row = research.normalize_data_list(row, fast_ma_norm_terms)\n",
    "                fast_ma_perc_chngs.append(row) \n",
    "\n",
    "            if len(fast_ma_perc_chngs) > 0:\n",
    "                fast_ma_seq_buf.append(fast_ma_perc_chngs[-1])\n",
    "\n",
    "            if len(fast_ma_seq_buf) > lstm_seq_len:\n",
    "                fast_ma_seq_buf.popleft()\n",
    "\n",
    "            # for slow MA model\n",
    "\n",
    "        #     slow_ma_window_buf.append(model_data_np[i])\n",
    "        #     if len(slow_ma_window_buf) > slow_ma_window:\n",
    "        #         slow_ma_window_buf.popleft()\n",
    "\n",
    "        #     if len(slow_ma_window_buf) == slow_ma_window:\n",
    "        #         row = apply_moving_avg_q(slow_ma_window_buf, ma_cols_set)\n",
    "        #         slow_ma_avgs.append(row)\n",
    "\n",
    "        #     if len(slow_ma_avgs) >= 2:\n",
    "        #         row = apply_perc_change_list(slow_ma_avgs[-2], slow_ma_avgs[-1], pc_cols_set)\n",
    "        #         row = normalize_data_list(row, slow_ma_norm_terms)\n",
    "        #         slow_ma_perc_chngs.append(row)  \n",
    "\n",
    "        #     if len(slow_ma_perc_chngs) > 0:\n",
    "        #         slow_ma_seq_buf.append(slow_ma_perc_chngs[-1])\n",
    "\n",
    "        #     if len(slow_ma_seq_buf) > lstm_seq_len:\n",
    "        #         slow_ma_seq_buf.popleft()\n",
    "\n",
    "            # now check if LSTMs have enough data to being trade simulation\n",
    "\n",
    "        #     if len(fast_ma_seq_buf) == lstm_seq_len and len(slow_ma_seq_buf) == lstm_seq_len:\n",
    "            if len(fast_ma_seq_buf) == lstm_seq_len:\n",
    "                \"\"\"\n",
    "                simulate trading\n",
    "                \"\"\"\n",
    "\n",
    "                if buffers_rdy_idx is None:\n",
    "                    buffers_rdy_idx = i\n",
    "                    print('model buffers full, beginning trade sim...')\n",
    "\n",
    "                # look for ichiomku signals\n",
    "                causes = []\n",
    "                for sig in open_trade_sigs:\n",
    "                    sig_i = feature_indices[sig]\n",
    "                    if test_data_np[i][sig_i] != 0:\n",
    "                        causes.append(sig)\n",
    "\n",
    "                start = time.time()\n",
    "                fast_ma_pred = fast_ma_model.predict(np.array([fast_ma_seq_buf]))\n",
    "                slow_ma_pred = [[0]] #slow_ma_model.predict(np.array([slow_ma_seq_buf]))\n",
    "                duration = time.time() - start\n",
    "                cnn_lstm_pred_times.append(duration)\n",
    "\n",
    "                fast_ma_preds.append(fast_ma_pred[0][0])\n",
    "                slow_ma_preds.append(slow_ma_pred[0][0])\n",
    "\n",
    "                if len(fast_ma_preds) > 1:\n",
    "                    fast_ma_diff = fast_ma_preds[-1] - fast_ma_preds[-2]    # remember this is the diff in the pct_change of the mov avg\n",
    "                    slow_ma_diff = slow_ma_preds[-1] - slow_ma_preds[-2]\n",
    "                else:\n",
    "                    fast_ma_diff = 0\n",
    "                    slow_ma_diff = 0\n",
    "\n",
    "                if pending_order is not None:\n",
    "                    pending_order_i, decision_label, decision_prob, order_causes, sig_fast_ma_diff, sig_slow_ma_diff = pending_order\n",
    "                    open_price = test_data_np[i][feature_indices['Open']]\n",
    "                    decision_prob_diff = abs(decision_label-decision_prob)\n",
    "\n",
    "                    trades[pending_order_i] = {\n",
    "                        'decision_label': decision_label,\n",
    "                        'decision_prob': decision_prob,\n",
    "                        'causes': order_causes,\n",
    "                        'open_price': open_price,\n",
    "                        'trade_open_tick_i': i,\n",
    "                        'profit': None,\n",
    "                        'best_profit': None,\n",
    "                        'ticks_till_close': None,\n",
    "                        'close_idx': None,\n",
    "                        'lots': lots_per_trade,\n",
    "                        'look_to_close': False,\n",
    "                        'forced_close': False,\n",
    "                        'fast_ma_diff_at_sig': sig_fast_ma_diff,\n",
    "                        'slow_ma_diff_at_sig': sig_slow_ma_diff,\n",
    "                        'fast_ma_diff_at_close': None,\n",
    "                        'slow_ma_diff_at_close': None,\n",
    "                        'fast_ma_diff_at_best_sign_to_close': None,\n",
    "                        'slow_ma_diff_at_best_sign_to_close': None\n",
    "                    }\n",
    "\n",
    "                    required_margin = research.get_margin(trades, buy_label=1, sell_label=0, contract_size=contract_size, leverage=leverage, \n",
    "                                                          tradersway_commodity=tradersway_commodity, in_quote_currency=in_quote_currency, hedged_margin=hedged_margin)\n",
    "\n",
    "                    # reference on opening trades and margin level https://www.luckscout.com/leverage-margin-balance-equity-free-margin-and-margin-level-in-forex-trading/\n",
    "                    if required_margin > free_margin or (margin_level is not None and margin_level <= 100) \\\n",
    "                            or len(trades) > max_concurrent_trades or decision_prob_diff > decision_prob_diff_thresh:\n",
    "                        del trades[pending_order_i]\n",
    "                    else:\n",
    "                        margin = required_margin\n",
    "\n",
    "                    pending_order = None\n",
    "\n",
    "                # update equity and free margin based on currently opened trades\n",
    "                for trade_i in trades:\n",
    "                    trade = trades[trade_i]\n",
    "                    close_price = test_data_np[i][feature_indices['Close']]\n",
    "                    trade_decision = xgb_labels_dict[trade['decision_label']]\n",
    "\n",
    "                    profit = research.get_profit(close_price, trade['open_price'], pip_value=pip_value, \n",
    "                                                 pip_resolution=pip_resolution, in_quote_currency=in_quote_currency)\n",
    "                    if trade_decision == 'sell':\n",
    "                        profit *= - 1\n",
    "\n",
    "                    if trade['profit'] is None:\n",
    "                        profit_delta = profit\n",
    "                    else:\n",
    "                        profit_delta = profit - trade['profit']\n",
    "                    trade['profit'] = profit\n",
    "\n",
    "                    if trade['best_profit'] is None or profit > trade['best_profit']:\n",
    "                        trade['best_profit'] = profit\n",
    "                        if (fast_ma_diff < 0 and trade_decision == 'buy') or (fast_ma_diff > 0 and trade_decision == 'sell'):\n",
    "                            trade['fast_ma_diff_at_best_sign_to_close'] = fast_ma_diff\n",
    "                        if (slow_ma_diff < 0 and trade_decision == 'buy') or (slow_ma_diff > 0 and trade_decision == 'sell'):\n",
    "                            trade['slow_ma_diff_at_best_sign_to_close'] = slow_ma_diff\n",
    "\n",
    "                    equity += profit_delta\n",
    "                    free_margin = equity - margin \n",
    "                    margin_level = equity / margin * 100\n",
    "\n",
    "                    scaled_profit_noise = profit_noise if not in_quote_currency else profit_noise / close_price\n",
    "                    if abs(profit) >= scaled_profit_noise:\n",
    "                        trade['look_to_close'] = True\n",
    "\n",
    "                # check if equity is <= 0, and if so end the sim\n",
    "                if equity <= 0:\n",
    "                    stop = True\n",
    "                    print(f'strat failed (i={i}, dt={test_data_np[i][feature_indices[\"datetime\"]]}): no more equity')\n",
    "\n",
    "                # check if trades should be closed due to stop-out starting with biggest loss if so\n",
    "                if margin_level is not None and margin_level <= stop_out_pct:\n",
    "                    sorted_keys = sorted(trades, key=lambda trade_i: trades[trade_i]['profit'])\n",
    "                    for j, trade_i in enumerate(sorted_keys):\n",
    "                        balance += trades[trade_i]['profit']\n",
    "\n",
    "                        open_tick_i = trades[trade_i]['trade_open_tick_i']\n",
    "                        trades[trade_i]['ticks_till_close'] = i - open_tick_i\n",
    "                        trades[trade_i]['close_idx'] = i\n",
    "                        trades[trade_i]['forced_close'] = True\n",
    "                        trades[trade_i]['fast_ma_diff_at_close'] = fast_ma_diff\n",
    "                        trades[trade_i]['slow_ma_diff_at_close'] = slow_ma_diff\n",
    "                        backtest_trades[trade_i] = trades[trade_i]\n",
    "\n",
    "                        del trades[trade_i]\n",
    "\n",
    "                        if j != len(sorted_keys) - 1:\n",
    "                            margin = research.get_margin(trades, buy_label=1, sell_label=0, contract_size=contract_size, leverage=leverage, \n",
    "                                                         tradersway_commodity=tradersway_commodity, in_quote_currency=in_quote_currency, hedged_margin=hedged_margin)\n",
    "                            free_margin = equity - margin\n",
    "                            margin_level = equity / margin * 100                    \n",
    "                            if margin_level > stop_out_pct:\n",
    "                                break   \n",
    "\n",
    "                # find trades to close based on CNN-LSTM preds\n",
    "                closed_trades = []\n",
    "                losing_trades = 0\n",
    "                for trade_i in trades: \n",
    "                    trade = trades[trade_i]\n",
    "                    trade_decision = xgb_labels_dict[trade['decision_label']]\n",
    "\n",
    "                    if trade['look_to_close']:\n",
    "                        if abs(fast_ma_diff) >= fast_ma_diff_thresh:\n",
    "                            # (MA pct_change is decreasing on a long trade) or (MA pct_change is increasing on a short trade)\n",
    "                            if (fast_ma_diff < 0 and trade_decision == 'buy') or (fast_ma_diff > 0 and trade_decision == 'sell'):  \n",
    "                                closed_trades.append(trade_i)\n",
    "                                continue    # continue to not count this trade in losing trades if it will be closed anyways\n",
    "\n",
    "                    if trade['profit'] < 0:\n",
    "                        losing_trades += 1\n",
    "\n",
    "                for trade_i in closed_trades:\n",
    "                    balance += trades[trade_i]['profit']\n",
    "\n",
    "                    open_tick_i = trades[trade_i]['trade_open_tick_i']\n",
    "                    trades[trade_i]['ticks_till_close'] = i - open_tick_i\n",
    "                    trades[trade_i]['close_idx'] = i\n",
    "                    trades[trade_i]['fast_ma_diff_at_close'] = fast_ma_diff\n",
    "                    trades[trade_i]['slow_ma_diff_at_close'] = slow_ma_diff\n",
    "                    backtest_trades[trade_i] = trades[trade_i]\n",
    "\n",
    "                    del trades[trade_i]\n",
    "\n",
    "                if len(trades) == 0:\n",
    "                    margin = None\n",
    "                    margin_level = None\n",
    "\n",
    "                # generate decision w/ XGB classifier and create pending order\n",
    "                if len(causes) > 0 and not stop:\n",
    "                    start = time.time()\n",
    "                    model_input = pd.DataFrame([xgb_model_perc_chngs[-1]], columns=model_data.columns)\n",
    "                    model_input = xgb.DMatrix(model_input)\n",
    "                    decision_prob = xgb_decision_predictor.predict(model_input)[0]\n",
    "                    duration = time.time() - start # inlucde converting input in pred time\n",
    "                    xgb_pred_times.append(duration)\n",
    "\n",
    "                    decision_label = np.around(decision_prob)\n",
    "\n",
    "        #             if (decision_label == 1 and fast_ma_diff > 0) or (decision_label == 0 and fast_ma_diff < 0):\n",
    "        #                 pending_order = (i, decision_label, decision_prob, causes, fast_ma_diff, slow_ma_diff)\n",
    "                    pending_order = (i, decision_label, decision_prob, causes, fast_ma_diff, slow_ma_diff)\n",
    "\n",
    "                cur_pct_done = int((i-buffers_rdy_idx+1) / (len(test_data_np)-buffers_rdy_idx) * 100)\n",
    "                if cur_pct_done != pct_done and cur_pct_done % 10 == 0:\n",
    "                    pct_done = cur_pct_done\n",
    "                    print(f'backtest percentage done: {cur_pct_done}%')\n",
    "\n",
    "            free_margins.append(free_margin)\n",
    "            equities.append(equity)\n",
    "            balances.append(balance)\n",
    "            margins.append(margin)\n",
    "            margin_levels.append(margin_level)\n",
    "            open_trades_counts.append(len(trades))\n",
    "            losing_trades_counts.append(losing_trades)\n",
    "\n",
    "            final_dt = test_data_np[i][feature_indices[\"datetime\"]]\n",
    "            if stop:\n",
    "                break\n",
    "\n",
    "        # print backtest results\n",
    "\n",
    "        backtest_runtime = time.time() - start_time\n",
    "        start_dt = test_data_np[buffers_rdy_idx][feature_indices['datetime']]\n",
    "        end_dt = final_dt\n",
    "\n",
    "        margin_levels_no_none = [ml for ml in margin_levels if ml is not None]\n",
    "        max_margin_level = None if len(margin_levels_no_none) == 0 else max(margin_levels_no_none)\n",
    "        min_margin_level = None if len(margin_levels_no_none) == 0 else min(margin_levels_no_none)\n",
    "        margins_no_none = [m for m in margins if m is not None]\n",
    "        max_margin = None if len(margins_no_none) == 0 else max(margins_no_none)\n",
    "        min_margin =  None if len(margins_no_none) == 0 else min(margins_no_none)\n",
    "\n",
    "        num_won = 0\n",
    "        num_lost = 0\n",
    "        num_won_sells = 0\n",
    "        num_won_buys = 0\n",
    "        num_lost_sells = 0\n",
    "        num_lost_buys = 0\n",
    "        ma_diff_stat_names = ['fast_ma_diff_at_sig', 'slow_ma_diff_at_sig', 'fast_ma_diff_at_close', 'slow_ma_diff_at_close',\n",
    "                              'fast_ma_diff_at_best_sign_to_close', 'slow_ma_diff_at_best_sign_to_close']\n",
    "        losses_ma_diff_stats = {name: {'list': [], 'agree_list':[], 'oppose_list':[]} for name in ma_diff_stat_names}\n",
    "        wins_ma_diff_stats = {name: {'list': [], 'agree_list':[], 'oppose_list':[]} for name in ma_diff_stat_names}\n",
    "        for trade_i in backtest_trades:\n",
    "            trade = backtest_trades[trade_i]\n",
    "            if trade['profit'] > 0:\n",
    "                if trade['decision_label'] == 1:\n",
    "                    num_won_buys += 1\n",
    "                else:\n",
    "                    num_won_sells += 1\n",
    "                num_won += 1\n",
    "\n",
    "                if (trade['decision_label'] == 1 and trade['fast_ma_diff_at_sig'] > 0) or (trade['decision_label'] == 0 and trade['fast_ma_diff_at_sig'] < 0):\n",
    "                    wins_ma_diff_stats['fast_ma_diff_at_sig']['agree_list'].append(abs(trade['fast_ma_diff_at_sig']))\n",
    "                elif (trade['decision_label'] == 1 and trade['fast_ma_diff_at_sig'] < 0) or (trade['decision_label'] == 0 and trade['fast_ma_diff_at_sig'] > 0):\n",
    "                    wins_ma_diff_stats['fast_ma_diff_at_sig']['oppose_list'].append(abs(trade['fast_ma_diff_at_sig']))\n",
    "                if (trade['decision_label'] == 1 and trade['slow_ma_diff_at_sig'] > 0) or (trade['decision_label'] == 0 and trade['slow_ma_diff_at_sig'] < 0):\n",
    "                    wins_ma_diff_stats['slow_ma_diff_at_sig']['agree_list'].append(abs(trade['slow_ma_diff_at_sig']))\n",
    "                elif (trade['decision_label'] == 1 and trade['slow_ma_diff_at_sig'] < 0) or (trade['decision_label'] == 0 and trade['slow_ma_diff_at_sig'] > 0):\n",
    "                    wins_ma_diff_stats['slow_ma_diff_at_sig']['oppose_list'].append(abs(trade['slow_ma_diff_at_sig']))\n",
    "\n",
    "                wins_ma_diff_stats['fast_ma_diff_at_close']['list'].append(abs(trade['fast_ma_diff_at_close']))\n",
    "                wins_ma_diff_stats['slow_ma_diff_at_close']['list'].append(abs(trade['slow_ma_diff_at_close']))\n",
    "\n",
    "                if trade['fast_ma_diff_at_best_sign_to_close'] is not None:\n",
    "                    wins_ma_diff_stats['fast_ma_diff_at_best_sign_to_close']['list'].append(abs(trade['fast_ma_diff_at_best_sign_to_close']))\n",
    "                if trade['slow_ma_diff_at_best_sign_to_close'] is not None:\n",
    "                    wins_ma_diff_stats['slow_ma_diff_at_best_sign_to_close']['list'].append(abs(trade['slow_ma_diff_at_best_sign_to_close']))\n",
    "            else:\n",
    "                if trade['decision_label'] == 1:\n",
    "                    num_lost_buys += 1\n",
    "                else:\n",
    "                    num_lost_sells += 1\n",
    "                num_lost += 1\n",
    "\n",
    "                if (trade['decision_label'] == 1 and trade['fast_ma_diff_at_sig'] > 0) or (trade['decision_label'] == 0 and trade['fast_ma_diff_at_sig'] < 0):\n",
    "                    losses_ma_diff_stats['fast_ma_diff_at_sig']['agree_list'].append(abs(trade['fast_ma_diff_at_sig']))\n",
    "                elif (trade['decision_label'] == 1 and trade['fast_ma_diff_at_sig'] < 0) or (trade['decision_label'] == 0 and trade['fast_ma_diff_at_sig'] > 0):\n",
    "                    losses_ma_diff_stats['fast_ma_diff_at_sig']['oppose_list'].append(abs(trade['fast_ma_diff_at_sig']))\n",
    "                if (trade['decision_label'] == 1 and trade['slow_ma_diff_at_sig'] > 0) or (trade['decision_label'] == 0 and trade['slow_ma_diff_at_sig'] < 0):\n",
    "                    losses_ma_diff_stats['slow_ma_diff_at_sig']['agree_list'].append(abs(trade['slow_ma_diff_at_sig']))\n",
    "                elif (trade['decision_label'] == 1 and trade['slow_ma_diff_at_sig'] < 0) or (trade['decision_label'] == 0 and trade['slow_ma_diff_at_sig'] > 0):\n",
    "                    losses_ma_diff_stats['slow_ma_diff_at_sig']['oppose_list'].append(abs(trade['slow_ma_diff_at_sig']))\n",
    "\n",
    "                losses_ma_diff_stats['fast_ma_diff_at_close']['list'].append(abs(trade['fast_ma_diff_at_close']))\n",
    "                losses_ma_diff_stats['slow_ma_diff_at_close']['list'].append(abs(trade['slow_ma_diff_at_close']))\n",
    "\n",
    "                if trade['fast_ma_diff_at_best_sign_to_close'] is not None:\n",
    "                    losses_ma_diff_stats['fast_ma_diff_at_best_sign_to_close']['list'].append(abs(trade['fast_ma_diff_at_best_sign_to_close']))\n",
    "                if trade['slow_ma_diff_at_best_sign_to_close'] is not None:\n",
    "                    losses_ma_diff_stats['slow_ma_diff_at_best_sign_to_close']['list'].append(abs(trade['slow_ma_diff_at_best_sign_to_close']))\n",
    "        losses_ma_diff_stats = {name: {'arr': np.array(losses_ma_diff_stats[name]['list']), \n",
    "                                       'agree_arr': np.array(losses_ma_diff_stats[name]['agree_list']), \n",
    "                                       'oppose_arr': np.array(losses_ma_diff_stats[name]['oppose_list'])} for name in losses_ma_diff_stats}\n",
    "        wins_ma_diff_stats = {name: {'arr': np.array(wins_ma_diff_stats[name]['list']), \n",
    "                                     'agree_arr': np.array(wins_ma_diff_stats[name]['agree_list']), \n",
    "                                     'oppose_arr': np.array(wins_ma_diff_stats[name]['oppose_list'])} for name in wins_ma_diff_stats}\n",
    "\n",
    "        print('\\n--------------------------------------------------------------------\\n')\n",
    "        print('BACKTEST RESULTS:')\n",
    "        print(f'ticks data duration: {(end_dt-start_dt).days} days')\n",
    "        print(f'starting balance: {starting_balance}')\n",
    "        print(f'ending balance: {balance}')\n",
    "        print(f'number of trades won: {num_won}')\n",
    "        print(f'number of trades lost: {num_lost}')\n",
    "        print(f'number of buys: {num_won_buys+num_lost_buys} ({num_won_buys} won, {num_lost_buys} lost)')\n",
    "        print(f'number of sells: {num_won_sells+num_lost_sells} ({num_won_sells} won, {num_lost_sells} lost)')\n",
    "        print(f'balance range: [{min(balances)}, {max(balances)}]')\n",
    "        print(f'equity range: [{min(equities)}, {max(equities)}]')\n",
    "        print(f'free margin range: [{min(free_margins)}, {max(free_margins)}]')\n",
    "        print(f'margins range: [{min_margin}, {max_margin}]')\n",
    "        print(f'margin levels range: [{min_margin_level}, {max_margin_level}]')\n",
    "        print(f'concurrently open trades range: [{min(open_trades_counts)}, {max(open_trades_counts)}]')\n",
    "        print(f'concurrently losing trades range: [{min(losing_trades_counts)}, {max(losing_trades_counts)}]')\n",
    "        print(f'backtest runtime: {backtest_runtime/60} min')\n",
    "\n",
    "        print('\\nWON TRADES RESULTS:')\n",
    "        for stat in wins_ma_diff_stats:\n",
    "            stat_arr = wins_ma_diff_stats[stat]['arr']\n",
    "            stat_agree_arr = wins_ma_diff_stats[stat]['agree_arr']\n",
    "            stat_oppose_arr = wins_ma_diff_stats[stat]['oppose_arr']\n",
    "            if len(stat_arr) > 0:\n",
    "                print(f'{stat}: count={len(stat_arr)}, min={np.amin(stat_arr)}, max={np.amax(stat_arr)},'\n",
    "                      f' mean={np.mean(stat_arr)}, median={np.median(stat_arr)}')\n",
    "            if len(stat_agree_arr) > 0:\n",
    "                print(f'{stat} that aggreed: count={len(stat_agree_arr)}, min={np.amin(stat_agree_arr)}, max={np.amax(stat_agree_arr)},'\n",
    "                      f' mean={np.mean(stat_agree_arr)}, median={np.median(stat_agree_arr)}')\n",
    "            if len(stat_oppose_arr) > 0:\n",
    "                print(f'{stat} that opposed: count={len(stat_oppose_arr)}, min={np.amin(stat_oppose_arr)}, max={np.amax(stat_oppose_arr)},'\n",
    "                      f' mean={np.mean(stat_oppose_arr)}, median={np.median(stat_oppose_arr)}')\n",
    "\n",
    "        print('\\nLOST TRADES RESULTS:')\n",
    "        for stat in losses_ma_diff_stats:\n",
    "            stat_arr = losses_ma_diff_stats[stat]['arr']\n",
    "            stat_agree_arr = losses_ma_diff_stats[stat]['agree_arr']\n",
    "            stat_oppose_arr = losses_ma_diff_stats[stat]['oppose_arr']\n",
    "            if len(stat_arr) > 0:\n",
    "                print(f'{stat}: count={len(stat_arr)}, min={np.amin(stat_arr)}, max={np.amax(stat_arr)},'\n",
    "                      f' mean={np.mean(stat_arr)}, median={np.median(stat_arr)}')\n",
    "            if len(stat_agree_arr) > 0:\n",
    "                print(f'{stat} that aggreed: count={len(stat_agree_arr)}, min={np.amin(stat_agree_arr)}, max={np.amax(stat_agree_arr)},'\n",
    "                      f' mean={np.mean(stat_agree_arr)}, median={np.median(stat_agree_arr)}')\n",
    "            if len(stat_oppose_arr) > 0:\n",
    "                print(f'{stat} that opposed: count={len(stat_oppose_arr)}, min={np.amin(stat_oppose_arr)}, max={np.amax(stat_oppose_arr)},'\n",
    "                      f' mean={np.mean(stat_oppose_arr)}, median={np.median(stat_oppose_arr)}')\n",
    "\n",
    "        print('\\nMODELS STATS:')\n",
    "        #         print(f'average pred time of fast & slow MA CNN+LSTM models: {sum(cnn_lstm_pred_times)/len(cnn_lstm_pred_times)*1000} ms')\n",
    "        print(f'average pred time of fast MA CNN+LSTM models: {sum(cnn_lstm_pred_times)/len(cnn_lstm_pred_times)*1000} ms')\n",
    "        print(f'average pred time of XGB model: {sum(xgb_pred_times)/len(xgb_pred_times)*1000} ms')\n",
    "        \n",
    "        results = {\n",
    "            'tenkan_period': tenkan_period,\n",
    "            'kijun_period': kijun_period,\n",
    "            'chikou_period': kijun_period,\n",
    "            'senkou_b_period': senkou_b_period,\n",
    "            'fast_ma_model_path': fast_ma_model_path,\n",
    "#             'slow_ma_model_path': slow_ma_model_path,\n",
    "            'fast_ma_window': fast_ma_window,\n",
    "#             'slow_ma_window': slow_ma_window,\n",
    "            'xgb_model_path': xgb_model_path,\n",
    "            'lots_per_trade': lots_per_trade,\n",
    "            'profit_noise_percent': profit_noise_percent,\n",
    "            'stop_out_pct': stop_out_pct,\n",
    "            'starting_balance': starting_balance,\n",
    "            'leverage': leverage,\n",
    "            'max_concurrent_trades': max_concurrent_trades,\n",
    "            'currency_side': currency_side,\n",
    "            'fast_ma_diff_thresh': fast_ma_diff_thresh,\n",
    "#             'slow_ma_diff_thresh': slow_ma_diff_thresh,\n",
    "            'decision_prob_diff_thresh': decision_prob_diff_thresh,\n",
    "            'ending_balance': balance,\n",
    "            'max_balance': max(balances),\n",
    "            'min_balance': min(balances),\n",
    "            'max_equity': max(equities),\n",
    "            'min_equity': min(equities),\n",
    "            'max_free_margin': max(free_margins),\n",
    "            'min_free_margin': min(free_margins),\n",
    "            'max_margin': max_margin,\n",
    "            'min_margin': min_margin,\n",
    "            'max_margin_level': max_margin_level,\n",
    "            'min_margin_level': min_margin_level,\n",
    "            'max_concurrently_open_trades': max(open_trades_counts),\n",
    "            'min_concurrently_open_trades': min(open_trades_counts),\n",
    "            'num_won_trades': num_won,\n",
    "            'num_lost_trades': num_lost,\n",
    "            'num_buys': num_won_buys+num_lost_buys,\n",
    "            'num_won_buys': num_won_buys,\n",
    "            'num_lost_buys': num_lost_buys,\n",
    "            'num_sells': num_won_sells+num_lost_sells,\n",
    "            'num_won_sells': num_won_sells,\n",
    "            'num_lost_sells': num_lost_sells\n",
    "        }\n",
    "        \n",
    "        strat_score = balance\n",
    "        if best_strat_results is None or best_strat_score < strat_score:\n",
    "            best_strat_results = results\n",
    "            best_strat_score = strat_score\n",
    "            \n",
    "        backtest_results.append(results)\n",
    "        \n",
    "        print('\\n--------------------------------------------------------------------------------')\n",
    "        print(f'{params_i_2+1}/{len(strat_params)} strat params tested, runtime of last params: {(time.time()-s2)/60} min')\n",
    "        print('--------------------------------------------------------------------------------\\n')\n",
    "        print(f'last backtest results:')\n",
    "        print(f'{results}\\n')\n",
    "        print(f'best backtest results:')\n",
    "        print(f'{best_strat_results}\\n')\n",
    "        \n",
    "        if params_i_2 % 100 == 0:\n",
    "            backtest_results_sorted = sorted(backtest_results, key=lambda d: d['ending_balance'], reverse=True)\n",
    "            backtest_results_sorted_df = pd.DataFrame(backtest_results_sorted)\n",
    "            backtest_results_sorted_df.to_csv(grid_search_save_path)\n",
    "        \n",
    "    print('\\n--------------------------------------------------------------------------------')\n",
    "    print(f'{params_i+1}/{len(param_grid)} model combos tested, runtime of last combo: {(time.time()-s1)/60} min')\n",
    "    print('--------------------------------------------------------------------------------\\n')\n",
    "\n",
    "    backtest_results_sorted = sorted(backtest_results, key=lambda d: d['ending_balance'], reverse=True)\n",
    "    backtest_results_sorted_df = pd.DataFrame(backtest_results_sorted)\n",
    "    backtest_results_sorted_df.to_csv(grid_search_save_path)\n",
    "\n",
    "print(f'grid search runtime: {(time.time()-grid_search_start_time)/60} min')\n",
    "\n",
    "backtest_results_sorted = sorted(backtest_results, key=lambda d: d['ending_balance'], reverse=True)\n",
    "backtest_results_sorted_df = pd.DataFrame(backtest_results_sorted)\n",
    "backtest_results_sorted_df.to_csv(grid_search_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# notes on things to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "if self.data[i][self.feature_indices['datetime']].strftime('%Y-%m-%dT%H:%M') == '2013-05-28T10:00':\n",
    "    print('yo')\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "To-do:\n",
    "\n",
    "1) tune hyperparams for backtest of xgboost for opening and CNN+Bi-LSTM for closing strat\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
